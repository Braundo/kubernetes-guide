{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Currently reworking the content and organization of the site.</p> <p> Welcome to the Kubernetes Guide, a quick summary of core Kubernetes concepts intended to help get you from zero to proficient!  Feel free to pick and choose any section in any order, but you'll likely be best served by following along in the default order of the site.</p> <p></p> <p>Legal discalimer:  </p> <ul> <li> <p>\"Kubernetes\", \"K8s\" and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  </p> </li> <li> <p>Neither myself nor this site are officially associated with the Linux Foundation.  </p> </li> </ul>"},{"location":"config-maps-secrets/","title":"ConfigMaps and Secrets","text":"<p>test</p>"},{"location":"deployments/","title":"Deployments","text":""},{"location":"deployments/#overview","title":"Overview","text":"<p>The main idea behind Deployments is that you tell Kubernetes the desired state of your application a looping controller watches your app and continuously attempts to reconcile the actual state of your app with the desired state.</p>"},{"location":"deployments/#deployment-spec","title":"Deployment Spec","text":"<p>The way you tell Kubernetes how you want your application to look is through the use of a YAML file (Deployment spec). When you POST the Deployment spec (via <code>kubectl</code>) to the API server, Kubernetes goes through the process of deploying your application to match the desired state and leverages a Deployment controller to continuously watch your application state.  </p> <p>It should be noted that every Deployment object will only manage a single Pod object. If you have an application with more than one Pod, you will need more than one Deployment object. But, a single Deployment object can manage any number of replicas of a given Pod.</p>"},{"location":"deployments/#replicasets","title":"ReplicaSets","text":"<p>Under the covers, Deployments actually leverage a different Kubernetes object to handle Pod scaling and reboots - the RelpicaSet. You should never be managing ReplicaSets directly, but it's good to know they exist and understand the hierarchy of control here. Containers will be wrapped in Pods, which have their scaling and self-healing managed by ReplicaSets, which in turn are managed by Deployments.</p> <pre><code>flowchart TB\n    subgraph Deployment\n        subgraph ReplicaSet\n            Pod1[Pod]\n            Pod2[Pod]\n        end\n    end</code></pre>"},{"location":"deployments/#scaling-and-self-healing","title":"Scaling and Self-Healing","text":"<p>If you deploy a pod by itself (either via a YAML file or <code>kubectl</code>), if it dies or fails, the Pod is lost forever.</p> <p>We never \"revive\" Pods; the appropriate way to \"revive\" a failed Pod is to create a new one to replace it.</p> <p>However, with the magic of Deployments, if a Pod that was created via Deployment fails, it will be replaced. Remember that Deployment controllers continuously watch for deviations from your desired state; so if you specified that your application should run 3 Pods and one of the Pods fails, the controller will recognize that actual state (2 Pods) no longer matches desired state (3 Pods), and it will kick off a series of actions to deploy another Pod.</p>"},{"location":"deployments/#rolling-updates","title":"Rolling Updates","text":"<p>This same logic allows seamless, zero-downtime updates for your applications. Let's say you defined your application to have 5 Pods running and labeled it as being <code>v1.2</code>. Your team introduces some new features or implements some bug fixes and creates <code>v1.3</code> of your application. Your next step will be to go in and update your desired state (Deployment spec) from <code>v1.2</code> <code>v1.3</code>. The Deployment controller will then recognize that the actual state (<code>v1.2</code>) no longer matches the desired state (<code>v1.3</code>) and begin the process of spinning down outdated Pods and spinning up new Pods with the new version.  </p> <p>Some things to keep in mind for this to work: your application(s) need(s) to maintain loose coupling and maintain backwards and forwards compatability (cloud native application design pattern).  </p> <p>There are different rolling update strategies you can employ that specify how to handle rollouts/rollbacks, how many can be deploye or spun down at once, etc. For more in-depth information on these strategies, refer to the official Kubernetes documentation.</p>"},{"location":"deployments/#rollbacks","title":"Rollbacks","text":"<p>Rollbacks work in the same manner as rolling updates from above but in reverse. Essentially, imagine you had an issue with <code>v1.3</code> and need to roll back to <code>v1.2</code>. It's as simple as updating your Deployment spec and letting the Deployment controller notice this change and begin that reconcilliation process. Kubernetes let you specify how many revisions (old versions) of your Deployments should be maintained for the purposes of rollbacks. In your Deployment spec, this is defined by the <code>revisionHistoryLimit</code> block.  </p> <p>You can view the update history of a deployment by running the following command: <pre><code>kubectl rollout history deployment/&lt;deployment-name&gt;\n</code></pre></p>"},{"location":"deployments/#scaling","title":"Scaling","text":"<p>Performing manual scaling operations with Deployments is also super straightforward and can be done in a similar manner to the one above. If you decide you actually want 10 Pods instead of 5, it's as simple as updating your Deployment spec and updating the <code>Replicas</code> block to the desired amount of Pods. Once again, the Deployment controller will notice the variation in states and begin reconciliation.</p>"},{"location":"dns/","title":"DNS","text":"<p>test</p>"},{"location":"ingress/","title":"Ingress","text":""},{"location":"ingress/#overview","title":"Overview","text":"<p>Ingress aims to bridge the gap that exists with NodePort and LoadBalancer Services. NodePorts are great, but must use a high port number and require you to know the FQDN or IP address of your nodes. LoadBalancer Services don't require this, but they are limited to one internal Service per load-balancer. So, if you have 50 applications you need exposed to the interent, you'd need 50 of your cloud provider's load-balancers instantiated - which would probably be cost prohibitive in most cases.  </p> <p>Ingresses come into play here by allowing multiple Services to be \"fronted\" by a single cloud load-balancer. To accomplish this, Ingress will use a single LoadBalancer Service and use host-based or path-based routing to send traffic to the appropriate underlying Service.  </p> <pre><code>flowchart LR\n    CLD[cloud] --&gt; LBS\n    LBS[&lt;b&gt;LoadBalancer&lt;br&gt;Service] --&gt; ing1[Ingress controller]\n    ing[/routing rules&lt;br&gt;reading host &amp;&lt;br&gt;path names/] -.- ing1\n    ing1 --&gt; SVC1[&lt;b&gt;Service]\n    ing1 --&gt; SVC2[&lt;b&gt;Service]\n    ing1 --&gt; SVC3[&lt;b&gt;Service]</code></pre>"},{"location":"ingress/#routing-examples","title":"Routing Examples","text":""},{"location":"ingress/#host-based-routing","title":"Host-based Routing","text":"<p><pre><code>flowchart LR\n    CLD[client] --&gt; |ham.foo.bar| LBS\n    CLD[client] --&gt; |eggs.foo.bar| LBS\n    LBS[&lt;b&gt;LoadBalancer&lt;br&gt;Service] --&gt; ing1[Ingress controller]\n    ing1 --&gt; |ham.foo.bar|SVC1[&lt;b&gt;ham-svc]\n    ing1 --&gt; |eggs.foo.bar|SVC2[&lt;b&gt;eggs-svc]</code></pre> </p>"},{"location":"ingress/#path-based-routing","title":"Path-based Routing","text":"<pre><code>flowchart LR\n    CLD[client] --&gt; |foo.bar/ham| LBS\n    CLD[client] --&gt; |foo.bar/eggs| LBS\n    LBS[&lt;b&gt;LoadBalancer&lt;br&gt;Service] --&gt; ing1[Ingress controller]\n    ing1 --&gt; |foo.bar/ham|SVC1[&lt;b&gt;ham-svc]\n    ing1 --&gt; |foo.bar/eggs|SVC2[&lt;b&gt;eggs-svc]</code></pre>"},{"location":"ingress/#more-information","title":"More Information","text":"<p>For a deeper dive into Ingress, refer to the official Kubernetes documentation.</p>"},{"location":"namespaces/","title":"Namespaces","text":""},{"location":"namespaces/#overview","title":"Overview","text":"<p>Namespaces are used to partition Kubernetes clusters and provide easy ways to apply policies and quotas at a more granular level.  </p> <p>Namespaces are not intended to be used for secure isolation</p> <p>If you need secure isolation, the best practice is to use multiple clusters.</p> <p>Namespaces can be a useful construct for partitioning a single cluster among various environments for teams. For instance, the a single cluster might have development and production environments partitioned by Namespace.  </p> <p>Kubernetes comes with a number of Namspaces already created. You can run the following command to view all namespaces on a cluster:  </p> <pre><code>$ kubectl get namespaces\n    NAME              STATUS   AGE\n    default           Active   22h\n    gmp-public        Active   22h\n    gmp-system        Active   22h\n    kube-node-lease   Active   22h\n    kube-public       Active   22h\n    kube-system       Active   22h\n</code></pre> <p>Your output will vary based on your environment.  </p>"},{"location":"namespaces/#deploying-objects-to-namespaces","title":"Deploying Objects to Namespaces","text":"<p>When deploying objects on Kubernetes you can specify the target Namespace imperatively by adding the <code>-n &lt;Namespace&gt;</code> flag to your command, or declaratively by specifying the Namespace in your YAML file.  </p> <p>If you don't explicitly define a Namespace, objects will be deployed to the <code>default</code> Namespace.</p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#history","title":"History","text":"<p>Kubernetes itself was born out of Google's experience running billions of containers at scale and managing them with proprietary systems called Borg and Omega. In 2014 Google donated Kubernetes as an open-source project to the Cloud Native Computing Foundation (CNCF).</p>"},{"location":"overview/#20k-foot-view","title":"20K-foot View","text":"<p>At a high-level, Kubernetes is responsible for deploying your applications and dynamically responding to changes to keep your applications running how you intended. Kubernetes runs on any cloud or on-premise datacenter, abstracting away all of the underlying infrastructure and letting you focus on application development. All applications running on Kubernetes must be containerized, and those containers must be running inside of a Pod.  </p> <p>Fundamentally, Kubernetes is a cluster - a group of machines, so to speak. These machines are called nodes in the Kubernetes world and can be cloud instances, virtual machines, physical servers, your laptop, etc.  </p> <p>A Kubernetes cluster consists of a control plane and any number of worker nodes. The control plane is the \"brain\" of Kubernets and handles things such as scheduling workloads to nodes, implenting the API, and watching for changes that need to be responded to. The worker nodes handle the leg-work of actually running applications.</p>"},{"location":"overview/#api-server","title":"API Server","text":"<p>Speaking of, the API server is the central component for all communication for all components in Kubernetes.  </p> <p>Any communication inbound or outbound to/from the Kubernetes cluster must be routed through the API server.</p>"},{"location":"overview/#cluster-store","title":"Cluster Store","text":"<p>The control plane, like many aspects of Kubernetes, exists in a stateless manner. However, the clsuter store does not - it persistently stores the state of the cluster and other configuration data. As of Kubernetes v1.28, <code>etcd</code> is the distributed databse that Kubernetes leverages for it's cluster store.  </p> <p><code>etcd</code> is installed on every control plane node by default for high-availability. However, it does not tolerate split-brain scenarios and will prevent updates to the cluster in such states - but it will still allow applications to run in those scenarios.</p>"},{"location":"overview/#controllers","title":"Controllers","text":"<p>Kubernetes consists of many different controllers, which are essentially background loops that watch for changes to the cluster (and alert when things don't match up so other components can take action). All controllers are managed and implemented by a higher-level component called the controller manager. </p> <p>The following logic is at the core of what Kubernetes is and how it works:  </p> <pre><code>flowchart TD\n    A(Obtain desired state) --&gt; B(Observe current state)\n    B --&gt; C{desired = current?}\n    C --&gt;|Yes| B\n    C --&gt;|No| E[Take action]</code></pre>"},{"location":"overview/#declarative-model","title":"Declarative Model","text":"<p>At the core of Kubernetes is the concept of the declarative model. You tell Kubernetes how you want your application to look and run (how many replicas, which image to use, network settings, commands to run, how to perform updates, etc.), and it's Kubernetes job to ensure that happens. You \"tell\" Kubernetes through the use of manifest files written in YAML.  </p> <p>You take those manifest files and <code>POST</code> them to the Kubernetes API server (typically through the use of <code>kubectl</code> commands). The API server will then authenticate the request, inspect the manifest for formatting, route the request to the appropriate controller (i.e. if you've defined a manifest file for a Deployment, it will send the request to the Deployments controller), and then it will record your desired state in the cluster store (remember, <code>etcd</code>). After this, the relevant controller will get started on performing any tasks necessary to get your application into it's desired state.  </p> <p>After your application is up and running, controllers begin monitoring it's state in the background and ensuring it matches the desired state in <code>etcd</code> (see simple logic diagram above).</p>"},{"location":"pods/","title":"Pods","text":"<p>Pods are the atomic unit of scheduling in Kubernetes. As virtual machines were in the VMware world, so are Pods in the world of Kubernetes. Every container running on Kubernetes must be wrapped up in a Pod. The most simple implementation of this are single-container Pods - one container inside one Pod. However there are certain instances where multi-container Pods make sense.</p> <p>It's important to note that when you scale up/down application in Kubernetes, you're not doing so by adding/removing containers directly - you do so by adding/removing Pods.</p>"},{"location":"pods/#atomic","title":"Atomic","text":"<p>Pod deployment is atomic in nature - a Pod is only considered Ready when all of its containers are up and running. Either the entire Pod comes up successfully and is running, or the entire thing doesn't. - there are no partial states.</p>"},{"location":"pods/#lifecycle","title":"Lifecycle","text":"<p>Pods are designed to be ephemeral in nature. Once a Pod dies, it's not meant to be restarted or revived. Instead, the intent to spin up a brand new Pod in the failed ones place (based off of your defined Manifest). Further, Pods are immutable and should not be changed once running. If you need to chance your application, you update the configuration via the manifest and deploy a new Pod.</p>"},{"location":"pods/#multi-container-pods","title":"Multi-container Pods","text":"<p>As mentioend above, the simplest way to run an app on Kubernetes is to run a single container inside of a single Pod. However, in situations where you need to tightly couple two or more functions you can co-locate multiple containers inside of the same pod. One such example would be leveraging the sidecar pattern for logging wherein the main container dumps logs to a supporting container that can sanitize and format the logs for consumption. This frees up the main container from having to worry about formatting logs.</p>"},{"location":"security/","title":"Security","text":"<p>test</p>"},{"location":"services/","title":"Services","text":""},{"location":"services/#overview","title":"Overview","text":"<p>As mentioned in the Deployments section, Pods will likely be spinning up and down a lot in your environment throughout the course of updates, rollbacks, failures, etc. As such, it's never a good idea for any client to connect directly to a Pod. Pods are there one minute, gone the next - awfully unreliable in and of themselves.  </p> <p>This is where Services come in. Services provide stable, long-lived connection points for clients to connect. They also maintain a list of Pods to route to and provide basic load-balancing capabilities. With Services, the underlying Pods can come and go, but the client should be able to maintain open communication with the application as the Service provides the logic to know which Pods are healthy and where to route traffic.  </p> <pre><code>flowchart LR\n    Client --&gt; SVC[Service]\n    SVC --&gt; Pod1[Pod]\n    SVC --&gt; Pod2[Pod]\n    SVC --&gt; Pod3[Pod]\n    SVC --&gt; Pod4[Pod]\n\n    subgraph Deployment\n        Pod1\n        Pod2\n        Pod3\n        Pod4\n    end</code></pre>"},{"location":"services/#labels-and-selectors","title":"Labels and Selectors","text":"<p>So how does that work? How do Services know which Pods they should be sending traffic to? The short answer is labels and selectors. In essence, when you define a Service, you specify labels and selectors that - when matched with the same ones on Pods - will route traffic to them.  </p> <p>As an example, image you want to put a stable Service in front of series of Pods that make up your shopping application. When you defined the Deployment of the application you listed the following labels and selectors for the Pods: <code>env=prod</code> and <code>app=shop</code>. Now, when you set up this new Service, you used those same labels in it's YAML definition. The new Service will find all Pods on the cluster with those same labels and is now in charge of routing traffic to them.  </p> <p>Similar to other Kubernetes objects, the Services controller will continually monitor new Pods labels and continually update it's \"list\" (more on that list later) of Pods to route to.  </p> <pre><code>flowchart LR\n    SVC[&lt;b&gt;Service&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop] --&gt; Pod1[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop]\n    SVC --&gt; Pod2[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop]\n    SVC --&gt; Pod3[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop]</code></pre> <p>One thing to note is that Pods can have extra labels and still be managed by the Service if it's other labels still match. As a concrete example, both of the Pods below will still have traffic routed to them, even though one of them has a label that the Service does not.</p> <pre><code>flowchart LR\n    SVC[&lt;b&gt;Service&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop] --&gt; Pod1[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop&lt;br&gt;cur=usd]\n    SVC --&gt; Pod2[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop]</code></pre>"},{"location":"services/#endpointslices","title":"EndpointSlices","text":"<p>As mentioned above, as Pods are spinning up and down, the Service will keep an updated list of Pods with the given labels and selectors. How it does this is through the use of EndpointSlices, which are effectively just dynamic lists of healthy Pods that match a given label selector.  </p> <p>Any new pods that are created on the cluster that match a Service's label selector will automatically be added to the given Selector's EndpointSlice object. When a Pod disappears (fails, node goes down, etc.) it will be removed from the EndpointSlice. The net result is that the Service's EndpointSlice should always be up to date with a list of healthy pods that the Service can route to.  </p>"},{"location":"services/#service-types","title":"Service Types","text":""},{"location":"services/#clusterip","title":"ClusterIP","text":"<p>Kubernetes supports different types of Services, but the default type is ClusterIP, which is only accessible from inside the cluster. Any time you create a Service in Kubernetes it will automatically get a ClusterIP that's registered in the cluster's internal DNS service (more on the DNS service in a different section). Every single Pod on a cluster leverages the cluster's DNS service - which results in all Pods being able to resolve Service names to ClusterIPs.  </p>"},{"location":"services/#nodeport","title":"NodePort","text":"<p>Another type of Service that Kubernetes supports is called NodePort. This is very similar to ClusterIP but adds the ability for external access on a dedicated port on every node in the cluster. NodePort intentionally uses high-numbered ports (30000-32767) to avoid clashing with common ports. To access a NodePort Service from an external client, you simply direct traffic to the IP address of any node in the cluster on the given port. The Service will then route the request to the appropriate Pod based on it's list of healthy ones in it's EndpointSlice object.</p>"},{"location":"services/#loadbalancer","title":"LoadBalancer","text":"<p>If you're running your Kubernetes cluster on a public cloud environment you can leverage a LoadBalancer Service. This will provision an internet-facing load-balance that you can leverage to send traffic to your Service. For more specifics on this type of Service, refer to the official Kubernetes documentation.</p>"},{"location":"statefulsets/","title":"StatefulSets","text":"<p>test</p>"},{"location":"storage/","title":"Storage","text":"<p>test</p>"}]}