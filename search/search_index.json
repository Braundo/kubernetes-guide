{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Kubernetes Guide, a quick and easy-to-digest summary of core Kubernetes concepts intended to help get you from zero to proficient!  Feel free to pick and choose any section in any order, but you'll likely be best served by following along in the default order of the site.  <pre><code>graph TB\n    cc[Core Concepts] --&gt;\n    Security --&gt;\n    Storage --&gt;\n    Networking --&gt;\n    misc[Misc. Topics]</code></pre> </p> <p>Legal discalimer:  </p> <ul> <li> <p>\"Kubernetes\", \"K8s\" and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  </p> </li> <li> <p>Neither myself nor this site are officially associated with the Linux Foundation. </p> </li> </ul> <p></p> <p> Connect with me</p> <p> Suggest changes</p>"},{"location":"storage/","title":"Storage","text":""},{"location":"storage/#overview","title":"Overview","text":"<p>Arguably the most important aspect of any application is the ability to persist and retrieve data. Thankfully, Kubernetes supports a wide variety of storage back-ends and also integrates with many third-party systems that provide things such as replication, snapshots, backup and more.  </p> <p>Kubernetes can also support different types of storage - anything from objects to files or blocks. However, regardless of the type of storage or where it's located (on-premise, cloud, etc.), Kubernetes will treat it as a volume.  </p> <p>Kubernetes is able to support so many different storage types and services by leveraging the Container Storage Interface (CSI). The CSI is an established standard that provides a straightforward interface for Kubernetes.</p> <pre><code>flowchart LR\n    subgraph external storage\n        netapp[(NetApp)]\n        azureblock[(Azure)]\n        etc[(etc)]\n    end\n    netapp --&gt; CSI\n    azureblock --&gt; CSI\n    etc --&gt; CSI\n    subgraph Kubernetes cluster\n    CSI --&gt;\n    subsystem[\"&lt;b&gt;Persistent Volume subsystem&lt;/b&gt;&lt;br&gt;&lt;tt&gt;pv, pvc, sc\"]\n    end</code></pre> <p>The only thing required for an external storage provider to be surfaced as a volume in Kubernetes is for it to have a CSI plugin. On the right side of the diagram you'll also notice three Kubernetes API objects:  </p> <ul> <li>Persistent Volumes (PV): map to external storage objects</li> <li>Persistent Volume Claims (PVC): akin to \"tickets\" that authorize Pods to be able to use the relevant PV</li> <li>Storage Classes (SC): wrap the previous two in some automation</li> </ul> <p>Take an example below where our cluster is running on GKE and we have a 2TB block of storage called <code>gce-pd</code>. We then create a PV called <code>k8s-vol</code> that will map to the <code>gce-pd</code> with the <code>pd.csi.storage.gke.io</code> CSI plugin. Here's how that might look visually:  </p> <pre><code>flowchart LR\n    storage[(&lt;tt&gt;gce-pd)] --- |pd.csi.storage.gke.io|k8s[\"Kubernetes cluster\"]\n    pv[\"&lt;b&gt;k8s-vol&lt;/b&gt;&lt;br&gt;&lt;tt&gt;pv\"]\n    k8s --- pv\n    pv -.- storage\n    pv --- pvc[\"pvc fa:fa-ticket\"]\n    pvc --- pod</code></pre> <p>Multiple Pods cannot access the same volume.</p> <p>You cannot map an external storage volume to multiple PVs.</p>"},{"location":"storage/#container-storage-interface-csi","title":"Container Storage Interface (CSI)","text":"<p>The CSI is an open-source project that defines interfaces in a clear manner so that storage can be leveraged across Kubernetes (and other container orchestrators).</p> <p>While CSI is a critical piece of getting storage working in Kubernetes, unless you explicitly work on writing storage plugins you'll likely never interact with it directly. Most of your interaction with CSI will simply be referencing your relevant CSI plugin in YAML files.  </p>"},{"location":"storage/#persistent-volumes","title":"Persistent Volumes","text":"<p>At a high level, PVs are the way external storage objects are represented in Kubernetes.</p>"},{"location":"storage/#storage-classes","title":"Storage Classes","text":"<p>StorageClasses (SCs) allow you to define different types of storage. How they are defined depends on the type of storage you're using. For example, if you're using Google Cloud Storage you have classes such as Standard, Nearline, Coldline, and Archive. You may also have simpler/more straightforward classes at your disposal such as SSD and HDD. When you create a SC you map both of those definitions so Pods in your cluster can use either or.</p> External Type K8s StorageClass SSD sc-fast HDD sc-slow <p>Below is an example of how a StorageClass YAML definition may look for leveraging Google Cloud Storage:  </p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ssd\nprovisioner: pd.csi.storage.gke.io  # Google Cloud CSI plugin\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nparameters:\n  type: pd-ssd  # Google Cloud SSD drives\n  provisioned-iops-on-create: '10000'\n</code></pre> <p>StorageClass objects are immutable. You cannot modify them after they are deployed.</p> <p>Below is the high-level flow for creating and using StorageClasses:</p> <ol> <li>Ensure you have a storage back-end (cloud, on-prem, etc.)</li> <li>Have a running Kubernetes cluster</li> <li>Install and setup the CSI storage plugin to connect to Kubernetes</li> <li>Create at least one StorageClass on Kubernetes</li> <li>Deploy Pods with PVCs that reference those Storage classes</li> </ol> <p>Below is an example YAML file that ties SC, PVC, and Pods together so you can see how they all interact:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ssd  # this will be referenced by the PVC below\nprovisioner: pd.csi.storage.gke.io\nparameters:\n  type: pd-ssd\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: mypvc  # this will be referenced by the Pod below\nspec:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n        requests:\n            storage: 256Gi\n    storageClassName: ssd  # matches the name of the SC above\n---\napiVersion: v1\nkind: Pod\nmetadata:\n    name: mypod\nspec:\n    volumes:\n    - name: data\n    persistentVolumeClaim:\n        claimName: mypvc  # matches the name of the PVC above\n...\n</code></pre> <p>This YAML is only partially complete - it's mainly for showing the relationships between these objects via metadata.</p>"},{"location":"storage/#access-mode","title":"Access Mode","text":"<p>Kubernetes StorageClasses support three different types of volume access modes:</p> <ol> <li>ReadWriteOnce(RWO): PV can only be bound as read/write by a single PVC (or Pod)</li> <li>ReadWriteMany: PV can be bound as read/write by multiple PVCs (or Pods)</li> <li>ReadOnlyMany: PV can be bound as read-only by multiple PVCs (or Pods)</li> </ol>"},{"location":"storage/#reclaim-policy","title":"Reclaim Policy","text":"<p>When you define a reclaim policy on a volume, you tell Kubernetes how it should deal with a PV after the relevant PVC is released. There are two options that can be selected:  </p> <ol> <li>Delete: Default option that will delete the PV and any underlying storage resources on the external system itself once the PVC is released.</li> <li>Retain: This will keep the PV object as well as any underlying data on the external system. However, no PVCs can use it going forward. </li> </ol>"},{"location":"core-concepts/config-maps-secrets/","title":"ConfigMaps & Secrets","text":""},{"location":"core-concepts/config-maps-secrets/#background","title":"Background","text":"<p>Alongside the ability to store and retrieve data, another key capability for applications is the ability to be configured or instantiated via environment variables or commands.  </p> <p>In the traditional monolithic application days, environment variables and configurations were bundled up with the application and deployed as one large object. However, in the cloud-native application model it's important to decouple these for many reasons:  </p> <ol> <li>Environment Flexibility: Decoupling allows the same application to run across different environments (development, staging, production) without code changes. Environment-specific configurations can be applied externally, improving the portability of the application.</li> <li>Scalability and Dynamic Management: When configuration is externalized, it's easier to scale applications horizontally since the configuration can be managed and applied independently. This allows for dynamic reconfiguration in response to changes in load or other factors without redeploying or restarting containers.</li> <li>Security and Sensitive Data Handling: Keeping sensitive configuration data, such as secrets and credentials, separate from the application codebase helps maintain security. It ensures that sensitive data is not exposed within the code and can be securely managed using secrets management tools.</li> <li>Continuous Deployment and Rollbacks: Decoupling facilitates continuous deployment practices by allowing configurations to be updated independently of the application. This separation also simplifies rollback procedures in case a configuration change needs to be reverted without affecting the application version that's running.</li> <li>Maintainability and Clarity: Keeping configuration separate from application code helps maintain a clean codebase and makes it clearer for developers to understand the application logic. It avoids cluttering the application with environment-specific conditionals and settings, making the code easier to maintain and evolve.  </li> </ol> <p>Let's take a look at an example from point #1 there. Imagine you have an application that runs in 3 different environments: <code>dev</code>, <code>perf</code>, and <code>prod</code>. Each environment has different configurations such as credentials, network policies, security policies, etc. In the old world, if you were to package those configurations with the application, you'd end up with three separate images stored in three separate repositories. Any time a developer needs to make an update to the application, they must ensure they update it across all three repos, rebuild all three images, and redeploy all three images.</p> <p><pre><code>flowchart LR\n    subgraph app repos\n        dev[(&lt;b&gt;dev&lt;/b&gt;&lt;br&gt;&lt;tt&gt;- dev credentials&lt;br&gt;- dev network policy&lt;br&gt;- dev security policy)]\n        perf[(&lt;b&gt;perf&lt;/b&gt;&lt;br&gt;&lt;tt&gt;- perf credentials&lt;br&gt;- perf network policy&lt;br&gt;- perf security policy)]\n        prod[(&lt;b&gt;prod&lt;/b&gt;&lt;br&gt;&lt;tt&gt;- prod credentials&lt;br&gt;- prod network policy&lt;br&gt;- prod security policy)]\n    end\n    subgraph app images\n        dev2[&lt;b&gt;dev image]\n        perf2[&lt;b&gt;perf image]\n        prod2[&lt;b&gt;prod image]\n    end\n    subgraph environments\n        dev3[&lt;b&gt;dev]\n        perf3[&lt;b&gt;perf]\n        prod3[&lt;b&gt;prod]\n    end\n    dev --&gt;|builds| dev2\n    perf --&gt;|builds| perf2\n    prod --&gt;|builds| prod2\n    dev2 --&gt;|deployed| dev3\n    perf2 --&gt;|deployed| perf3\n    prod2 --&gt;|deployed| prod3</code></pre> </p> <p>A better way to handle this is by decoupling those configuration values from your application. You build and maintain a single application repository and build &amp; run that single image in all environments. For this to be possible, your applications should be built as plain as possible with as little configuration necessary embedded. The configurations for each environment are then stored separately and applied to the various environments at runtime. </p> <p>In this manner, application code is updated in one repository, one image is used, and configurations are independently managed.</p> <pre><code>flowchart LR\n    subgraph app repo\n        app[(&lt;b&gt;app source code&lt;/b&gt;)]\n    end\n    subgraph app image\n        app2[&lt;b&gt;app image]\n    end\n    subgraph environments\n        dev[&lt;b&gt;dev]\n        perf[&lt;b&gt;perf]\n        prod[&lt;b&gt;prod]\n    end\n    subgraph ConfigMaps\n    dev1[(\"dev CM\")]\n    perf1[(\"perf CM\")]\n    prod1[(\"prod CM\")]\n    end\n    app --&gt;|builds| app2\n    app2 --&gt;|apply config| dev1\n    app2 --&gt;|apply config| perf1\n    app2 --&gt;|apply config| prod1\n    dev1 --&gt; |deployed| dev\n    perf1 --&gt; |deployed| perf\n    prod1 --&gt; |deployed| prod</code></pre>"},{"location":"core-concepts/config-maps-secrets/#configmaps","title":"ConfigMaps","text":"<p>Kubernetes allows this to happen through the use of a ConfigMap (CM). ConfigMaps are used to store non-sensitive information and configuration data such as:  </p> <ol> <li>Hostnames</li> <li>Server configurations</li> <li>Database configurations</li> <li>Account names</li> <li>Environment variables</li> </ol> <p>You should not use ConfigMaps to store sensitive data such as passwords. Secrets should be used for that purpose.</p> <p>Under the covers, ConfigMaps are effectively key-value pairs. Keys are completely arbitrary and can be any name created from letters, numbers, underscores, dashes, and dots. Values can contain anything. As with many other key-value paradigms, they are separated by a colon (<code>key</code>:<code>value</code>). </p> <p>As mentioned above, a (simple) database configuration might look something like this in a ConfigMap definition:</p> <pre><code>hostname: mysql-dev-01\ndb-port: 3306\nusername: vinny\n</code></pre> <p>Data stored in ConfigMaps can be injected into a container in a number of ways:  </p> <ol> <li>As environment variable(s)</li> <li>Arguments in the container's startup command</li> <li>As files in a volume</li> </ol> <p>These methods are all transparent to the application - it has no idea the ConfigMap is even a thing, it just knows it's data is where it's supposed to be. How it got there is an irrelevant mystery.</p>"},{"location":"core-concepts/config-maps-secrets/#environment-variables","title":"Environment Variables","text":"<p>To inject data as environment variables, you created a ConfigMap and map its entries to environment variables inside of the Pod spec template. Once the Pod and underlying container start, the environment variables will appear as standard environment variables for the OS relevant to that container.  </p> <p>Here's how that might look when injecting some database information into a container using a ConfigMap called <code>myCM</code>:  </p> <p><code>myCM</code>: <pre><code>...\ndata:\n  database: mysql-01\n  loc: STL\n  user: vinny\n</code></pre></p> <p>Pod definition: <pre><code>spec:\n  containers:\n  ...\n    env:\n      - name: host\n      valueFrom:\n        configMapKeyRef:\n          name: myCM\n          key: database\n      - name: location\n      valueFrom:\n        configMapKeyRef:\n          name: myCM\n          key: loc\n      - name: user\n      valueFrom:\n        configMapKeyRef:\n          name: myCM\n          key: user\n</code></pre></p> <p>With these configs, this is what the mapping from ConfigMap to container variables would look like:  </p> <ul> <li>database host</li> <li>loc location</li> <li>user user </li> </ul> <p>If you were to login interactively to the container, you would be able to seamlessly view these environment variables:  </p> <pre><code>$ echo $host\nmysql-01\n\n$ echo $location\nSTL\n\n$ echo $user\nvinny\n</code></pre>"},{"location":"core-concepts/config-maps-secrets/#container-startup-commands","title":"Container Startup Commands","text":"<p>This method of injecting data into containers from ConfigMaps is pretty straightforward. In your Pod template YAML, you specify a startup command and insert variables defined from your ConfigMap. Below is an example of inserting the database hostname from above into a startup command for the container. Here is how the Pod YAML might look:</p> <pre><code>...\nspec:\n  containers:\n  - name: my-container-1\n    image: busybox\n    command: [\"/bin/sh\", \"-c\", \"echo Database to use is $(host)\"]\n    env:\n      - name: host\n        valueFrom:\n          configMapKeyRef:\n            name: myCM\n            key: database\n</code></pre> <p>From this Pod definition, when the container starts it will run the following shell command that we defined:</p> <pre><code>echo Database to use is $(host)\n</code></pre> <p>In our config, we mapped <code>host</code> to the <code>database</code> key in <code>myCM</code>, which we defined as having a value of <code>mysql-01</code>. Thus, when the container runs that command, it will produce the following output:</p> <pre><code>Database to use is mysql-01\n</code></pre>"},{"location":"core-concepts/config-maps-secrets/#volumes","title":"Volumes","text":"<p>The most flexible way to leverage ConfigMaps is with volumes. By using them with volumes you can reference entire configuration files and make live updates to them which will be reflected in running containers. The entire process can be summed up in the following steps:</p> <ol> <li>Create a ConfigMap</li> <li>Create a ConfigMap volume in your Pod spec</li> <li>Mount the ConfigMap volume into the container</li> </ol> <pre><code>flowchart LR\n    subgraph ConfigMap\n        cm1[&lt;tt&gt;central = STL]\n        cm2[&lt;tt&gt;west = SF]\n    end\n\n    subgraph Pod\n        cmv[(ConfigMap&lt;br&gt;vol)]\n        subgraph container\n        fs[\"&lt;b&gt;/etc/regions&lt;/b&gt;&lt;br&gt;&lt;tt&gt;-- central&lt;br&gt;-- west\"]\n        end\n    end\nConfigMap --&gt; cmv\ncmv --&gt; fs</code></pre> <p>Here's an example YAML file that would create a Pod called <code>configMapVol</code>, a volume called <code>volMap</code>, and mounts the <code>volMap</code> volume to <code>/etc/regions</code>:  </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: configMapVol\nspec:\n  volumes:\n    - name: volMap\n      configMap:\n        name: my-cm\n  containers:\n    - name: container1\n      image: busybox\n      volumeMounts:\n        - name: volMap\n          mountPath: /etc/regions\n</code></pre> <p>If you were to deploy this Pod and exec into it, you could run an <code>ls</code> command to view the files we defined in the ConfigMap diagram above mounted at <code>/etc/regions</code>:  </p> <pre><code>$ kubectl exec configMapVol -- ls /etc/regions\ncentral\nwest\n</code></pre>"},{"location":"core-concepts/config-maps-secrets/#secrets","title":"Secrets","text":"<p>Secrets are extremely similar in shape and function to ConfigMaps in that they hold configuration data that can be injected into containers at run-time. However, Secrets differ in the fact that they base-64 encode values and are made for storing sensitive data such as tokens, certificates, and passwords.</p> <p>These values are not encrypted by default and can easily be decoded.</p> <p>A standard flow for implementing secrets looks like this:  </p> <p><pre><code>flowchart TD\n    A[Create Secret and persist to cluster store - unencrypted] --&gt; B[Pod is configured to use Secret]\n    B --&gt; C[Secret data is transferred - unencrypted - to the node]\n    C --&gt; D[Node kubelet starts the Pod and its containers]\n    D --&gt; E[Secret is mounted into the container's temp filesystem and decoded into plain text]\n    E --&gt; F[Application consumes Secret]\n    F --&gt; G[Secret is deleted from the node once the Pod is deleted]</code></pre> Additionally here is an example of how a YAML file might look for creating a Secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\ntype: Opaque\ndata:\n  password: UGFzc3dvcmQxMjM=\n  user: dmlubnk=\n\n...\n</code></pre> <p>And here's an example of how to define a Pod and use the Secret as a volume:  </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  volumes:\n  - name: secret-vol\n    secret: \n      secretName: my-secret\n  containers:\n  - name: my-container\n    image: busybox\n    volumeMounts:\n    - name: secret-vol\n      mountPath: /etc/secrets/\n</code></pre> <p>Secrets are mounted as read-only objects in the containers to prevent accidental manipulation.</p>"},{"location":"core-concepts/container-runtime/","title":"Container Runtime","text":""},{"location":"core-concepts/container-runtime/#history","title":"History","text":"<ul> <li> <p>Originally, Docker was the most popular and thus defacto container runtime that Kubernetes was built to orchestrate </p> </li> <li> <p>As Kubernetes grew in popularity, Kubernetes introduced the Container Runtime Interface (CRI) which allowed any runtime to be used as long as they adhered to the Open Container Initiative (OCI). There are two main specs to be aware of: </p> <ul> <li><code>imagespec</code> - how an image should be built</li> <li><code>runtimespec</code> - how a container runtime should be developed </li> </ul> </li> <li> <p>Docker did not support CRI because it was built before the CRI standard was developed</p> </li> <li>Docker introduced the Docker Shim, which was a \u201chacky\u201d way to support Docker on Kubernetes without using the CRI</li> <li> <p>Docker also includes a lot of other features - but notably includes containerd, which is OCI compliant and can be run without Docker</p> </li> <li> <p>As of Kubernetes version <code>1.24</code>, Docker Shim support was removed completely, and containerd was used as the default</p> <ul> <li>However, all Docker images do follow the <code>imagespec</code> OCI standard, so they can continue to be used on Kubernetes with the containerd runtime</li> </ul> </li> </ul>"},{"location":"core-concepts/container-runtime/#containerd","title":"containerd","text":"<ul> <li>Graduated status member of CNCF that can be installed without Docker itself</li> <li>Comes with a CLI tool called <code>ctr</code> which is not very user-friendly and mainly used for debugging containerd</li> </ul>"},{"location":"core-concepts/controllers/","title":"Controllers","text":"<ul> <li> <p>In Kubernetes, a controller is a process that continuously monitors the state of various components and works toward bringing the system into its desired state. </p> </li> <li> <p>For example, the Node Controller is responsible for monitoring Kubernetes Nodes:</p> <ul> <li>It receives heartbeats from the Nodes every 5 seconds - via the Kubernetes API Server</li> <li>If it doesn\u2019t receive a heartbeat from a Node, it gives it a 40-second grace period before marking the Node as <code>Unreachable</code></li> <li>Once a node is marked <code>Unreachable</code>, it gives the Node 5 minutes to come back online before it evicts Pods from that Node and begins scheduling them on a healthy Node (if part of a ReplicaSet).</li> </ul> </li> </ul> <p></p> <ul> <li> <p>The Replication Controller is in charge of monitoring the state of ReplicaSets.  </p> </li> <li> <p>There are many other types of Controllers within Kubernetes (Deployment, Namespace, Endpoint, CronJob, ServiceAccount, Job, StatefulSet, etc\u2026) </p> </li> <li> <p>All of these Controllers are packaged into a single process known as the Kubernetes Controller Manager.</p> </li> </ul>"},{"location":"core-concepts/deployments/","title":"Deployments","text":""},{"location":"core-concepts/deployments/#overview","title":"Overview","text":"<p>The main idea behind Deployments is that you tell Kubernetes the desired state of your application while a looping controller watches your app and continuously attempts to reconcile the actual state of your app with the desired state you previously defined.</p>"},{"location":"core-concepts/deployments/#deployment-spec","title":"Deployment Spec","text":"<p>The way you tell Kubernetes how you want your application to look is through the use of a YAML file (Deployment spec). When you POST the Deployment spec (via <code>kubectl</code>) to the API server, Kubernetes goes through the process of deploying your application to match the desired state and leverages a Deployment controller to continuously watch your application state.  </p> <p>It should be noted that every Deployment object will only manage a single Pod object. If you have an application with more than one Pod, you will need more than one Deployment object. But, a single Deployment object can manage any number of replicas of a given Pod.</p>"},{"location":"core-concepts/deployments/#replicasets","title":"ReplicaSets","text":"<p>Under the covers, Deployments actually leverage a different Kubernetes object to handle Pod scaling and reboots - the ReplicaSet. You should never be managing ReplicaSets directly, but it's good to know they exist and understand the hierarchy of control here. Containers will be wrapped in Pods, which have their scaling and self-healing managed by ReplicaSets, which in turn are managed by Deployments.</p> <pre><code>flowchart TB\n    subgraph Deployment\n        subgraph ReplicaSet\n            Pod1[Pod]\n            Pod2[Pod]\n        end\n    end</code></pre>"},{"location":"core-concepts/deployments/#scaling-and-self-healing","title":"Scaling and Self-Healing","text":"<p>If you deploy a pod by itself (i.e. not via a Deployment), if it dies or fails, the Pod is lost forever.</p> <p>We never \"revive\" Pods; the appropriate way to \"revive\" a failed Pod is to create a new one to replace it.</p> <p>However, with the magic of Deployments, if a Pod that was created via Deployment fails, it will be replaced. Remember that Deployment controllers continuously watch for deviations from your desired state; so if you specified that your application should run 3 Pods and one of the Pods fails, the controller will recognize that actual state (2 Pods) no longer matches desired state (3 Pods), and it will kick off a series of actions to deploy another Pod.</p>"},{"location":"core-concepts/deployments/#rolling-updates","title":"Rolling Updates","text":"<p>This same logic allows seamless, zero-downtime updates for your applications. Let's say you defined your application to have 5 Pods running and labeled it as being <code>v1.2</code>. Your team introduces some new features or implements some bug fixes and creates <code>v1.3</code> of your application. Your next step will be to go in and update your desired state (Deployment spec) from <code>v1.2</code> <code>v1.3</code>. The Deployment controller will then recognize that the actual state (<code>v1.2</code>) no longer matches the desired state (<code>v1.3</code>) and begin the process of spinning down outdated Pods and spinning up new Pods with the new version.  </p> <p>Some things to keep in mind for this to work: your application(s) need(s) to maintain loose coupling and maintain backwards and forwards compatability (cloud native application design pattern). For upgrades or rollbacks to truly have zero-downtime, forward and backword compatibility is a must, as is having clearly defined API specs.</p> <p>There are different rolling update strategies you can employ that specify how to handle rollouts/rollbacks, how many can be spun up or down at once, etc. For more in-depth information on these strategies, refer to the official Kubernetes documentation.</p>"},{"location":"core-concepts/deployments/#rollbacks","title":"Rollbacks","text":"<p>Rollbacks work in the same manner as rolling updates from above but in reverse. Imagine you had an issue with <code>v1.3</code> and need to roll back to <code>v1.2</code>. It's as simple as updating your Deployment spec and letting the Deployment controller notice this change and begin that reconciliation process. Kubernetes let you specify how many revisions (old versions) of your Deployments should be maintained for the purposes of rollbacks. In your Deployment spec, this is defined by the <code>revisionHistoryLimit</code> block.  </p> <p>You can view the update history of a Deployment by running the following command: <pre><code>kubectl rollout history deployment/&lt;deployment-name&gt;\n</code></pre></p>"},{"location":"core-concepts/deployments/#scaling","title":"Scaling","text":"<p>Performing manual scaling operations with Deployments is also super straightforward and can be done in a similar manner to the one above. If you decide you actually want 10 Pods instead of 5, it's as simple as updating your Deployment spec and updating the <code>Replicas</code> block to the desired amount of Pods. Once again, the Deployment controller will notice the variation in states and begin reconciliation.</p>"},{"location":"core-concepts/kube-proxy/","title":"Kube Proxy","text":"<ul> <li> <p>Every Pod can reach every other Pod in a Kubernetes cluster</p> <ul> <li>This is done by implementing a Pod network </li> </ul> </li> <li> <p>A Pod network is a virtual network that spans all Nodes in the cluster </p> </li> <li> <p>Services cannot join Pod networks because they are not running processes with any interfaces </p> </li> <li>Kube Proxy is a process that runs on each Node in the cluster<ul> <li>It searches for any new Services on the cluster and creates local IP rules on each Node when a new Service is found</li> <li>It then forwards any traffic inbound to a given Service to the IP of the Pod(s)</li> </ul> </li> </ul>"},{"location":"core-concepts/kubelet/","title":"Kubelet","text":"<ul> <li> <p>Kubelets are initially responsible for registering a Node with the cluster </p> </li> <li> <p>When the Scheduler determines a Node to place a Pod on, it informs the Kubelet on the Node (via the API Server)</p> <ul> <li>The Kubelet then instructs the container runtime to pull the image and run the instance</li> <li>The Kubelet will then continually monitor the Pod and report status to the API Server</li> </ul> </li> </ul>"},{"location":"core-concepts/kubernetes-api/","title":"API Server","text":""},{"location":"core-concepts/kubernetes-api/#overview","title":"Overview","text":"<p>Kubernetes revolves entirely around its API, which serves as the central nervous system of the platform. Every interaction within Kubernetes\u2014whether it's creating, reading, updating, or deleting resources like Pods and Services\u2014occurs through requests made to the API and processed by the API server.  </p> <p>While <code>kubectl</code> is the go-to command-line tool for sending these requests, they can also be composed programmatically or via specialized API development tools. Regardless of how they're formulated, all requests are funneled to the API server, where they undergo authentication and authorization checks. Once verified, these requests are actioned within the cluster. For instance, a request to create a resource results in its deployment to the cluster, and the object's configuration is then stored in the cluster's datastore. This API-centric design ensures a consistent and secure method for managing the cluster's state and operations.  </p> <pre><code>---\ntitle: API request flow\n---\nflowchart LR\n    client[\"&lt;b&gt;client\"] --&gt;\n    id1{{\"API server&lt;br&gt;auth\"}} --&gt;\n    api{{\"&lt;tt&gt;API\"}} --&gt;\n    sched{{\"&lt;tt&gt;Scheduler\"}} --&gt;\n    etcd{{\"&lt;tt&gt;Cluster&lt;br&gt;store\"}}</code></pre> <p>In Kubernetes, the intricate dance of communication is choreographed with serialization, where objects like Pods and Services are transformed into JSON strings for transmission over HTTP. This transformation occurs both ways: clients such as <code>kubectl</code> serialize objects to JSON when making requests to the API server, and the API server does the same when sending back responses. What's more, Kubernetes captures the serialized state of these objects in the cluster's persistent storage, commonly etcd, ensuring the cluster's state is maintained and recoverable.  </p> <p>Serialization in Kubernetes isn't limited to JSON; it also embraces Protobuf, a schema known for its speed and efficiency, outpacing JSON in performance and scalability. However, Protobuf's complexity makes it less accessible for direct inspection and debugging, which is why it's primarily utilized for internal communications within the cluster, while JSON remains the go-to format for external client interactions.  </p> <p>To smooth out the serialization process, clients specify their supported formats using the Content-Type header in their HTTP requests. For instance, a client that only understands JSON will declare <code>Content-Type: application/json</code>, prompting Kubernetes to respond with data serialized in JSON, adhering to the client's capabilities and preferences.  </p> <p>Kubernetes is a world of API-defined objects, ranging from the familiar Pods and Services to the traffic-managing Ingresses. All these elements are accessible via the API server, which acts as the gateway for interaction with the cluster. You typically use <code>kubectl</code>, the command-line interface, to make requests for these objects. The beauty of Kubernetes extends to its extensibility, allowing third parties to define custom resources that are just as accessible through <code>kubectl</code> and the API server.  </p> <p>When you make a request for an object, the API server springs into action, creating that object within your cluster. But it doesn't just stop there; the API server provides a watch functionality, letting you observe the object as it comes to life. Once the object is up and running, Kubernetes maintains a vigilant watch over it, with the API server offering real-time insight into its current state. Whether you're scaling up with more objects or pruning with deletions, these interactions are all routed through the central hub of the API server.  </p> <pre><code>sequenceDiagram\n    participant user\n    participant kubectl\n    participant api-server\n    participant etcd\n\n    user-&gt;&gt;kubectl: Request Object Creation\n    kubectl-&gt;&gt;api-server: Create Object\n    api-server-&gt;&gt;etcd: Serialize and Persist Object\n    etcd--&gt;&gt;api-server: Confirm Object Stored\n    api-server--&gt;&gt;kubectl: Object Creation Watch\n    kubectl--&gt;&gt;user: Object Status Updates\n    Note over user,etcd: Object is now ready for use\n\n    user-&gt;&gt;kubectl: Query Object State\n    kubectl-&gt;&gt;api-server: Get Object State\n    api-server-&gt;&gt;etcd: Retrieve Object Data\n    etcd--&gt;&gt;api-server: Object Data\n    api-server--&gt;&gt;kubectl: Object State\n    kubectl--&gt;&gt;user: Object State Response</code></pre>"},{"location":"core-concepts/kubernetes-api/#api-server","title":"API Server","text":"<p>The Kubernetes API server is the central hub through which all interactions in the cluster are routed, functioning as the front-end interface for Kubernetes' API. Picture it as the Grand Central Station of Kubernetes \u2014 every command, status update, and inter-service communication passes through the API server via RESTful calls over HTTPS. Here's a snapshot of how it operates:  </p> <ul> <li><code>kubectl</code> commands are directed to the API server, whether it's for creating, retrieving, updating, or deleting Kubernetes objects.</li> <li>Node Kubelets keep an eye on the API server, picking up new tasks and sending back their statuses.</li> <li>The control plane services don't chat amongst themselves directly; they communicate through the API server.  </li> </ul> <p>Zooming in on the API server itself, it's part of the Kubernetes control plane services, often running as a Pod set within the kube-system Namespace on the control plane nodes. For those managing their own Kubernetes clusters, ensuring the high availability and robust performance of the control plane is crucial to keep the API server operational. In contrast, for hosted Kubernetes services, these details are abstracted away from the user.  </p> <p>At its core, the API server's role is to make the Kubernetes API accessible, both to clients within the cluster and to those outside. It secures client connections with TLS encryption and applies various authentication and authorization protocols to vet and process only legitimate requests. All requests, no matter their origin, are subject to the same stringent auth checks.  </p> <p>The \"RESTful\" part of the API means it adheres to a modern web API structure that deals with CRUD-style (Create, Read, Update, Delete) requests via standard HTTP methods like <code>POST</code>, <code>GET</code>, <code>PUT</code>, <code>PATCH</code>, and <code>DELETE</code>.  </p> <p>Typically, the API server is available on ports 443 or 6443, although these can be configured to suit specific needs. The flexibility of the API server ensures that it can cater to different environments while maintaining strict security and reliable service.  </p> <p>The following command will show you the address and port your Kubernetes cluster is exposed on: $ kubectl cluster-info<pre><code>    Kubernetes control plane is running at https://192.168.1.105:6443\n    CoreDNS is running at https://192.168.1.105:6443/api/v1/namespaces/...\n    Metrics-server is running at https://192.168.1.105:6443/api/v1/namespaces/...\n</code></pre></p> <p>In essence, the Kubernetes API server serves as the gateway to the cluster, offering a secure, RESTful interface for interacting with the cluster's state. Operating from the control plane, it necessitates robust availability and performance to ensure swift and reliable handling of requests, embodying the critical link between the user's commands and the cluster's operational response.</p> <p>Note</p> <p>The API Server is the only component in Kubernetes that interacts directly with etcd.</p> <p>If you're unfamiliar with REST, AWS has a great one-pager to get you up to speed.  </p>"},{"location":"core-concepts/kubernetes-api/#api","title":"API","text":"<p>The Kubernetes API is expansive and RESTful, structured to define all Kubernetes resources. Initially, the API was a single, monolithic entity, but as Kubernetes evolved, it transitioned into a more modular form for better manageability, distinguishing between the core group and named groups of API resources.  </p> <p>Core API Group: This group houses the original, fundamental resources such as Pods, Nodes, and Services, accessible under <code>/api/v1</code>. These objects, crucial from the early Kubernetes days, have paths that may vary based on whether they are namespaced (e.g., Pods within a specific namespace) or cluster-wide (e.g., Nodes).  </p> <p>Named Groups: Representing the evolution and expansion of the Kubernetes API, named groups contain newer resources organized by functionality. For instance, the \"apps\" group includes workload-related resources like Deployments and StatefulSets, while \"networking.k8s.io\" focuses on network aspects such as Ingresses. Unlike the core group, resources in named groups are found under <code>/apis/{group-name}/{version}/</code>, reflecting their categorization and versioning.  </p> <p>This division enhances the API's scalability and navigability, facilitating the introduction of new resources. To explore available resources and their groupings, <code>kubectl api-resources</code> provides a comprehensive overview, indicating whether resources are namespaced or cluster-scoped, alongside their shortnames and API group affiliations. This command is instrumental in understanding the API's layout and the scope of resources within a Kubernetes cluster.  </p>"},{"location":"core-concepts/kubernetes-api/#core-group","title":"Core Group","text":"Resource REST Path Pods <code>/api/v1/namespaces{namespace}/pods/</code> Services <code>/api/v1/namespaces/{namespace}services</code> Nodes <code>/api/v1/nodes/</code> Namespaces <code>/api/v1/namespaces</code>"},{"location":"core-concepts/kubernetes-api/#named-groups","title":"Named Groups","text":"Resource REST Path Ingress <code>/apis/networking.k8s.io/v1/namespaces/{namespace}/ingresses/</code> RoleBinding <code>/apis/rbac.authorization.k8s.io/v1/namespaces/{namespace}/rolebindings/</code> ClusterRole <code>/apis/rbac.authorization.k8s.io/v1/clusterroles/</code> StorageClass <code>/apis/storage.k8s.io/v1/storageclasses/</code> <p>Note: This is not all-inclusive, just a few examples.</p> <p>The <code>kubectl api-resources</code> command is a great way to see which API resources are available on your cluster, as well as useful information about them:  </p> $ kubectl api-resources<pre><code>NAME                              SHORTNAMES   APIVERSION     NAMESPACED\nbindings                                       v1             true\ncomponentstatuses                 cs           v1             false\nComponentStatus\nconfigmaps                        cm           v1             true\nConfigMap\nendpoints                         ep           v1             true\nEndpoints\nevents                            ev           v1             true\nlimitranges                       limits       v1             true\nLimitRange\nnamespaces                        ns           v1             false\nNamespace\nnodes                             no           v1             false\npersistentvolumeclaims            pvc          v1             true\nPersistentVolumeClaim\npersistentvolumes                 pv           v1             false\nPersistentVolume\n</code></pre> <p>Truncated for brevity</p> <p>In Kubernetes discussions, you might hear \"resources,\" \"objects,\" and \"primitives\" used as if they're the same. While common usage often blends these terms together, there's a technical distinction worth noting: Kubernetes fundamentally operates on a resource-based API model.  </p> <p>What this means: At its core, the Kubernetes API deals with \"resources.\" These resources are predominantly \"objects\" like Pods, Services, and Ingresses. Yet, the API isn't limited to objects alone; it also encompasses lists and a select few operations. Given that the bulk of resources are indeed objects, the terms \"resource\" and \"object\" are frequently used interchangeably without causing confusion.  </p> <p>Scope of Resources: Kubernetes differentiates between namespaced and cluster-scoped resources. Namespaced resources must reside within a specific Namespace, tailoring their scope and impact to that Namespace. For instance, Pods and Services require a Namespace to exist. Conversely, cluster-scoped resources either span multiple Namespaces or operate outside the Namespace system altogether. Nodes, for example, are cluster-scoped resources existing beyond Namespace boundaries, while ClusterRoles can be tied to specific Namespaces through RoleBindings to apply permissions across the cluster.  </p> <p>To get a grip on the resources available in your cluster and their scope, <code>kubectl api-resources</code> is an invaluable command. It provides a snapshot of all resources, highlighting whether they are namespaced or cluster-scoped, thereby offering insight into how Kubernetes structures and manages its diverse set of resources.  </p>"},{"location":"core-concepts/kubernetes-api/#extending-the-api","title":"Extending the API","text":"<p>Kubernetes offers a powerful framework for managing and automating containerized applications, largely through its predefined set of resources and controllers that observe and manage the state of objects within the cluster. Yet, one of Kubernetes' most compelling features is its extensibility, allowing you to tailor the system to your specific needs by introducing custom resources and controllers.  </p> <p>Extending Kubernetes with Custom Resources and Controllers A vivid example of such extensibility can be observed in the storage domain, where third-party vendors integrate advanced functionalities\u2014like snapshot scheduling\u2014directly into Kubernetes through custom resources. While Kubernetes natively supports storage operations through StorageClasses and PersistentVolumeClaims, these custom resources enable the exposure of vendor-specific features within the same Kubernetes ecosystem.</p> <p>The Extension Blueprint Extending the Kubernetes API typically involves two key steps:</p> <ol> <li> <p>Creating a Custom Controller: This involves developing a controller that operates on your custom logic, watching for changes to your custom resources and ensuring the desired state is achieved within the cluster.  </p> </li> <li> <p>Defining a Custom Resource: Kubernetes facilitates this through the CustomResourceDefinition (CRD) API object. CRDs allow you to define new types of resources that integrate seamlessly with the Kubernetes API, complete with their own RESTful paths. Once defined, these custom resources can be managed via <code>kubectl</code> just like built-in resources, offering a native Kubernetes experience for your custom logic.  </p> </li> </ol> <p>This approach not only enriches the Kubernetes ecosystem with new functionalities but also maintains the uniformity and coherence of the Kubernetes API, ensuring that custom resources are as accessible and manageable as the built-in ones. Through CRDs, Kubernetes embraces an extendable architecture, empowering developers to innovate and expand the platform's capabilities to meet their unique operational requirements.  </p> <p>Info</p> <p>Creating a custom resource doesn't do a whole lot unless you create a custom controller to accompany it. If you're interested in digging into those details, I recommend reading the official Kubernetes documentation on custom controllers.  </p>"},{"location":"core-concepts/namespaces/","title":"Namespaces","text":""},{"location":"core-concepts/namespaces/#overview","title":"Overview","text":"<p>Namespaces are used to partition Kubernetes clusters and provide an easy way to apply policies and quotas at a more granular level.  </p> <p>Namespaces are not intended to be used for secure isolation</p> <p>If you need secure isolation, the best practice is to use multiple clusters.</p>"},{"location":"core-concepts/namespaces/#common-uses-for-namespaces","title":"Common Uses for Namespaces","text":"<p>Namespaces are frequently used to separate environments within a cluster, such as differentiating between development, staging, and production. They can also be used for resource management, applying specific policies or quotas to a subset of the cluster.</p>"},{"location":"core-concepts/namespaces/#built-in-namespaces","title":"Built-in Namespaces","text":"<p>Kubernetes starts with several built-in Namespaces:</p> <ul> <li><code>default</code>: The space where objects are placed if no other Namespace is specified.</li> <li><code>kube-system</code>: For objects created by the Kubernetes system.</li> <li><code>kube-public</code>: Usually reserved for resources that should be visible and readable publicly throughout the whole cluster.</li> <li><code>kube-node-lease</code>: For lease objects associated with nodes which help the Kubelet in determining node health.  </li> </ul> <p>You can run the following command to view all Namespaces on a cluster:  </p> $ kubectl get namespaces<pre><code>    NAME              STATUS   AGE\n    default           Active   22h\n    gmp-public        Active   22h\n    gmp-system        Active   22h\n    kube-node-lease   Active   22h\n    kube-public       Active   22h\n    kube-system       Active   22h\n</code></pre> <p>Your output will vary based on your environment.  </p>"},{"location":"core-concepts/namespaces/#deploying-objects-to-namespaces","title":"Deploying Objects to Namespaces","text":"<p>When deploying objects on Kubernetes you can specify the target Namespace imperatively by adding the <code>-n &lt;Namespace&gt;</code> flag to your command, or declaratively by specifying the Namespace in your YAML file:  </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: my-namespace\n...\n</code></pre>"},{"location":"core-concepts/namespaces/#namespace-creation","title":"Namespace Creation","text":"<p>Creating a new Namespace is as simple as applying a new YAML file:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-new-namespace\n</code></pre> <p>You can also create a Namespace with the kubectl command:</p> <pre><code>kubectl create namespace my-new-namespace\n</code></pre>"},{"location":"core-concepts/overview/","title":"Overview","text":"<p>We'll dive into many of these topics in greater detail later on, but for now here is a primer on Kubernetes to get you started.  </p>"},{"location":"core-concepts/overview/#history","title":"History","text":"<p>Kubernetes itself was born out of Google's experience running billions of containers at scale and managing them with proprietary systems called Borg and Omega. In 2014 Google donated Kubernetes as an open-source project to the Cloud Native Computing Foundation (CNCF).</p>"},{"location":"core-concepts/overview/#20k-foot-view","title":"20K-foot View","text":"<p>At a high-level, Kubernetes is responsible for deploying your applications and dynamically responding to changes to keep your applications running how you intended. Kubernetes runs on any cloud or on-premise datacenter, abstracting away all of the underlying infrastructure and letting you focus on application development. All applications running on Kubernetes must be containerized, and those containers must be running inside of a Pod.  </p> <p>Fundamentally, Kubernetes is a cluster - a group of machines, so to speak. These machines are called nodes in the Kubernetes world and can be cloud instances, virtual machines, physical servers, your laptop, etc.  </p> <p>A Kubernetes cluster consists of a control plane and any number of worker nodes. The control plane is the \"brain\" of Kubernetes and handles things such as scheduling workloads to nodes, implementing the API, and watching for changes that need to be responded to. The worker nodes handle the leg-work of actually running applications.</p> <pre><code>graph TD;\n  subgraph Control Plane\n    kube-apiserver[API Server];\n    etcd[etcd];\n    kube-scheduler[Scheduler];\n    kube-controller-manager[Controller Manager];\n    cloud-controller-manager[Cloud Controller Manager];\n  end;\n  subgraph Node Components\n    kubelet[Kubelet];\n    kube-proxy[Kube Proxy];\n    container-runtime[Container Runtime];\n  end;\n  kube-apiserver --&gt; etcd;\n  kube-apiserver --&gt; kube-scheduler;\n  kube-apiserver --&gt; kube-controller-manager;\n  kube-apiserver --&gt; cloud-controller-manager;\n  kube-scheduler --&gt; kubelet;\n  kube-controller-manager --&gt; kubelet;\n  cloud-controller-manager --&gt; kubelet;\n  kubelet --&gt; kube-proxy;\n  kubelet --&gt; container-runtime;\n  kube-proxy --&gt; container-runtime;</code></pre>"},{"location":"core-concepts/overview/#api-server","title":"API Server","text":"<p>Speaking of, the API server is the central component for all communication for all components in Kubernetes.  </p> <p>Any communication inbound or outbound to/from the Kubernetes cluster must be routed through the API server.</p>"},{"location":"core-concepts/overview/#etcd","title":"etcd","text":"<p>The control plane, like many aspects of Kubernetes, exists in a stateless manner. However, <code>etcd</code> does not - it persistently stores the state of the cluster and other configuration data.  </p> <p><code>etcd</code> is installed on every control plane node by default for high-availability. However, it does not tolerate split-brain scenarios and will prevent updates to the cluster in such states - but it will still allow applications to run in those scenarios.</p> <p>Every result you see when you a run <code>kubectl get</code> command is actually data returned from <code>etcd</code> (via the API Server).</p>"},{"location":"core-concepts/overview/#controllers","title":"Controllers","text":"<p>Kubernetes consists of many different controllers, which are essentially background loops that watch for changes to the cluster (and alert when things don't match up so other components can take action). All controllers are managed and implemented by a higher-level component called the controller manager. </p> <p>The following logic is at the core of what Kubernetes is and how it works:  </p> <pre><code>flowchart LR\n    subgraph ControlLoops\n    A(Obtain&lt;br&gt;&lt;b&gt;desired&lt;/b&gt; state)\n    B(Observe&lt;br&gt;&lt;b&gt;current&lt;/b&gt; state)\n    end\n    B &lt;-.-&gt; api(API Server)\n    A &lt;-.-&gt; etcd[(etcd)]\n    ControlLoops --&gt; C{current &lt;br&gt;=&lt;br&gt; desired?}\n    C --&gt;|Yes| ControlLoops\n    C --&gt;|No| E[Take action]</code></pre>"},{"location":"core-concepts/overview/#declarative-model","title":"Declarative Model","text":"<p>Key to truly mastering Kubernetes is the concept of the declarative model. You tell Kubernetes how you want your application to look and run (how many replicas, which image to use, network settings, commands to run, how to perform updates, etc.), and it's Kubernetes job to ensure that happens. You \"tell\" Kubernetes through the use of manifest files written in YAML.  </p> <p>You take those manifest files and <code>POST</code> them to the Kubernetes API server (typically through the use of <code>kubectl</code> commands). The API server will then authenticate the request, inspect the manifest for formatting, route the request to the appropriate controller (i.e. if you've defined a manifest file for a Deployment, it will send the request to the Deployments controller), and then it will record your desired state in the cluster store (remember, <code>etcd</code>). After this, the relevant controller will get started on performing any tasks necessary to get your application into its desired state.  </p> <p>After your application is up and running, controllers begin monitoring its state in the background and ensuring it matches the desired state in <code>etcd</code> (see simple logic diagram above).</p>"},{"location":"core-concepts/overview/#primitives","title":"Primitives","text":"<p>Many of these primitives will not make sense until you read through the various sections of this guide; but this will be a good diagram to refer back to:  </p> <pre><code>graph TB\n    subgraph Controllers\n        Deployment\n        ReplicaSet\n        StatefulSet\n    end\n    subgraph Services\n        Service\n        Ingress\n    end\n    subgraph Volumes\n        PersistentVolumeClaim\n        PersistentVolume\n    end\n    subgraph Config\n        ConfigMap\n        Secret\n    end\n    subgraph Policies\n        NetworkPolicy\n        HorizontalPodAutoscaler\n        RBAC\n    end\n    Pod[\"&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Pod&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;\"]\n    Deployment --&gt;|manages| ReplicaSet\n    ReplicaSet --&gt;|controls| Pod\n    Service --&gt;|routes traffic to| Pod\n    PersistentVolumeClaim --&gt;|claims| PersistentVolume\n    Pod --&gt;|mounts| PersistentVolumeClaim\n    Ingress --&gt;|exposes| Service\n    Pod --&gt;|uses| ConfigMap\n    Pod --&gt;|uses| Secret\n    NetworkPolicy --&gt;|controls access to| Pod\n    HorizontalPodAutoscaler --&gt;|scales| Deployment\n    RBAC --&gt;|secures| Pod\n    StatefulSet --&gt;|manages|Pod</code></pre>"},{"location":"core-concepts/pods/","title":"Pods","text":"<p>Pods are the atomic unit of scheduling in Kubernetes. As virtual machines were in the VMware world, so are Pods in the world of Kubernetes. Every container running on Kubernetes must be wrapped up in a Pod. Think of a Pod as a wrapper for your application\u2019s container(s), similar to how a virtual machine encapsulates an entire operating system and its applications.  </p> <p>The most simple implementation of this are single-container Pods - one container inside one Pod. However there are certain instances where multi-container Pods make sense.</p> <p>It's important to note that when you scale up/down applications in Kubernetes, you're not doing so by adding/removing containers directly - you do so by adding/removing Pods.</p>"},{"location":"core-concepts/pods/#atomic","title":"Atomic","text":"<p>Pod deployment is atomic in nature - a Pod is only considered Ready when all of its containers are up and running. Either the entire Pod comes up successfully and is running, or the entire thing doesn't - there are no partial states.</p>"},{"location":"core-concepts/pods/#lifecycle","title":"Lifecycle","text":"<p>Pods are designed to be ephemeral in nature. Once a Pod dies, it's not meant to be restarted or revived. Instead, the intent is to spin up a brand new Pod in the failed ones place (based off of your defined manifest). Further, Pods are immutable and should not be changed once running. If you need to change your application, you update the configuration via the manifest and deploy a new Pod.  </p> <p>Pods also follow a defined restart policy in order to handle container failures:  </p> <ul> <li><code>Always</code>: The container is restarted even if it exits successfully.</li> <li><code>OnFailure</code>: The container is only restarted if it exits with an error.</li> <li><code>Never</code>: The container is never restarted, regardless of the exit status.</li> </ul>"},{"location":"core-concepts/pods/#shared-resources-and-communication","title":"Shared Resources and Communication","text":"<p>Containers within a Pod share an IP address and port space, allowing them to communicate using localhost. They can also share volumes, providing a common space for storage that persists across container restarts and simplifies data sharing.</p>"},{"location":"core-concepts/pods/#multi-container-pods","title":"Multi-container Pods","text":"<p>As mentioned above, the simplest way to run an app on Kubernetes is to run a single container inside of a single Pod. However, in situations where you need to tightly couple two or more functions you can co-locate multiple containers inside of the same pod. One such example would be leveraging the sidecar pattern for logging wherein the main container dumps logs to a supporting container that can sanitize and format the logs for consumption. This frees up the main container from having to worry about formatting logs.  </p>"},{"location":"core-concepts/pods/#affinity-and-anti-affinity","title":"Affinity and Anti-affinity","text":"<p>Kubernetes offers ways to control where Pods are placed relative to other Pods or to specific nodes. Pod affinity rules attract Pods to nodes with specific labels, while anti-affinity rules repel them, ensuring high availability and optimal resource utilization.</p>"},{"location":"core-concepts/pods/#resource-requests-and-limits","title":"Resource Requests and Limits","text":"<p>Pods can specify the amount of CPU and memory required (requests) and the maximum that can be consumed (limits). This helps Kubernetes make better scheduling decisions and manage system resources efficiently.</p>"},{"location":"core-concepts/pods/#probes-readiness-and-liveness","title":"Probes: Readiness and Liveness","text":"<p>Kubernetes uses readiness probes to know when a Pod is ready to start accepting traffic and liveness probes to know when to restart a container:</p> <ul> <li>Readiness probes protect your service\u2019s availability by not sending traffic to Pods that aren\u2019t ready.</li> <li>Liveness probes help maintain a healthy application state by restarting containers that fail the defined check.</li> </ul>"},{"location":"core-concepts/pods/#init-containers","title":"Init Containers","text":"<p>Init containers run before the application containers and are used to perform setup tasks or wait for some condition before the app starts. They run to completion and must exit before the main application containers start.</p>"},{"location":"core-concepts/pods/#quality-of-service-qos-classes","title":"Quality of Service (QoS) Classes","text":"<p>Pods are assigned QoS classes based on their resource requests and limits:</p> <ul> <li><code>Guaranteed</code>: Pods with defined and equal requests and limits, ensuring the highest priority.</li> <li><code>Burstable</code>: Pods with defined requests lower than limits, giving some flexibility.</li> <li><code>BestEffort</code>: Pods with no requests or limits, receiving the lowest priority.</li> </ul>"},{"location":"core-concepts/pods/#pod-disruption-budgets","title":"Pod Disruption Budgets","text":"<p>Pod Disruption Budgets (PDBs) allow you to ensure that a minimum number of Pods are always available during voluntary disruptions, such as node maintenance, safeguarding against outages.</p>"},{"location":"core-concepts/pods/#annotations-and-labels","title":"Annotations and Labels","text":"<p>Labels are key/value pairs for organizing and selecting groups of Pods, while annotations provide a way to store additional metadata to help manage applications.</p>"},{"location":"core-concepts/pods/#service-accounts","title":"Service Accounts","text":"<p>Pods use service accounts to authenticate to the Kubernetes API, which is crucial for Pods that need to interact with the API for automation and orchestration.</p>"},{"location":"core-concepts/pods/#summary","title":"Summary","text":"<p>Pods are the building blocks of a Kubernetes application. They ensure that containers run in a controlled, isolated, and secure environment with all the necessary configurations and resources. While Pods are inherently transient, their patterns and behaviors are foundational to how applications are designed and managed in Kubernetes. For more granular control and advanced features, refer to the official Kubernetes documentation.</p>"},{"location":"core-concepts/scheduler/","title":"Scheduler","text":"<ul> <li> <p>The Scheduler is only responsible for deciding which Pods go on which nodes</p> <ul> <li>It doesn\u2019t actually place the Pods on the Nodes (the Kubelet does that) </li> </ul> </li> <li> <p>There are various criteria that the Scheduler uses to determine which Node to place a Pod on:</p> <ul> <li>Resource requirements</li> <li>Application requirements  </li> </ul> </li> <li> <p>The Scheduler uses a ranking system (0-10) to determine which Node is best; taking into account resources, taints, tolerations, affinity, etc.</p> </li> </ul>"},{"location":"core-concepts/scheduling/","title":"Scheduling","text":"<ul> <li>Every Pod spec definition has a <code>nodeName</code> field that is typically not set, but Kubernetes adds it automatically when the Pod is created</li> <li>When the Scheduler reviews Pods, it finds Pods that do not have that field set and those are the candidates for scheduling</li> <li>It then runs the scheduling algorithm to determine which Node to place the Pod on and populates the <code>nodeName</code> field</li> </ul>"},{"location":"core-concepts/scheduling/#manual-scheduling","title":"Manual Scheduling","text":"<ul> <li>You can manually set the specific Node in your Pod definition:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: nginx\n  nodeName: k8s02 # you can specify the Node you want here\n</code></pre> <p>The <code>nodeName</code> field cannot be updated once the Pod is running. If you want to update the Node a running Pod resides on, you have to create a <code>Binding</code> object and send a POST request to the Binding API of the Pod,</p>"},{"location":"core-concepts/scheduling/#taints-and-tolerations","title":"Taints and Tolerations","text":"<ul> <li>Taints are specific to Nodes</li> <li>Tolerations are specific to Pods</li> <li>Taints on a Node say \u201conly these certain Pods can be scheduled here\u201d</li> <li>Tolerations on a Pod say \u201cyou can tolerate a Node that has this given taint\u201d</li> </ul>"},{"location":"core-concepts/scheduling/#node-selectors","title":"Node Selectors","text":"<ul> <li>Node selectors specify which Node you want a Pod to run on based on labels you place on the Node itself<ul> <li>i.e. if you label a Pod as <code>disktype=ssd</code>, implying it can only be scheduled on Nodes with that label, you can specify in the Pod definition YAML:</li> </ul> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  nodeSelector:\n    disktype: ssd # label here\n</code></pre> <p>Note there are limitations here as you cannot use complex operators like <code>IS NOT</code> , <code>OR</code> , <code>EXISTS</code> , etc.</p>"},{"location":"core-concepts/scheduling/#node-affinity","title":"Node Affinity","text":"<ul> <li>Node affinity does let you use complex operators</li> <li>Node affinity is\u00a0a set of rules used by the Scheduler to determine where a Pod can be placed.</li> <li>The rules are defined using custom labels on Nodes and label selectors specified in Pods.</li> <li>Node affinity allows a Pod to specify an affinity (or anti-affinity) towards a group of Nodes it can be placed on.</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - ssd\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"core-concepts/scheduling/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Among other things, the Scheduler takes into account the resources required by the Pod and the resources available on the Node(s) when attempting to schedule a Pod</li> <li>You can specify how much CPU and memory to request for your Pod when defining it<ul> <li>This is called a Resource Request</li> </ul> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp-color\n  labels:\n    name: simple-webapp-color\nspec:\n  containers:\n  - name: simple-webapp-color\n    image: nginx\n    ports:\n      - containerPort: 8080\n    resources:\n      requests:\n        memory: \"4Gi\"\n        cpu: 2\n</code></pre> <ul> <li>By default, a Pod has no limits on the amount of resources it can consume from a Node</li> <li>You can also specify a limit how many resources your Pod can consume when defining it</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp-color\n  labels:\n    name: simple-webapp-color\nspec:\n  containers:\n  - name: simple-webapp-color\n    image: nginx\n    ports:\n      - containerPort: 8080\n    resources:\n      requests:\n        memory: \"2Gi\"\n        cpu: 4\n      limits:\n        memory: \"8Gi\"\n        cpu: 10\n</code></pre> <ul> <li>When a Pod tries to go beyond the CPU limit, the Node will throttle the CPU usage of the Pod</li> <li> <p>A Pod can use more memory than it\u2019s limit however and will throw an OOM (Out of Memory) error if it happens frequently</p> </li> <li> <p>To ensure all Pods have some sort of limit set, you can introduce a LimitRange, which ensures all created Pods within a Namespace have certain default values without having to set them at the Pod definition-level</p> </li> </ul> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-resource-constraint\nspec:\n  limits:\n  - default: # limit\n      cpu: 500m\n    defaultRequest: # request\n      cpu: 500m\n    max: # limit\n      cpu: \"1\"\n    min: # request\n      cpu: 100m\n    type: Container\n</code></pre> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\nname: memory-resource-constraint\nspec:\nlimits:\n- default: # limit\n  memory: 1Gi\ndefaultRequest: # request\n  memory: 1Gi\nmax: # limit\n  memory: 1Gi\nmin: # request\n  memory: 500Mi\ntype: Container\n</code></pre> <p>Changing a LimitRange will not affect running Pods - only newly created ones</p> <ul> <li>To set limits at a Namespace-level, you use Resource Quotas</li> </ul> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: my-resource-quota\nspec:\n  hard:\n    requests.cpu: \"4\"\n    requests.memory: 4Gi\n    limits.cpu: \"10\"\n    limits.memory: 10Gi\n</code></pre>"},{"location":"core-concepts/scheduling/#daemonsets","title":"DaemonSets","text":"<ul> <li> <p>DaemonSets run a single copy of each Pod on every Node of the cluster</p> <ul> <li>If a new Node is added to the cluster, a new Pod is put onto the new Node</li> </ul> </li> <li> <p>Use cases might be:</p> <ul> <li>Monitoring agent</li> <li>Logging agent</li> <li>Networking solutions which require an agent on every Node</li> </ul> </li> <li> <p>The <code>kube-proxy</code> is actually a DaemonSet as well</p> </li> <li> <p>DaemonSets ensures a Pod runs on every single Node in the cluster by using Node Affinity rules</p> </li> </ul> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetdata:\n  name: monitoring-daemon\nspec:\n  selector:\n    matchLabels:\n      app: monitoring-agent\n  template:\n    metadata:\n      labels:\n        app: monitoring-agent\n     spec:\n       containers:\n       - name: monitoring-agent-container\n         image: monitoring-agent-software\n</code></pre>"},{"location":"core-concepts/scheduling/#multiple-schedulers","title":"Multiple Schedulers","text":"<ul> <li>You can write your own Scheduler program and deploy it as the default scheduler or supplemental schedulers</li> <li>When defining and deploying a Pod, you can instruct it to leverage a specific Scheduler</li> </ul> <pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- schedulerName: my-scheduler\nleaderElection: # if running multiple masters\n  leaderElect: true\n  resourceNamespace: kube-system\n  resourceName: lock-object-my-scheduler\n</code></pre> <ul> <li>You can deploy an additional Scheduler as a Pod</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-custom-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-scheduler\n    - --address=127.0.0.1\n    - --kubeconfig=/etc/kubernetes/scheduler.conf\n    - --config=/etc/kubernetes/my-scheduler-config.yaml\n\n    image: k8s.grc.io/kube-scheduler-amd64:v1.11.3\n    name: kube-scheduler\n</code></pre> <ul> <li> <p>You can also deploy it as a Deployment</p> </li> <li> <p>To configure a Pod to use a custom Scheduler, we define it in the Pod definition:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - image: nginx\n    name: nginx-container\n  schedulerName: my-custom-scheduler\n</code></pre> <ul> <li>To view which Scheduler was used for Pods you can look at events by running <code>kubectl get events -o wide</code></li> </ul>"},{"location":"core-concepts/services/","title":"Services","text":""},{"location":"core-concepts/services/#overview","title":"Overview","text":"<p>As mentioned in the Deployments section, Pods will likely be spinning up and down a lot in your environment throughout the course of updates, rollbacks, failures, etc. As such, it's never a good idea for any client to connect directly to a Pod. Pods are there one minute, gone the next - awfully unreliable in and of themselves.  </p> <p>This is where Services come in. Services provide stable, long-lived connection points for clients to connect to. They also maintain a list of Pods to route to and provide basic load-balancing capabilities. With Services, the underlying Pods can come and go, but any client should be able to maintain open communication with the application as the Service provides the logic to know which Pods are healthy and where to route traffic.  </p> <pre><code>flowchart LR\n    client --&gt; SVC[Service]\n    SVC --&gt; Pod1[Pod]\n    SVC --&gt; Pod2[Pod]\n    SVC --&gt; Pod3[Pod]\n    SVC --&gt; Pod4[Pod]\n\n    subgraph Deployment\n        Pod1\n        Pod2\n        Pod3\n        Pod4\n    end</code></pre>"},{"location":"core-concepts/services/#labels-and-selectors","title":"Labels and Selectors","text":"<p>So how does that work? How do Services know which Pods they should be sending traffic to? The short answer is labels and selectors. In essence, when you define a Service, you specify labels and selectors that - when matched with the same ones on Pods - will route traffic to them.  </p> <p>As an example, image you want to put a stable Service in front of series of Pods that make up your shopping application. When you defined the Deployment of the application you listed the following labels and selectors for the Pods: <code>env=prod</code> and <code>app=shop</code>. Now, when you set up this new Service, you used those same labels in it's YAML definition. The new Service will find all Pods on the cluster with those same labels and is now in charge of routing traffic to them.  </p> <p>Similar to other Kubernetes objects, the Services controller will continually monitor new Pods labels and continually update it's \"list\" (more on that list later) of Pods to route to.  </p> <pre><code>flowchart LR\n    SVC[&lt;b&gt;Service&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop] --&gt; Pod1[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop]\n    SVC --&gt; Pod2[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop]\n    SVC --&gt; Pod3[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop]\n    SVC ~~~ Pod4[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=dev&lt;br&gt;app=shop]</code></pre> <p>One thing to note is that Pods can have extra labels and still be managed by the Service if it's other labels still match. As a concrete example, both of the Pods below will still have traffic routed to them, even though one of them has a label that the Service does not.</p> <pre><code>flowchart LR\n    SVC[&lt;b&gt;Service&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop] --&gt; Pod1[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop&lt;br&gt;cur=usd]\n    SVC --&gt; Pod2[&lt;b&gt;Pod&lt;/b&gt;&lt;tt&gt;&lt;br&gt;env=prod&lt;br&gt;app=shop]</code></pre>"},{"location":"core-concepts/services/#endpointslices","title":"EndpointSlices","text":"<p>As mentioned above, as Pods are spinning up and down, the Service will keep an updated list of Pods with the given labels and selectors. How it does this is through the use of EndpointSlices, which are effectively just dynamic lists of healthy Pods that match a given label selector.  </p> <p>Any new Pods that are created on the cluster that match a Service's label selector will automatically be added to the given Service's EndpointSlice object. When a Pod disappears (fails, node goes down, etc.) it will be removed from the EndpointSlice. The net result is that the Service's EndpointSlice should always be up to date with a list of healthy Pods that the Service can route to.  </p>"},{"location":"core-concepts/services/#service-types","title":"Service Types","text":""},{"location":"core-concepts/services/#clusterip","title":"ClusterIP","text":"<p>Kubernetes supports different types of Services, but the default type is ClusterIP, which is only accessible from inside the cluster. Any time you create a Service in Kubernetes it will automatically get a ClusterIP that's registered in the cluster's internal DNS service (more on the DNS service in a different section). Every single Pod on a cluster leverages the cluster's DNS service - which results in all Pods being able to resolve Service names to ClusterIPs.  </p>"},{"location":"core-concepts/services/#nodeport","title":"NodePort","text":"<p>Another type of Service that Kubernetes supports is called NodePort. This is very similar to ClusterIP but adds the ability for external access on a dedicated port on every node in the cluster. NodePort intentionally uses high-numbered ports (30000 - 32767) to avoid clashing with common ports. To access a NodePort Service from an external client, you simply direct traffic to the IP address of any node in the cluster on the given port. The Service will then route the request to the appropriate Pod based on it's list of healthy ones in it's EndpointSlice object.  </p> <p>Here's a sample definition file of a NodePort that's exposing an application on port 80 via NodePort:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\nspec:\n  type: NodePort\n  ports:\n  - targetPort: 80 # port exposed on the Pod/container\n    port: 80 # port exposed on the Service\n    nodePort: 30008 # port exposed on the Node\n  selector:\n    app: myapp\n    type: front-end\n</code></pre> <p>Only <code>port</code> is required in the configuration for <code>spec.ports</code>. <code>targetPort</code> is assumed to be the same as port if not specified. <code>nodePort</code> is auto-assigned if not specified.</p> <pre><code>graph LR\n    subgraph Node\n        NodePortService[&lt;b&gt;NodePort Service&lt;/b&gt;&lt;br&gt;NodePort: 30008&lt;br&gt;Port: 80]\n        Pod[&lt;b&gt;Pod&lt;/b&gt;&lt;br&gt;TargetPort: 80]\n        NodePortService -.-|port 80| Pod\n    end\n    User -.- |port 30008| NodePortService</code></pre> <p></p>"},{"location":"core-concepts/services/#loadbalancer","title":"LoadBalancer","text":"<p>If you're running your Kubernetes cluster on a public cloud environment you can leverage a LoadBalancer Service. This will provision an internet-facing load-balancer that you can leverage to send traffic to your Service. For more specifics on this type of Service, refer to the official Kubernetes documentation.</p>"},{"location":"core-concepts/statefulsets/","title":"StatefulSets","text":""},{"location":"core-concepts/statefulsets/#overview","title":"Overview","text":"<p>StatefulSets are a Kubernetes feature designed to manage applications that need to remember their state, such as databases or systems that keep data consistent across pod restarts. They offer each pod a stable identity and storage that sticks around even when pods are rescheduled to different machines in the cluster.  </p> <p>It's easy to compare StatefulSets with Deployments given they are v1 API objects and follow the controller architecture - but there are notable differences. StatefulSets are Kubernetes tools for running and managing applications that need to remember who they are and what they know\u2014think of them like memory keepers for your apps, such as databases that need to recall data after a reboot. Unlike Deployments that are more about stateless apps (think of them as forgetful but easily replaceable), StatefulSets make sure each of their Pods has a consistent name, network identity, and storage, even if they move around in the cluster. This makes StatefulSets perfect for when your app's individual identity and history are crucial for running smoothly.  </p> <p>StatefulSets can guarantee Pod names, volume bindings, and DNS hostnames across reboots - whereas Deployments cannot. Below are two diagrams that illustrate this point:  </p> <p><pre><code>---\ntitle: Node Replacement w/ Deployments\n---\nflowchart LR\n    subgraph _\n    dep-pod[\"&lt;b&gt;my-pod1&lt;/b&gt;&lt;br&gt;&lt;tt&gt;10.0.0.5\"]\n    vb1[(&lt;b&gt;myVol)]\n    dep-pod -.- vb1\n    end\n    _ --&gt; pf[/\"Pod/node failure\"/]\n    subgraph __\n    dep-pod2[\"&lt;b&gt;my-pod2&lt;/b&gt;&lt;br&gt;&lt;tt&gt;10.0.0.9\"]\n    end\n    pf --&gt;|\"replace failed Pod\"| __</code></pre> </p> <pre><code>---\ntitle: Node Replacement w/ StatefulSets\n---\nflowchart LR\n    subgraph _\n    ss-pod[\"&lt;b&gt;my-pod1&lt;/b&gt;&lt;br&gt;&lt;tt&gt;10.0.0.5\"]\n    vb1[(&lt;b&gt;myVol)]\n    ss-pod -.- vb1\n    end\n    _ --&gt; pf[/\"Pod/node failure\"/]\n    subgraph __\n    ss-pod2[\"&lt;b&gt;my-pod1&lt;/b&gt;&lt;br&gt;&lt;tt&gt;10.0.0.5\"]\n    vb2[(&lt;b&gt;myVol)]\n    ss-pod2 -.- vb2\n    end\n    pf --&gt;|\"replace failed Pod\"| __</code></pre> <p>Notice how with a Deployment, when a Pod is replaced it comes up with a new name, IP address, and its volume is no longer bound to it. With StatefulSets, the new Pod comes up looking exactly the same as the previous failed one.  </p> <p>Below is a typical YAML file for defining a StatefulSet:  </p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: my-sts\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n    serviceName: \"my-sts\"\n    replicas: 5\n    template: \n      metadata:\n        labels:\n          app: nginx\n      spec:\n        containers:\n        - name: nginx-container\n          image: nginx:latest\n\n...\n</code></pre> <p>In this example, the name of the StatefulSet is <code>my-sts</code> and it defines 5 Pod replicas that will run the latest version of the NGINX image. Once you post this to the API server (via <code>kubectl</code>), the definition will be persisted to the cluster store (<code>etcd</code>), replicas will be assigned to nodes, and the StatefulSet controller will begin monitoring the state of the cluster to ensure observed = desired.  </p>"},{"location":"core-concepts/statefulsets/#naming","title":"Naming","text":"<p>One nice feature of StatefulSets is that all Pods managed by them get a predictable name. The format of the names given to Pods managed by StatefulSets is <code>&lt;StatefulSet name&gt;-&lt;integer&gt;</code>. The integer begins with 0 and increases each time a new Pod from this StatefulSet is deployed. So in the example of <code>my-sts</code> above, the Pods would have the names of <code>my-sts-0</code>, <code>my-sts-1</code>, <code>my-sts-2</code>, <code>my-sts-3</code>, etc.</p> <p>StatefulSet names need to be valid DNS names</p>"},{"location":"core-concepts/statefulsets/#order-of-creationdeletion","title":"Order of Creation/Deletion","text":"<p>StatefulSets in Kubernetes are all about order and precision. They initiate and terminate Pods one at a time, following a strict sequence. This approach guarantees that each Pod is fully operational and ready to handle requests before the next Pod in the sequence is brought to life. Unlike the more free-form Deployments, which may initiate a bunch of Pods at once through a ReplicaSet\u2014potentially tripping over themselves with race conditions\u2014StatefulSets are the thoughtful orchestrators ensuring each Pod gets the attention it needs to start or stop without a rush.  </p> <pre><code>---\ntitle: Deploying a StatefulSet\n---\nflowchart \n    subgraph StatefulSet\n        pod1[&lt;b&gt;&lt;tt&gt;my-sts-0] --&gt; |\"waiting for running and ready\"|pod2[&lt;b&gt;&lt;tt&gt;my-sts-1] --&gt; |\"waiting for running and ready\"|pod3[&lt;b&gt;&lt;tt&gt;my-sts-2] --&gt; |\"waiting for running and ready\"|pod4[&lt;b&gt;&lt;tt&gt;...]\n    end</code></pre> <p>StatefulSets in Kubernetes not only ensure a methodical boot-up but also adhere to a careful scaling strategy, both up and down. For instance, when scaling from five to seven replicas, the StatefulSet will sequentially initiate each new Pod and ensure it's fully operational before moving on to the next. Conversely, during scale-down, the StatefulSet will remove Pods starting from the highest index, allowing each to decommission completely before proceeding. This step-by-step approach is critical for applications like databases, where simultaneous termination could lead to data loss.  </p> <p>StatefulSets also offer mechanisms like the <code>terminationGracePeriodSeconds</code> to fine-tune this process, ensuring no data is compromised. Moreover, unlike Deployments which rely on a ReplicaSet for managing replicas, StatefulSet controllers handle scaling and self-healing autonomously, ensuring stateful applications maintain their integrity and data throughout their lifecycle.  </p> <p>Deleting a StatefulSet does not terminate Pods in order</p> <p>If you want to terminate StatefulSet Pods in order, consider scaling to 0 replicas before deleting the StatefulSet.</p>"},{"location":"core-concepts/statefulsets/#statefulsets-and-volumes","title":"StatefulSets and Volumes","text":"<p>StatefulSets in Kubernetes are intrinsically tied to their volumes, which form an integral part of the Pods' state. Each Pod in a StatefulSet is bound to its distinct volumes, which are created simultaneously with the Pod and bear unique identifiers linking them directly to their respective Pods. Thanks to the Persistent Volume Claim (PVC) system, these volumes enjoy a separate lifecycle from the Pods, ensuring their preservation across Pod failures and deletions. When a StatefulSet Pod is terminated or fails, its volumes remain intact, ready to be reattached to any new Pod that takes its place, even if that Pod spins up on a different node within the cluster.  </p> <p>Scaling down a StatefulSet doesn't affect the existence of these volumes. If a Pod is removed during a scale-down, its dedicated volume waits patiently to be reconnected to a new Pod that may be created during a scale-up, ensuring data persistence and consistency. This feature is particularly crucial for safeguarding data in stateful applications; even if you mistakenly delete a Pod, the data is not lost, as the underlying volume can be reattached to a new Pod, effectively rescuing the situation.  </p>"},{"location":"core-concepts/statefulsets/#handling-failures","title":"Handling Failures","text":"<p>The StatefulSet controller in Kubernetes meticulously monitors the cluster's status, making sure the current state aligns with the intended setup. Consider a StatefulSet with five replicas; if one, say <code>my-sts-4</code>, goes down, the controller promptly replaces it, ensuring it retains the same name and rebinds it to the original volumes. </p> <p>But, complications arise if the failed Pod makes a comeback post-replacement and suddenly you've got twin Pods vying for the same volume. To avoid this, the StatefulSet controller handles failures with extra caution.</p> <p>When it comes to node failures, the controller faces a tricky challenge. If a node goes silent, it's hard to tell if it's down for good or just temporarily unreachable due to network issues, a kubelet crash, or a reboot. Since the controller can't guarantee that a termination command will reach a Pod on an unresponsive node, it hesitates to substitute Pods until it's certain of the node's fate. This precaution means that when a node appears to be down, manual intervention is usually necessary before Kubernetes will venture to replace any Pods that were running on it. This cautious approach ensures data integrity at the cost of requiring human oversight in ambiguous failure scenarios.</p>"},{"location":"core-concepts/statefulsets/#dns","title":"DNS","text":"<p>StatefulSets cater to applications that demand reliability and persistence in their Pods. This predictability extends to other applications and services that might need to establish direct connections with specific Pods. To enable these direct connections, StatefulSets employ a headless Service, which provides a stable DNS entry for each Pod based on its unique, predictable hostname. As a result, other components within the ecosystem can retrieve the entire roster of Pod hostnames via DNS queries, allowing for precise and direct Pod communications.  </p> <p>Below is a snippet of YAML that shows a headless Service being defined:  </p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  clusterIP: None  # this is the headless Service\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sts-mysql\nspec:\n  serviceName: mysql  # this is the governing Service\n</code></pre> <p>Headless Services in Kubernetes are essentially standard Service objects that lack a dedicated IP address, achieved by setting <code>spec.clusterIP</code> to <code>None</code>. This Service transforms into what is known as a governing Service for a StatefulSet when it is referenced in the StatefulSet's configuration under <code>spec.serviceName</code>.</p> <p>Linking a headless Service to a StatefulSet in this way prompts the Service to create DNS SRV records for each Pod that fits the headless Service's label selector criteria. This setup allows other Pods and applications within the cluster to discover and connect to the StatefulSet's Pods by querying the DNS for the headless Service's name. To leverage this feature, applications will need to be specifically coded to perform such DNS lookups and handle connections to the StatefulSet members dynamically.</p>"},{"location":"core-concepts/statefulsets/#summary","title":"Summary","text":"<p>This section contained a lot of \"in the weeds\" information and probably warrants a quick summary. StatefulSets within Kubernetes serve as a robust solution for deploying and managing state-persistent applications. StatefulSets are equipped with the ability to self-repair, scale both upwards and downwards, and conduct orderly rollouts - although rollbacks typically require a manual process.</p> <p>StatefulSets provide each of their Pod replicas with consistent and enduring identities. This includes predictable names, DNS hostnames, and a unique set of volumes that remain associated through the Pod's lifecycle, encompassing failures, restarts, and rescheduling events. These persistent identities are not just superficial labels; they are fundamental to the StatefulSet's scaling mechanics and their interaction with persistent storage.</p> <p>To conclude, it's important to recognize that StatefulSets offer a structural blueprint rather than a complete solution. They set the stage for resilience and consistency, but it's up to the applications themselves to be architecturally compatible with the StatefulSet paradigm to fully harness its benefits.</p>"},{"location":"misc/backup-restore/","title":"Backup & Restore","text":"<ul> <li> <p>You can save many objects on the cluster by querying the API Server and exporting it to YAML by running:</p> <p><pre><code>kubectl get all --all-namespaces -o yaml &gt; all-deploy-services.yaml\n</code></pre> </p> </li> <li> <p>Instead of backing up resources, you can back up the etcd server itself - etcd comes with a built-in snapshot solution:</p> <p><pre><code>etcdctl snapshot save /opt/snapshot-pre-boot.db\n--endpoints=https://127.0.0.1:2379 \\\n--cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n--cert=/etc/kubernetes/pki/etcd/server.crt \\\n--key=/etc/kubernetes/pki/etcd/server.key\n</code></pre> </p> </li> <li> <p>You can view the status of a snapshot by running:</p> <p><pre><code>etcdctl snapshot status snapshot.db\n</code></pre> </p> </li> <li> <p>Here are the generalized steps to restore from etcd from a backup:</p> <p><pre><code># first stop the API Server\nservice kube-apiserver stop\n\n# then restore\n# note the new directory being used by etcd\netcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup\n\n# edit etcd service to use new directory\nvi /etc/kubernetes/manifests/etcd.yaml\n\n# restart the etcd service\nsystemctl daemon-reload\n\n# wait 1-2 minutes for pods to come back up\nservice etcd restart\n\n# start API Server\nservice kube-apiserver start\n</code></pre> </p> </li> <li> <p>An easy way to view etcd servers for a cluster is by inspecting the API Server pod running in the <code>kube-system</code> Namespace</p> <ul> <li>It will have a field called <code>--etcd-servers</code> under the <code>command</code> field </li> </ul> </li> <li> <p>To view details about the etcd server you can also run:</p> <p><pre><code>ps -ef | grep etcd\n</code></pre> </p> </li> <li> <p>You can edit the etcd service at:     <pre><code>vi /etc/systemd/system/etcd.service\n</code></pre></p> </li> </ul>"},{"location":"misc/ha/","title":"Configuring HA","text":""},{"location":"misc/ha/#configuring-ha","title":"Configuring HA","text":"<ul> <li> <p>If you lose a Master node, your applications will remain running on worker nodes and users can still access those applications</p> <ul> <li>However, Pods won\u2019t restart or update as part of Deployment because there will be no controllers </li> </ul> </li> <li> <p>The API Server can be run in an <code>Active</code>-<code>Active</code> mode on multiple masters because they process one request at a time and simply pass information to other services which take action</p> <ul> <li>With multiple masters you can point your Kubeconfig to a loadbalancer on port 6443 in front of the masters to distribute traffic between the underlying API Servers </li> </ul> </li> <li> <p>The Controller Manager and Scheduler watch the cluster for changes and taking necessary actions</p> <ul> <li>These must be run in an <code>Active</code>-<code>Standby</code> mode to ensure that actions are duplicated</li> <li>Leader election process picks the <code>Active</code> one</li> </ul> </li> </ul>"},{"location":"misc/ha/#etcd-in-ha","title":"ETCD in HA","text":"<ul> <li>ETCD can be configured in two topologies:<ul> <li>Stacked Topology: running on Kubernetes master nodes</li> <li>External Topology: running on dedicated servers external to the cluster</li> <li>Remember the API Server is the only component that talks to ETCD<ul> <li>And must be configured to point to the servers, if hosted externally </li> </ul> </li> </ul> </li> </ul> <p>How does ETCD stay consistent when it allows you to read or write from any instance?</p> <ul> <li>With reads, the same data is available across all nodes so it\u2019s straight forward</li> <li>With writes, etcd ensures that only one instance is responsible for PROCESSING the writes via leader election<ul> <li>The leader then ensures the followers are sent a copy of the data</li> </ul> </li> <li>Writes that come in to an instance other than the leader, they are forwarded to the leader</li> <li>A write is only considered complete once the data has been copied to a majority of the instances<ul> <li>If an instance goes offline during a write, but the majority copy it - the data will be copied over to the node if/when it comes back online</li> </ul> </li> </ul>"},{"location":"misc/upgrades/","title":"Upgrades","text":""},{"location":"misc/upgrades/#os-upgrades","title":"OS Upgrades","text":"<ul> <li>By default, Nodes has 5 minutes to come back up before their Pods are killed</li> <li>When performing an OS upgrade, it\u2019s best to <code>drain</code> the Node before upgrading, because it may take longer than 5 minutes<ul> <li>Use the <code>--ignore-daemonsets</code> flag</li> </ul> </li> <li>Once the Node comes back up, you\u2019ll have to <code>uncordon</code> the Node to make it available to schedule Pods again</li> <li>The <code>cordon</code> command simply marks the Node as un-schedulable (but does not drain the Pods from the Node)</li> </ul>"},{"location":"misc/upgrades/#cluster-upgrade-process","title":"Cluster Upgrade Process","text":"<ul> <li> <p>It is not mandatory for all components to have the same version numbers</p> <ul> <li>However, no components should have a higher version than the API Server</li> <li>Controller Manager can be one version lower</li> <li>Scheduler can be one version lower</li> <li>Kubelet and Kube Proxy can each be two versions lower</li> <li>Kubectl can be one version higher OR lower</li> </ul> </li> <li> <p>Kubernetes supports up to the latest 3 minor versions</p> </li> <li> <p>The recommended approach to upgrade is one minor version at a time</p> </li> <li> <p>Upgrade master nodes first</p> </li> <li> <p>Kubeadm does not manage Kubelets, which must be upgraded manually</p> </li> <li> <p>The output of <code>kubectl get nodes</code> shows the versions of the Kubelets on each node</p> </li> </ul> <p>Generalized steps for upgrading:</p> <pre><code># show recommended version to ugprade to\nkubeadm upgrade plan\n\n# upgrade the kudeadm tool\napt-get upgrade -y kubeadm=&lt;version&gt;\n\n# on the master node, upgrade control plane services\nkubeadm upgrade apply v&lt;version&gt;\n\n# upgrade the kubelet\napt-get upgrade -y kubelet=&lt;version&gt;\n\n# upgrade the node\nkubeadm upgrade node\n\n# restart the kubelet\nsystemctl restart kubelet\n</code></pre>"},{"location":"networking/dns/","title":"DNS","text":""},{"location":"networking/dns/#service-discovery","title":"Service Discovery","text":"<p>As we saw in the previous sections, Kubernetes can be a very busy platform with Pods constantly coming and going. Services help calm some of the storm by providing a stable endpoint for clients to connect to. But how do apps find other apps on a cluster? Through Service discovery! There are two main concepts that make up Service discovery as a whole: Registration and Discovery.</p>"},{"location":"networking/dns/#service-registration","title":"Service registration","text":"<p>This is the process of an app on Kubernetes providing its connection details to a registry in order for other apps on the cluster to be able to find it. This happens automatically when Services are created.  </p> <p>As briefly mentioned in the previous section, Kubernetes provides its own DNS service (typically referred to as the cluster DNS). It's deployed as a series of Pods managed by a Deployment called <code>coredns</code>. These Pods are behind a Service called <code>kube-dns</code>. All of these reside within the <code>kube-system</code> Namespace.  </p> <p>Every Service created on a Kubernetes cluster will automatically register itself with the cluster DNS to ensure that all Pods across the cluster can \"find\" it.</p> <p><pre><code>flowchart LR\n    SVC[&lt;b&gt;Service&lt;/b&gt;&lt;br&gt;&lt;tt&gt;foo-svc&lt;br&gt;10.0.0.8]\n    SVC --&gt; |1. Service registered| REG[&lt;b&gt;Service registry&lt;/b&gt;&lt;br&gt;&lt;tt&gt;foo-svc: 10.0.0.8]\n    CON[&lt;tt&gt;app] --&gt; |2. Discover Service| REG\n    CON --&gt; |3. Consume Service| SVC</code></pre> The high-level flow of Service registration is as follows: </p> <ol> <li>Post a Service manifest to the API server (via <code>kubectl</code>)</li> <li>The Service is given a stable IP address called a ClusterIP</li> <li>EndpointSlices are created to maintain the list of healthy Pods which match the Service's label selector</li> <li>The Service's name and IP are registered with the cluster DNS.  </li> </ol> <p>It's worth noting that cluster DNS implements its own controller which constantly watches the API server for new Services being created. When a new one is observed, it automatically creates the DNS records mappings - meaning neither applications nor Services need to perform their own Service registration.  </p> <p>Every node's <code>kube-proxy</code> also watches the API server for new EndpointSlices and creates local networking rules when one is observed. This helps with redirecting ClusterIP traffic to Pod IPs.</p>"},{"location":"networking/dns/#service-discovery_1","title":"Service Discovery","text":"<p>The best way to explain discovery is likely through an example. So let's assume we have two applications on the same cluster - <code>ham</code> and <code>eggs</code>. Each application has their Pods fronted by a Service, which in turn each has their own ClusterIP.</p> $ kubectl get svc<pre><code>NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nham-svc   ClusterIP     192.168.1.200               443/TCP   5d19h\neggs-svc  ClusterIP     192.168.1.208               443/TCP   5d19h\n</code></pre> <p>Visually depicted as follows:  </p> <pre><code>flowchart\n    subgraph ham app\n        direction TB\n        ham-svc[\"&lt;b&gt;ham-svc&lt;/b&gt;\\n&lt;tt&gt;name: ham\\nIP: 192.168.1.200\\nPort: 443\"] --- Pod1[\"Pod\"]\n        ham-svc --- Pod2[\"Pod\"]\n        ham-svc --- Pod3[\"Pod\"]\n    end\n    subgraph kube-dns\n        direction TB\n        dns-svc[\"Cluster DNS\"]\n        dns-svc --- svc-reg[\"&lt;b&gt;registry&lt;/b&gt;&lt;br&gt;&lt;tt&gt;eggs: 192.168.1.208&lt;br&gt;ham: 192.168.1.200\"]\n    end\n    subgraph eggs app\n        direction TB\n        eggs-svc[\"&lt;b&gt;eggs-svc&lt;/b&gt;\\n&lt;tt&gt;name: eggs\\nIP: 192.168.1.208\\nPort: 443\"] --- Pod4[\"Pod\"]\n        eggs-svc --- Pod5[\"Pod\"]\n        eggs-svc --- Pod6[\"Pod\"]\n    end</code></pre> <p>In order for the <code>ham</code> application to communicate with the <code>eggs</code> application, it needs to know two things:  </p> <ol> <li>The name of the <code>eggs</code> application's Service (<code>eggs-svc</code>)</li> <li>How to convert that name to an IP address</li> </ol> <p>In the case of #1, it's the responsibility of the application developers to know which applications they need to communicate with. Kubernetes internal DNS handles point #2.  </p> <p>As mentioned above, Kubernetes automatically configures each container in the cluster to be able to resolve the IP address of the cluster DNS Service. It also appends any relevant search domains to unqualified names. It performs these actions by populating the <code>/etc/resolv.conf</code> on every container.  </p> <p>ClusterIPs exist on their own special Service network, so it takes a bit of work for traffic to get there. One thing to note is that every node in a cluster has a <code>kube-proxy</code> controller that creates IPVS rules any time a new Service is created. The steps that occur after an application attempts to communicate with another application on the cluster is a series of routing steps that can be summarized as follows:</p> <ol> <li>The application container's default gateway routes the traffic to the node it is running on.</li> <li>The node itself does not have a route to the Service network so it routes the traffic to the node kernel.</li> <li>The node kernel recognizes traffic intended for the service network (recall the IPVS rules) and routes the traffic to a healthy Pod that matches the label selector of the Service.  </li> </ol>"},{"location":"networking/dns/#namespaces","title":"Namespaces","text":"<p>A key point in understanding cluster DNS is knowing that Namespaces are able to partition a cluster's address space. Cluster address spaces are typically denoted as <code>cluster.local</code> and then have object names prepended to it. For instance, the <code>ham-svc</code> Service from above exists in the default Namespace and would have an FQDN of <code>ham-svc.default.svc.cluster.local</code>.  </p> <p>Now imagine you wanted to partition the cluster domain further with <code>perf</code> and <code>qa</code> Namespaces. For a <code>ham-svc</code> Service in each of those Namespaces, the address would look as follows:</p> <ul> <li>Perf: <code>ham-svc.perf.svc.cluster.local</code></li> <li>QA: <code>ham-svc.qa.svc.cluster.local</code></li> </ul> <p>Objects within the same Namespace can connect to each other using short names. However, cross-Namespace communication must use the FQDN. To visualize this, take the following setup where we have a Service in each Namespace fronting a few Pods:</p> <pre><code>flowchart\n    subgraph qa Namespace\n        direction TB\n        eggs-svc[\"&lt;tt&gt;eggs-svc&lt;/b&gt;\"] --- Pod1[\"&lt;tt&gt;scrambled\"]\n        eggs-svc --- Pod2[\"&lt;tt&gt;fried\"]\n        eggs-svc --- Pod3[\"&lt;tt&gt;poached\"]\n    end\n    subgraph perf Namespace\n        direction TB\n        ham-svc[\"&lt;tt&gt;ham-svc&lt;/b&gt;\"] --- Pod4[\"&lt;tt&gt;bacon\"]\n        ham-svc --- Pod5[\"&lt;tt&gt;sausage\"]\n        ham-svc -.-|&lt;tt&gt;ham-svc| Pod6[\"&lt;tt&gt;salt\"]\n        eggs-svc -.-|&lt;tt&gt;eggs-svc.qa.svc.cluster.local| Pod6[\"&lt;tt&gt;salt\"]\n    end</code></pre> <p>For the <code>salt</code> Pod to communicate with the <code>ham-svc</code> Service, it can simply reference it by it's short name (<code>ham-svc</code>) since they are within the same <code>perf</code> Namespace.  </p> <p>However, for <code>salt</code> to communicate with the <code>eggs-svc</code> Service, which resides in the <code>qa</code> Namespace, it would have to leverage it's FQDN: <code>eggs-svc.qa.svc.cluster.local</code>.</p>"},{"location":"networking/ingress/","title":"Ingress","text":"<p>Ingress aims to bridge the gap that exists with NodePort and LoadBalancer Services. NodePorts are great, but must use a high port number and require you to know the FQDN or IP address of your nodes. LoadBalancer Services don't require this, but they are limited to one internal Service per load-balancer. So, if you have 50 applications you need exposed to the internet, you'd need 50 of your cloud provider's load-balancers instantiated - which would probably be cost prohibitive in most cases.  </p> <p>Ingresses come into play here by allowing multiple Services to be \"fronted\" by a single cloud load-balancer. To accomplish this, Ingress will use a single LoadBalancer Service and use host-based or path-based routing to send traffic to the appropriate underlying Service.  </p> <pre><code>flowchart LR\n    CLD[cloud] --&gt; LBS\n    LBS[&lt;b&gt;LoadBalancer&lt;br&gt;Service] --&gt; ing1[&lt;b&gt;Ingress controller&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;- routing rules&lt;br&gt;- reading host &amp;&lt;br&gt;path names]\n    ing1 --&gt; SVC1[&lt;tt&gt;svc a]\n    ing1 --&gt; SVC2[&lt;tt&gt;svc b]\n    ing1 --&gt; SVC3[&lt;tt&gt;svc c]</code></pre>"},{"location":"networking/ingress/#routing-examples","title":"Routing Examples","text":""},{"location":"networking/ingress/#host-based-routing","title":"Host-based Routing","text":"<p><pre><code>flowchart LR\n    CLD[client] --&gt; |ham.foo.bar| LBS\n    CLD[client] --&gt; |eggs.foo.bar| LBS\n    LBS[&lt;b&gt;LoadBalancer&lt;br&gt;Service] --&gt; ing1[&lt;b&gt;Ingress controller]\n    ing1 --&gt; |ham.foo.bar|SVC1[&lt;b&gt;ham-svc]\n    ing1 --&gt; |eggs.foo.bar|SVC2[&lt;b&gt;eggs-svc]</code></pre> </p>"},{"location":"networking/ingress/#path-based-routing","title":"Path-based Routing","text":"<p><pre><code>flowchart LR\n    CLD[client] --&gt; |foo.bar/ham| LBS\n    CLD[client] --&gt; |foo.bar/eggs| LBS\n    LBS[&lt;b&gt;LoadBalancer&lt;br&gt;Service] --&gt; ing1[&lt;b&gt;Ingress controller]\n    ing1 --&gt; |foo.bar/ham|SVC1[&lt;b&gt;ham-svc]\n    ing1 --&gt; |foo.bar/eggs|SVC2[&lt;b&gt;eggs-svc]</code></pre> </p> <p>Kubernetes does not come with an Ingress controller by default</p> <p></p>"},{"location":"networking/ingress/#ingress-controllers","title":"Ingress Controllers","text":"<p>An Ingress controller is deployed as a resource on Kubernetes like any other: <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress\n  template:\n        metdata:\n          labels:\n            name: nginx-ingress\n        spec:\n          containers:\n            - name: nginx-ingress-controller\n              image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0\n          args:\n            - /nginx-ingress-controller\n            - --configmap=$(POD_NAMESPACE)/ngnix-configuration\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          ports:\n            - name: http\n              containerPort: 80\n            - name: https\n              containerPort: 443\n</code></pre> </p> <p>You\u2019ll also need to create a configuration (ConfigMap) for the Ingress controller: <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n    name: nginx-configuration\n</code></pre> </p> <p>You\u2019ll also need a Service to expose the Ingress controller: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n    name: nginx-ingress\nspec:\n    type: NodePort\n    ports:\n    - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: HTTP\n    - port: 443\n    targetPort: 443\n    protocol: TCP\n    name: https\n    selector:\n    name: nginx-ingress\n</code></pre> </p> <p>Ingress controllers have intelligence built-in to monitor the Kubernetes cluster for Ingress changes and update the underlying Nginx server when something changes</p> <ul> <li>To do this, it needs a ServiceAccount with the right set of permissions:     <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: ngninx-ingress-serviceaccount\n</code></pre></li> </ul>"},{"location":"networking/ingress/#ingress-resources","title":"Ingress Resources","text":"<p>An Ingress Resource is a set of rules and configurations applied on the Ingress Controller - i.e. forward all traffic to a single application, route to different applications by URL path, domain, etc. - It is created as a Kubernetes definition file like all others:   <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n    name: ingress-eggs\nspec:\n    backend:\n    serviceName: eggs-service\n    servicePort: 80\n</code></pre>  You can set up multiple rules for handling different traffic scenarios</p> <ul> <li>Within each rule you can handle different paths </li> </ul> <p>You can define the rules in a manifest file definition for the Ingress as well:  <pre><code>apiVersion: v1\nkind: Ingress\nmetadata:\n    name: ingress-eggs-ham\nspec:\n    rules:\n    - http:\n        paths:\n        - path: /eggs\n        backend:\n            serviceName: eggs-service\n            servicePort: 80\n        - path: /ham\n        backend:\n            serviceName: ham-service\n            servicePort: 80\n</code></pre> </p> <p>Here\u2019s an example of how you would write rules based on domain names: <pre><code>apiVersion: v1\nkind: Ingress\nmetadata:\n  name: ingress-eggs-ham\nspec:\n  rules:\n  - host: eggs.my-online-store.com\n      http:\n          paths:\n          - backend:\n              serviceName: eggs-service\n              servicePort: 80\n  - host: ham.my-online-store.com\n      http:\n          paths:\n          - backend:\n              serviceName: ham-service\n              servicePort: 80\n</code></pre></p>"},{"location":"networking/ingress/#more-information","title":"More Information","text":"<p>For a deeper dive into Ingress, refer to the official Kubernetes documentation.</p>"},{"location":"networking/overview/","title":"Overview","text":""},{"location":"networking/overview/#pod-networking","title":"Pod Networking","text":"<ul> <li> <p>Kubernetes does not come with a built-in solution for Pod networking, but it does have clear expectations:</p> <ul> <li>Every Pod should have it\u2019s own unique IP address</li> <li>Every Pod should be able to communicate with every other Pod on the same Node</li> <li>Every Pod should be able to communicate with every other Pod on other nodes without NAT </li> </ul> </li> <li> <p>There mare any networking solutions that solve this for you:</p> <ul> <li>weave, calico, flannel, etc.</li> </ul> </li> </ul>"},{"location":"networking/overview/#cni-in-kubernetes","title":"CNI in Kubernetes","text":"<ul> <li> <p>The plugin is configured in the <code>kubelet.service</code></p> <ul> <li>So all of the networking magic can happen when the Kubelet is creating the containers </li> </ul> </li> <li> <p>You can view this by running <code>px -aux | grep kubelet</code> </p> </li> <li> <p>The <code>/opt/cni/bin</code> directory contains all of the CNI plugins as executables </p> </li> <li>The CNI config directory has a set of configuration files and the Kubelet looks here to find out what plugin to use: <code>/etc/cni/net.d</code><ul> <li>If there are multiple listed, it will chose the first one in alphabetical order</li> </ul> </li> </ul>"},{"location":"networking/overview/#network-policy","title":"Network Policy","text":"<ul> <li> <p>By default, all Pods and Services can talk to all other Pods and Services within a Kubernetes cluster regardless of which Node(s) they are on - <code>Allow All</code> by default </p> </li> <li> <p>To disable communication between certain Pods or Services, you would implement a NetworkPolicy</p> <ul> <li>You link a NetworkPolicy to one or more Pods </li> </ul> </li> <li> <p>To link a NetworkPolicy to a Pod, you leverage labels and selectors </p> </li> </ul> <p>This policy can be configured as part of a NetworkPolicy definition:</p> <p><pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n    name: db-policy\nspec:\n    podSelector:\n    matchLabels:\n        role: db\n    policyTypes:\n    - Ingress\n    ingress:\n    - from:\n    - podSelector:\n        matchLabels:\n            name: api-pod\n    - namespaceSelector:\n        matchLabels:\n            name: prod # must have this label on the Namespace for it to work\n    ports:\n    - protocol: TCP\n        port: 3306\n</code></pre> </p> <ul> <li>Kubernetes networking solutions that support NetworkPolicies:<ul> <li>Kube-router</li> <li>Calico</li> <li>Romana</li> <li>Weave-net </li> </ul> </li> </ul> <p>You do not need to allow egress for a response to ingress. For example, imagine an API pod hitting a DB pod. The DB can allow ingress from the API server and not have to specify to allow egress for the API server to get results - the response is allowed back by default</p> <ul> <li> <p>i.e. when you\u2019re determining rules, you only need to be concerned with where the traffic originates, not responses </p> </li> <li> <p>You can omit the <code>podSelector</code> and just use <code>namespaceSelector</code> to allow all traffic within the Namespace to connect </p> </li> </ul> <p>You can specify resources outside of the Kubernetes by IP addresses as well with the <code>ipBlock.cidr</code> section:</p> <p><pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n    name: db-policy\nspec:\n    podSelector:\n    matchLabels:\n        role: db\n    policyTypes:\n    - Ingress\n    ingress:\n    - from:\n    **- ipBlock:\n        cidr: 192.168.5.10/32**\n    ports:\n    - protocol: TCP\n        port: 3306\n</code></pre> </p> <p>You specify the AND/OR criteria of the selectors by dashes. For example, this rule will allow traffic from Pods that match the given label AND match the given Namespace:   <pre><code>...\ningress:\n- from:\n    **-** podSelector:\n        matchLabels:\n        name: api-pod\n    namespaceSelector:\n        matchLabels:\n        name: Prod\n...\n</code></pre> </p> <p>This will allow traffic from Pods that match either OR criteria:</p> <p><pre><code>...\ningress:\n- from:\n    **-** podSelector:\n        matchLabels:\n        name: api-pod\n    **-** namespaceSelector:\n        matchLabels:\n        name: Prod\n...\n</code></pre> </p> <p>For egress, we need an egress rule: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: db-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 192.168.5.10/32\n    ports:\n    - protocol: TCP\n      port: 3306\n  egress:\n  - to: \n    - ipBlock:\n        cidr: 192.168.5.10/32\n    ports:\n       - protocol: TCP\n         port: 80\n</code></pre></p>"},{"location":"security/auth-rbac/","title":"Auth/RBAC","text":""},{"location":"security/auth-rbac/#overview","title":"Overview","text":"<p>In Kubernetes, everything from creating new resources to updating or deleting them involves making requests to the API server. This is true for everyone and everything in the Kubernetes ecosystem: from developers using <code>kubectl</code>, to the Pods running in your cluster, to the kubelets on each node, and the control plane services that oversee cluster operations.  </p> <p>Let's take an example: imagine a user named \"vinny\" wants to deploy a new application using a Deployment named \"treats\" in the \"petropolis\" Namespace. vinny runs a <code>kubectl apply</code> command, which sends a request to the API server. This request is securely sent over TLS, carrying vinny's credentials. The API server first authenticates vinny, making sure they are who they claim to be. Next, it checks if vinny has the permissions (via RBAC) to create Deployments in the petropolis Namespace. If vinny passes these checks, the request goes through admission control for any additional policy checks before being executed on the cluster.</p>"},{"location":"security/auth-rbac/#authentication-authn","title":"Authentication (AuthN)","text":"<p>Authentication is all about proving who you are. It's often referred to as \"authN.\" At its core are credentials\u2014every request to the API server must include them. The authentication layer checks these credentials; if they don\u2019t match, you get a \"401 Unauthorized\" response. If they check out, you move on to authorization.  </p> <p>Kubernetes doesn\u2019t keep its own user database; instead, it connects to external systems like Active Directory or cloud IAM services for identity management. This setup prevents the creation of redundant identity systems. While Kubernetes supports client certificates out of the box, for practical use, you'll likely integrate it with your existing identity management system. Hosted Kubernetes services usually offer easy integration with their native IAM solutions.  </p>"},{"location":"security/auth-rbac/#checking-your-authentication-setup","title":"Checking Your Authentication Setup","text":"<p>Your connection details to Kubernetes are stored in a <code>kubeconfig</code> file. This file tells tools like <code>kubectl</code> which cluster to talk to and which credentials to use. It includes sections for defining clusters, users, contexts (which pair a user with a cluster), and the current context (the default cluster-user pair for commands).  </p> <p>The <code>clusters</code> section outlines details like the cluster's API server endpoint and its CA's public key. The <code>users</code> section lists user names and their tokens, which are often X.509 certificates signed by a trusted CA. The <code>contexts</code> section pairs users with clusters, and the <code>current-context</code> sets the default for commands.  </p> <p>Given a specific <code>kubeconfig</code>, <code>kubectl</code> commands are directed to the specified cluster and authenticated as the defined user. If your cluster uses an external IAM, it handles the authentication. Once authenticated, the request can proceed to authorization, where Kubernetes decides if you have the necessary permissions to carry out your request.  </p>"},{"location":"security/auth-rbac/#authorization","title":"Authorization","text":"<p>After you've proven your identity to Kubernetes (that's authentication), you're faced with authorization, often abbreviated as authZ. This is where Kubernetes decides if you're allowed to do what you're asking to do, like creating or deleting resources.</p> <p>Kubernetes uses a modular system for authorization, meaning you can have different methods in play. But once any method says \"yes\" to a request, it's off to the next step: admission control. The most common method for making these decisions is Role-Based Access Control (RBAC).</p>"},{"location":"security/auth-rbac/#key-concepts-in-rbac","title":"Key Concepts in RBAC","text":"<p>RBAC boils down to three main ideas:</p> <ul> <li>Users: Who is making the request?</li> <li>Actions: What are they trying to do?</li> <li>Resources: What are they trying to do it to?</li> </ul> <p>Essentially, RBAC controls which users can perform which actions on which resources.</p>"},{"location":"security/auth-rbac/#rbac-in-action","title":"RBAC in Action","text":"<p>In RBAC, you'll deal with Roles and RoleBindings:</p> <ul> <li>Roles specify permissions (what actions can be performed on what resources).</li> <li>RoleBindings link those permissions to users.</li> </ul> <p>For example, you might have a Role that allows reading Deployments in a specific Namespace and a RoleBinding that grants a user those read permissions.</p> <pre><code>kind: Role\nmetadata:\n  namespace: petropolis\n  name: read-deployments\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <p>This Role, by itself, doesn't do much. It needs to be connected to a user through a RoleBinding:</p> <pre><code>kind: RoleBinding\nmetadata:\n  name: read-deployments\n  namespace: petropolis\nsubjects:\n- kind: User\n  name: vinny\nroleRef:\n  kind: Role\n  name: read-deployments\n</code></pre> <p>With this setup, a user named \"vinny\" can list, watch, and get deployments in the \"petropolis\" Namespace.</p>"},{"location":"security/auth-rbac/#the-bigger-picture","title":"The Bigger Picture","text":"<p>Kubernetes doesn't just have Roles and RoleBindings; there are also ClusterRoles and ClusterRoleBindings for cluster-wide permissions. This system allows you to define permissions once at the cluster level and then apply them to specific Namespaces as needed.</p> <p>Most Kubernetes setups come with a set of pre-created roles to get you started, including powerful roles like <code>cluster-admin</code> that should be used cautiously.</p>"},{"location":"security/auth-rbac/#authorization-takeaways","title":"Authorization Takeaways","text":"<p>Authorization in Kubernetes, especially through RBAC, is about specifying what authenticated users are allowed to do within the cluster. It's a system built on allowing certain actions while denying everything else by default, making it crucial to carefully manage permissions to maintain security and functionality in your cluster.</p> <p>Once a request clears the authentication and authorization stages, it's evaluated by admission control to apply any further policies before being executed on the cluster.</p>"},{"location":"security/auth-rbac/#understanding-admission-control-in-kubernetes","title":"Understanding Admission Control in Kubernetes","text":"<p>After a request passes through authentication and authorization, it encounters the final gatekeeper before being executed: admission control. This stage is where Kubernetes applies various policies to ensure the request aligns with cluster rules and standards.</p>"},{"location":"security/auth-rbac/#types-of-admission-controllers","title":"Types of Admission Controllers","text":"<p>Kubernetes employs two main types of admission controllers:</p> <ul> <li>Mutating Admission Controllers: These can alter requests to ensure they comply with policies. For example, they might add a missing label to an object to meet a labeling policy.</li> <li>Validating Admission Controllers: These verify requests against policies but don't modify the requests. If a request violates a policy, it's rejected.</li> </ul> <p>Mutating controllers operate before validating ones, ensuring that any modifications are in place before final checks are made. Only requests that would change the cluster's state are subject to admission control; read-only requests bypass this process.</p>"},{"location":"security/auth-rbac/#example-in-action","title":"Example in Action","text":"<p>Imagine you have a policy requiring all objects to include an <code>app=shop</code> label. A mutating controller could automatically add this label if it's missing from a request, whereas a validating controller would reject any request lacking the label.</p>"},{"location":"security/auth-rbac/#admission-control-on-a-cluster","title":"Admission Control on a Cluster","text":"<p>On Docker Desktop, for instance, the <code>NodeRestriction</code> admission controller is enabled by default, limiting what nodes can modify within their scope. Real-world clusters typically enable a broader set of controllers for comprehensive policy enforcement.</p> <p>A notable example is the <code>AlwaysPullImages</code> controller, a mutating type that ensures Pods always pull their container images from a registry, preventing the use of potentially unsafe local images and ensuring only nodes with proper registry credentials can pull and run containers.</p>"},{"location":"security/auth-rbac/#admission-controls-role","title":"Admission Control's Role","text":"<p>If any admission controller rejects a request, it stops there\u2014no further processing occurs. But if a request gets the green light from all controllers, it's saved to the cluster store and deployed.</p> <p>Admission controllers are increasingly crucial for maintaining the security and integrity of production clusters, given their power to enforce policies directly on incoming requests.</p>"},{"location":"security/auth-rbac/#recap-of-authn-authz-and-rbac","title":"Recap of AuthN, AuthZ, and RBAC","text":"<ul> <li> <p>Authentication (AuthN) validates who you are, using credentials included in every API server request. While Kubernetes doesn't manage user identities internally, it integrates with external systems for robust identity checks.</p> </li> <li> <p>Authorization (AuthZ), particularly through RBAC, dictates what authenticated users can do. It's a system of allowing specific actions via Roles and RoleBindings, ensuring users have only the permissions they need.</p> </li> <li> <p>Admission Control is the last hurdle, enforcing policies on requests post-authorization. It plays a key role in keeping the cluster secure by either modifying requests to align with policies (mutating) or rejecting those that don't comply (validating).</p> </li> </ul> <p>Throughout these stages, TLS secures communication, ensuring that sensitive information remains protected as it travels to the Kubernetes API server.</p>"},{"location":"security/certs/","title":"Certificates","text":""},{"location":"security/certs/#tls","title":"TLS","text":"<ul> <li>A certificate is used to guarantee trust between two parties during a transaction</li> <li>Asymmetrical encryption is where generate private and public keys<ul> <li>You keep the private key with you</li> <li>The public key can, in theory, be shared out anywhere</li> <li>Even if an attacker has the public key (i.e. lock), they can\u2019t actually decrypt the data, access the server, etc. unless they have the private key to do so</li> </ul> </li> </ul>"},{"location":"security/certs/#certificates","title":"Certificates","text":"<ul> <li>Every component in Kubernetes communicates via secure communications</li> <li>Kubernetes requires at least one Certificate Authority (CA) per cluster</li> </ul>"},{"location":"security/certs/#creating-certificates","title":"Creating Certificates","text":"<p>For creating the certificates for the CA: </p> <p>Generate keys:</p> <pre><code>openssl genrsa -out ca.key 2048\n</code></pre> <p></p> <p>Create certificate signing request: <pre><code>openssl req -new -key ca.key -subj \"/CN=KUBERNETES=CA\" -out ca.csr\n</code></pre> </p> <p>Sign the certificate. Note this will be self-signed using it\u2019s own private key we generated in step 1 above, as we are just now initially creating the CA. Subsequent tickets will all be signed by this CA:   <pre><code>openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt\n</code></pre> </p> <p>Generating client certificates (i.e. admin user here) </p> <p>Generate keys:   <pre><code>openssl genrsa -out admin.key 2048\n</code></pre></p> <p></p> <p>Certificate signing request. Note you must add group details (<code>O=system:masters</code>) when creating user certs:  <pre><code>openssl req -new -key admin.key -sub \"/CN=kube-admin/O=system:masters\" -out admin.csr\n</code></pre> </p> <p>Sign certificates by specifying CA cert (<code>-CA ca.crt</code>) and key (<code>-CAkey ca.key</code>):   <pre><code>openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt\n</code></pre></p> <p></p>"},{"location":"security/certs/#what-to-do-with-certs","title":"What to do with certs?","text":"<p>One simple way to use them (i.e. admin user cert here) is to include them in REST calls to the API server. You must include the client key (<code>admin.key</code>), the client cert (<code>admin.crt</code>) and the CA cert (<code>ca.crt</code>) in your call:</p> <p><pre><code>curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt\n</code></pre>  Another easy thing to do is to include them in a kubeconfig file:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority: ca.crt\n    server: https://kube-apiserver:6443\n  name: kubernetes-cluster-1\nkind: Config\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate: admin.crt\n    client-key: admin.key\n</code></pre> <p>All certificate operations are carried out by the ControllerManager on the control plane</p> <p></p>"},{"location":"security/certs/#kubeconfig","title":"KubeConfig","text":"<p>A kubeconfig file lets you specify certificate information without having to type it in every time you run a <code>kubectl</code> command</p> <p>By default, <code>kubectl</code> will look for a kubeconfig file at <code>$HOME/.kube/config</code></p> <p></p> <p>A kubeconfig file consists of three specific parts: 1. clusters: specification of the cluster you want to connect to (i.e. dev, production, etc.) 2. users: the user account you will use to run commands(i.e. admin, dev, etc.) 3. contexts the \"marrying\" of a cluster and a user (i.e. dev user on production cluster)</p> <p>Under each cluster and user specs, you can list out the necessary certificates required for access</p> <p> <pre><code>apiVersion: v1\nkind: Config\n\nclusters:\n- name: my-kube-playground\n  cluster:\n    certificate-authority: ca.crt\n    server: https://my-kube-playground:6443\n\ncontexts:\n- name: my-kube-admin@my-kube-playground\n  context:\n    cluster: my-kube-playground\n    user: my-kube-admin\n    namespace: finance # this field is optional\nusers:\n- name: my-kube-admin\n  user:\n    client-certificate: admin.crt\n    client-key: admin.key\n</code></pre></p> <p>To change contexts, run:</p> <p><pre><code>kubectl config use-context &lt;context-name&gt;\n</code></pre> </p> <p>You can also specify the certificate info via base64 as well: <pre><code>apiVersion: v1\nkind: Config\n\nclusters:\n- name: my-kube-playground\n  cluster:\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNZekNDQWN5Z0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRVUZBREF1TVFzd0NRWURWUVFHRXdKVlV6RU0gCk1Bb0dBMVVFQ2hNRFNVSk5NUkV3RHdZRFZRUUxFd2hNYjJOaGJDQkRRVEFlRncwNU9URXlNakl3TlRBd01EQmEgCkZ3MHdNREV5TWpNd05EVTVOVGxhTUM0eEN6QUpCZ05WQkFZVEFsVlRNUXd3Q2dZRFZRUUtFd05KUWsweEVUQVAgCkJnTlZCQXNUQ0V4dlkyRnNJRU5CTUlHZk1BMEdDU3FHU0liM0RRRUJBUVVBQTRHTkFEQ0JpUUtCZ1FEMmJaRW8gCjd4R2FYMi8wR0hrck5GWnZseEJvdTl2MUptdC9QRGlUTVB2ZThyOUZlSkFRMFFkdkZTVC8wSlBRWUQyMHJIMGIgCmltZERMZ05kTnlubXlSb1MyUy9JSW5mcG1mNjlpeWMyRzBUUHlSdm1ISWlPWmJkQ2QrWUJIUWkxYWRrajE3TkQgCmNXajZTMTR0VnVyRlg3M3p4MHNOb01TNzlxM3R1WEtyRHN4ZXV3SURBUUFCbzRHUU1JR05NRXNHQ1ZVZER3R0cgCitFSUJEUVErRXp4SFpXNWxjbUYwWldRZ1lua2dkR2hsSUZObFkzVnlaVmRoZVNCVFpXTjFjbWwwZVNCVFpYSjIgClpYSWdabTl5SUU5VEx6TTVNQ0FvVWtGRFJpa3dEZ1lEVlIwUEFRSC9CQVFEQWdBR01BOEdBMVVkRXdFQi93UUYgCk1BTUJBZjh3SFFZRFZSME9CQllFRkozK29jUnlDVEp3MDY3ZExTd3IvbmFseDZZTU1BMEdDU3FHU0liM0RRRUIgCkJRVUFBNEdCQU1hUXp0K3phajFHVTc3eXpscjhpaU1CWGdkUXJ3c1paV0pvNWV4bkF1Y0pBRVlRWm1PZnlMaU0gCkQ2b1lxK1puZnZNMG44Ry9ZNzlxOG5od3Z1eHBZT25SU0FYRnA2eFNrcklPZVp0Sk1ZMWgwMExLcC9KWDNOZzEgCnN2WjJhZ0UxMjZKSHNRMGJoek41VEtzWWZid2ZUd2ZqZFdBR3k2VmYxbllpL3JPK3J5TU8KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLSA=\n\n...\n</code></pre></p>"},{"location":"security/container-security/","title":"Container Security","text":""},{"location":"security/container-security/#image-security","title":"Image Security","text":"<ul> <li> <p>Generally, it's best-practice to only use container images from repositories that you trust - either one's internal to your company, your own private repo, or Docker's verified registry (although you should still be careful with these). </p> </li> <li> <p>If you don\u2019t specify a registry in the <code>image</code> name in your manifest, it\u2019s assumed to be Docker\u2019s default registry - <code>docker.io</code></p> <ul> <li>i.e. putting <code>image: nginx</code> will assume it\u2019s actually <code>image: docker.io/library/nginx</code> </li> </ul> </li> <li> <p>Another popular container repo is <code>gcr.io</code> - Google\u2019s container repository where a lot of Kubernetes core images reside</p> </li> </ul>"},{"location":"security/container-security/#security-in-docker","title":"Security in Docker","text":"<ul> <li> <p>By default, Docker runs processes in containers as <code>root</code></p> <ul> <li>You can change this though </li> </ul> </li> <li> <p>Processes running in a container are also visible as running processes on the host itself </p> </li> <li> <p>The <code>root</code> user in the container is not the same as the <code>root</code> user on the host</p> <ul> <li>It\u2019s limited in it\u2019s ability to impact the host or other processes on the host</li> </ul> </li> </ul>"},{"location":"security/container-security/#security-contexts","title":"Security Contexts","text":"<p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-pod\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu\n      securityContext:\n        runAsUser: 1000\n        capabilities:\n          add: [\"MAC_ADMIN\"]\n</code></pre> In the snippet above, we configure security context for a Pod, including:</p> <ul> <li>what user to run as</li> <li> <p>and Linux capabilities to give the Pod </p> </li> <li> <p>You can specify the <code>runAsUser</code> at the container or Pod level - but <code>capabilities</code> is not supported at the Pod level </p> </li> <li>You can find out the user that is used to execute in a container by running:     <pre><code>kubectl exec &lt;pod-name&gt; -- whoami\n</code></pre></li> </ul>"},{"location":"security/service-account/","title":"Service Accounts","text":"<p>There are two types of accounts in Kubernetes</p> <ol> <li>Service - bots</li> <li>User - humans </li> </ol> <p>To create a service account, run <code>kubectl create serviceaccount &lt;name&gt;</code></p> <ul> <li>You must separately created a token (<code>kubectl create token &lt;name&gt;</code>), which the ServiceAccount can use as an authentication bearer token when interacting with the Kubernetes API </li> </ul> <p>For every Namespace in Kubernetes, there is a default ServiceAccount</p> <ul> <li>Whenever a Pod is created, the default ServiceAccount and it\u2019s token are automatically mounted to that Pod as a volume</li> </ul> <p>The default ServiceAccount only has basic permissions to run Kubernetes operations</p> <ul> <li> <p>You can modify your Pod definition to leverage a different ServiceAccount, if desired under <code>spec.serviceAccountName</code></p> <ul> <li>You cannot edit the existing ServiceAccount of a Pod (immutable), but you can for Deployments</li> </ul> </li> </ul>"}]}