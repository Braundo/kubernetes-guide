{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>  Welcome to the Kubernetes Guide, a quick and easy-to-digest summary of core Kubernetes concepts intended to help get you from zero to proficient! </p> <p>The initial focus of this guide is to cover topics to help you pass the Kubernetes Certified Administrator (CKA) exam. As time goes on, I will add more study content here to help prepare for other Kubernetes-related topics.</p> <p></p> <p>Feel free to pick and choose any section in any order, but you'll likely be best served by following along in the default order of the site.</p> <p></p> <p>One thing to note about text formatting in this guide: you'll notice some terms always start with a capital letter (i.e. Service, Pod, etc.). This is intentional and an attempt to adhere to standard formatting as laid out in the official Kubernetes documentation. Kubernetes API objects (like the ones just mentioned) should start with a capital letter.</p> <p></p> <p></p> <p>Legal disclaimer:  </p> <ul> <li> <p>\"Kubernetes\", \"K8s\" and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  </p> </li> <li> <p>Neither myself nor this site are officially associated with the Linux Foundation. </p> </li> </ul> <p></p> <p> Connect with me</p> <p> Suggest changes</p>"},{"location":"about/","title":"About Me","text":"<p>My name is Aaron Braundmeier and I've been working in the tech industry for over a decade at companies such as Mastercard, VMware, and Broadcom (new adventure coming soon). I've long been a Kubernetes fan and had the privilege of working hands-on in that space during my time within the Tanzu business unit at VMware.</p> <p>I'm a Certified Kubernetes Administrator (CKA) and am always interesting in learning and being hands-on with all things Kubernetes.</p> <p>If you're interested in connecting, I can be reached in the following ways:</p> <p> aaron@braundmeier.com</p> <p> LinkedIn</p> <p> Signal</p> <p></p>"},{"location":"configmaps-secrets/","title":"Managing Configuration and Secrets in Kubernetes","text":"<p>Modern applications require dynamic configuration management and secure handling of sensitive data. Kubernetes offers ConfigMaps and Secrets to handle these requirements efficiently, allowing you to decouple configuration from application code and manage sensitive information securely.</p>"},{"location":"configmaps-secrets/#introduction","title":"Introduction","text":"<p>In the traditional monolithic application days, environment variables and configurations were bundled up with the application and deployed as one large object. However, in the cloud-native application model it's important to decouple these for many reasons:  </p> <ol> <li>Environment Flexibility: Decoupling allows the same application to run across different environments (development, staging, production) without code changes. Environment-specific configurations can be applied externally, improving the portability of the application.</li> <li>Scalability and Dynamic Management: When configuration is externalized, it's easier to scale applications horizontally since the configuration can be managed and applied independently. This allows for dynamic reconfiguration in response to changes in load or other factors without redeploying or restarting containers.</li> <li>Security and Sensitive Data Handling: Keeping sensitive configuration data, such as secrets and credentials, separate from the application codebase helps maintain security. It ensures that sensitive data is not exposed within the code and can be securely managed using secrets management tools.</li> <li>Continuous Deployment and Rollbacks: Decoupling facilitates continuous deployment practices by allowing configurations to be updated independently of the application. This separation also simplifies rollback procedures in case a configuration change needs to be reverted without affecting the application version that's running.</li> <li>Maintainability and Clarity: Keeping configuration separate from application code helps maintain a clean codebase and makes it clearer for developers to understand the application logic. It avoids cluttering the application with environment-specific conditionals and settings, making the code easier to maintain and evolve.  </li> </ol>"},{"location":"configmaps-secrets/#understanding-configmaps","title":"Understanding ConfigMaps","text":"What are ConfigMaps? <p>ConfigMaps store non-sensitive configuration data as key-value pairs. They are first-class objects in the Kubernetes API, making them stable and widely supported.</p> Use Cases for ConfigMaps <p>ConfigMaps are used to store:</p> <ul> <li>Environment variables</li> <li>Configuration files (e.g., web server configs)</li> <li>Hostnames</li> <li>Service ports</li> <li>Account names</li> </ul> <p>Avoid storing sensitive data in ConfigMaps; use Secrets for that purpose.</p> Creating ConfigMaps <p>ConfigMaps can be created imperatively or declaratively.</p> Imperative Creation <p>Create a ConfigMap with literal values: <pre><code>$ kubectl create configmap app-config --from-literal=env=prod --from-literal=debug=false\n</code></pre></p> <p>Create a ConfigMap from a file: <pre><code>$ kubectl create configmap app-config --from-file=config.properties\n</code></pre></p> Declarative Creation <p>Define a ConfigMap in a YAML file (<code>configmap.yaml</code>): <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  env: prod\n  debug: \"false\"\n</code></pre></p> <p>Apply the YAML file: <pre><code>$ kubectl apply -f configmap.yaml\n</code></pre></p> Using ConfigMaps <p>Inject ConfigMap data into Pods using environment variables, command arguments, or volumes.</p> <p>cmpodappappvolvol<p>env var</p>env var<p>cmd</p>cmdkeykeyvaluevalue<p>=</p>=</p> As Environment Variables <p>Define environment variables in the Pod specification: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  containers:\n    - name: app-container\n      image: myapp:latest\n      env:\n        - name: ENV\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: env\n        - name: DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: debug\n</code></pre></p> As Volumes <p>Mount the ConfigMap as a volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  volumes:\n    - name: config-volume\n      configMap:\n        name: app-config\n  containers:\n    - name: app-container\n      image: myapp:latest\n      volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n</code></pre></p> Kubernetes-Native Applications <p>Kubernetes-native applications can access ConfigMap data directly via the API server, simplifying configuration management and reducing dependencies on environment variables or volumes.</p>"},{"location":"configmaps-secrets/#understanding-secrets","title":"Understanding Secrets","text":"What are Secrets? <p>Secrets store sensitive data such as passwords, tokens, and certificates. They are similar to ConfigMaps but are designed to handle sensitive information securely.</p> Are Kubernetes Secrets Secure? <p>By default, Kubernetes Secrets are not encrypted in the cluster store or in transit. They are base64-encoded, which is not secure. To enhance security, use additional tools like HashiCorp Vault for better encryption.</p> Creating Secrets <p>Secrets can also be created imperatively or declaratively.</p> Imperative Creation <p>Create a Secret with literal values: <pre><code>$ kubectl create secret generic db-credentials --from-literal=username=dbuser --from-literal=password=securepass\n</code></pre></p> Declarative Creation <p>Define a Secret in a YAML file (<code>secret.yaml</code>): <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\ndata:\n  username: ZGJ1c2Vy\n  password: c2VjdXJlcGFzcw==\n</code></pre></p> <p>Apply the YAML file: <pre><code>$ kubectl apply -f secret.yaml\n</code></pre></p> Using Secrets <p>Inject Secret data into Pods using environment variables, command arguments, or volumes.</p> As Volumes <p>Mount the Secret as a volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  volumes:\n    - name: secret-volume\n      secret:\n        secretName: db-credentials\n  containers:\n    - name: app-container\n      image: myapp:latest\n      volumeMounts:\n        - name: secret-volume\n          mountPath: /etc/secret\n</code></pre></p>"},{"location":"configmaps-secrets/#hands-on-examples","title":"Hands-On Examples","text":"Example ConfigMap <p>Create a ConfigMap with configuration data: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: example-config\ndata:\n  APP_ENV: \"production\"\n  APP_DEBUG: \"false\"\n</code></pre></p> <p>Deploy and inspect the ConfigMap: <pre><code>$ kubectl apply -f example-config.yaml\n$ kubectl describe configmap example-config\n</code></pre></p> Example Secret <p>Create a Secret with sensitive data: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-secret\ntype: Opaque\ndata:\n  username: ZGJ1c2Vy\n  password: c2VjdXJlcGFzcw==\n</code></pre></p> <p>Deploy and inspect the Secret: <pre><code>$ kubectl apply -f db-secret.yaml\n$ kubectl describe secret db-secret\n</code></pre></p>"},{"location":"configmaps-secrets/#summary","title":"Summary","text":"<p>ConfigMaps and Secrets are essential tools in Kubernetes for managing application configuration and sensitive data. By decoupling configuration from application code and handling sensitive information securely, you can create more flexible, maintainable, and secure applications.</p>"},{"location":"crd/","title":"Kubernetes API and Custom Resource Definitions (CRDs)","text":"<p>Kubernetes is highly extensible, allowing you to add custom resources to the API through Custom Resource Definitions (CRDs). This flexibility enables you to manage any kind of application-specific or domain-specific data and operations.</p>"},{"location":"crd/#extending-kubernetes-api","title":"Extending Kubernetes API","text":"Introduction to CRDs <p>Custom Resource Definitions (CRDs) allow you to define custom resources within the Kubernetes API. These custom resources can represent any type of data or application-specific configuration, extending Kubernetes' capabilities beyond its built-in resource types.</p> Benefits of Using CRDs <ul> <li>Custom Resources: Define custom resources tailored to your application's needs.</li> <li>Declarative Management: Manage custom resources using Kubernetes' declarative API.</li> <li>Integration: Integrate seamlessly with existing Kubernetes tools like <code>kubectl</code>, controllers, and operators.</li> </ul> Creating a CRD <p>To create a CRD, define it in a YAML file and apply it to your cluster.</p> <p>Example CRD Definition: <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: widgets.example.com\nspec:\n  group: example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                size:\n                  type: string\n                color:\n                  type: string\n  scope: Namespaced\n  names:\n    plural: widgets\n    singular: widget\n    kind: Widget\n    shortNames:\n    - wg\n</code></pre></p> <p>Apply the CRD to the cluster: <pre><code>$ kubectl apply -f widget-crd.yaml\n</code></pre></p> Using Custom Resources <p>Once the CRD is defined, you can create and manage instances of the custom resource.</p> <p>Example Custom Resource: <pre><code>apiVersion: example.com/v1\nkind: Widget\nmetadata:\n  name: my-widget\nspec:\n  size: large\n  color: blue\n</code></pre></p> <p>Apply the custom resource: <pre><code>$ kubectl apply -f my-widget.yaml\n</code></pre></p>"},{"location":"crd/#using-kubebuilder","title":"Using Kubebuilder","text":"Introduction to Kubebuilder <p>Kubebuilder is a framework for building Kubernetes APIs using CRDs. It simplifies the process of creating, managing, and extending Kubernetes resources by providing tools and libraries to generate boilerplate code and handle common tasks.</p> Benefits of Using Kubebuilder <ul> <li>Code Generation: Automatically generates boilerplate code for CRDs and controllers.</li> <li>Best Practices: Follows Kubernetes best practices for API development.</li> <li>Scaffolding: Provides scaffolding for custom resources and controllers, reducing development time.</li> </ul> Installing Kubebuilder <p>Install Kubebuilder by following the official installation guide from the Kubebuilder website.</p> <p>For Linux/macOS: <pre><code>$ curl -L https://github.com/kubernetes-sigs/kubebuilder/releases/download/v2.3.1/kubebuilder_2.3.1_$(uname -s)_$(uname -m).tar.gz | tar -xz -C /usr/local/\n$ export PATH=$PATH:/usr/local/kubebuilder/bin\n</code></pre></p> Creating a New Project <p>Initialize a new Kubebuilder project: <pre><code>$ mkdir widget-operator\n$ cd widget-operator\n$ kubebuilder init --domain example.com --repo github.com/example/widget-operator\n</code></pre></p> Creating a New API <p>Create a new API for the custom resource: <pre><code>$ kubebuilder create api --group apps --version v1 --kind Widget\n</code></pre></p> <p>This command generates boilerplate code for the custom resource and its controller.</p> Defining the Custom Resource <p>Edit the generated files to define the schema and behavior of the custom resource.</p> <p>Edit <code>api/v1/widget_types.go</code>: <pre><code>type WidgetSpec struct {\n  Size  string `json:\"size,omitempty\"`\n  Color string `json:\"color,omitempty\"`\n}\n\ntype WidgetStatus struct {\n  // Define observed state of the resource\n}\n\n// +kubebuilder:object:root=true\n// +kubebuilder:subresource:status\ntype Widget struct {\n  metav1.TypeMeta   `json:\",inline\"`\n  metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n  Spec   WidgetSpec   `json:\"spec,omitempty\"`\n  Status WidgetStatus `json:\"status,omitempty\"`\n}\n\n// +kubebuilder:object:root=true\ntype WidgetList struct {\n  metav1.TypeMeta `json:\",inline\"`\n  metav1.ListMeta `json:\"metadata,omitempty\"`\n  Items           []Widget `json:\"items\"`\n}\n</code></pre></p> Implementing the Controller <p>Edit the generated controller file to define the reconciliation logic.</p> <p>Edit <code>controllers/widget_controller.go</code>: <pre><code>func (r *WidgetReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {\n  ctx := context.Background()\n  log := r.Log.WithValues(\"widget\", req.NamespacedName)\n\n  // Fetch the Widget instance\n  var widget appsv1.Widget\n  if err := r.Get(ctx, req.NamespacedName, &amp;widget); err != nil {\n    log.Error(err, \"unable to fetch Widget\")\n    return ctrl.Result{}, client.IgnoreNotFound(err)\n  }\n\n  // TODO: Add application management logic here\n\n  return ctrl.Result{}, nil\n}\n</code></pre></p> Deploying the Operator <p>Build and push the Operator image: <pre><code>$ make docker-build docker-push IMG=&lt;your-image-registry&gt;/widget-operator:v0.1.0\n</code></pre></p> <p>Deploy the Operator to the cluster: <pre><code>$ make deploy IMG=&lt;your-image-registry&gt;/widget-operator:v0.1.0\n</code></pre></p> Using the Operator <p>Create and manage custom resources using the Operator.</p> <p>Apply the CRD: <pre><code>$ kubectl apply -f config/crd/bases/example.com_widgets.yaml\n</code></pre></p> <p>Create a Custom Resource: <pre><code>apiVersion: apps.example.com/v1\nkind: Widget\nmetadata:\n  name: my-widget\nspec:\n  size: large\n  color: blue\n</code></pre></p> <p>Apply the custom resource: <pre><code>$ kubectl apply -f my-widget.yaml\n</code></pre></p>"},{"location":"crd/#summary","title":"Summary","text":"<p>Kubernetes CRDs and Kubebuilder provide powerful tools for extending the Kubernetes API and managing custom resources. By leveraging these tools, you can create, manage, and automate complex application-specific logic within your Kubernetes cluster. Kubebuilder simplifies the process of building Kubernetes APIs, allowing you to focus on the business logic of your applications.</p>"},{"location":"deployments/","title":"Deployments","text":""},{"location":"deployments/#kubernetes-deployments","title":"Kubernetes Deployments","text":"<p>Deployments in Kubernetes provide powerful capabilities for managing stateless applications. They enable features like self-healing, scaling, rolling updates, and versioned rollbacks, making it easier to maintain robust and scalable applications.</p>"},{"location":"deployments/#key-concepts-of-deployments","title":"Key Concepts of Deployments","text":"What is a Deployment? <p>A Deployment in Kubernetes is a resource that manages a set of identical Pods, ensuring they are up and running as specified. Deployments provide a declarative way to manage updates and scaling of applications.</p> Why Use Deployments? <p>Deployments add several benefits to managing applications:</p> <ul> <li>Self-Healing: Automatically replaces failed Pods.</li> <li>Scaling: Adjusts the number of running Pods based on demand.</li> <li>Rolling Updates: Updates Pods without downtime.</li> <li>Rollbacks: Easily revert to previous versions if something goes wrong.</li> </ul>"},{"location":"deployments/#deployment-architecture","title":"Deployment Architecture","text":"Components <p>Deployments consist of two main components:</p> <ul> <li>Deployment Resource: Defines the desired state and configuration.</li> <li>Deployment Controller: Monitors the Deployment and ensures the current state matches the desired state through reconciliation.</li> </ul> Deployment and ReplicaSets <p>Deployments manage Pods indirectly through ReplicaSets. A ReplicaSet ensures a specified number of Pod replicas are running at any given time. The Deployment controller creates and manages ReplicaSets as needed to fulfill the Deployment's desired state.</p> <p>rollouts and rollbacksrollouts and rollbacksdeployscaling and self-healingscaling and self-healingrsshared exec. environmentshared exec. environmentpod </p> <p>That diagram may look overly complex and bloated with all of the layers of abstraction, but each layer provides powerful value-adds.</p>"},{"location":"deployments/#creating-and-managing-deployments","title":"Creating and Managing Deployments","text":"Creating a Deployment <p>You can create a Deployment using a YAML file that specifies the configuration.</p> <p>Example YAML for Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web-container\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>To apply this configuration, use the following command: <pre><code>kubectl apply -f deployment.yaml\n</code></pre> This command posts the deployment configuration to the Kubernetes API server, which will create the specified number of Pods and manage them according to the defined state.</p> <p>Example output: <pre><code>deployment.apps/web-deployment created\n</code></pre></p> Scaling a Deployment <p>You can scale a Deployment either imperatively or declaratively.</p> <p>Imperative Scaling: <pre><code>kubectl scale deploy web-deployment --replicas=5\n</code></pre> This command instructs Kubernetes to scale the number of Pods in the <code>web-deployment</code> Deployment to 5. Imperative commands are useful for quick changes.</p> <p>Example output: <pre><code>deployment.apps/web-deployment scaled\n</code></pre></p> <p>Declarative Scaling: Update the <code>replicas</code> field in your Deployment YAML file and apply the changes: <pre><code>spec:\n  replicas: 5\n</code></pre> <pre><code>kubectl apply -f deployment.yaml\n</code></pre> This method involves updating the YAML file to reflect the desired state and applying it. It aligns with the declarative model, where you describe the desired state and let Kubernetes handle the rest.</p> <p>Example output: <pre><code>deployment.apps/web-deployment configured\n</code></pre></p> Rolling Updates <p>Rolling updates allow you to update your application without downtime. Kubernetes gradually replaces old Pods with new ones.</p> <p>To update the image version in your Deployment, modify the YAML file: <pre><code>spec:\n  template:\n    spec:\n      containers:\n      - name: web-container\n        image: nginx:1.16.0\n</code></pre> Apply the updated YAML file: <pre><code>kubectl apply -f deployment.yaml\n</code></pre> This command updates the deployment with the new image version, triggering a rolling update.</p> <p>Example output: <pre><code>deployment.apps/web-deployment configured\n</code></pre></p> Monitoring Rollouts <p>You can monitor the status of a rollout using the following command: <pre><code>kubectl rollout status deploy web-deployment\n</code></pre> This command provides real-time feedback on the status of the deployment rollout, allowing you to ensure that the update is proceeding as expected.</p> <p>Example output: <pre><code>deployment \"web-deployment\" successfully rolled out\n</code></pre></p> Pausing and Resuming Rollouts <p>If needed, you can pause and resume rollouts:</p> <p>Pause: <pre><code>kubectl rollout pause deploy web-deployment\n</code></pre> Pausing a rollout halts the update process, which can be useful if you need to troubleshoot or make additional changes.</p> <p>Example output: <pre><code>deployment.apps/web-deployment paused\n</code></pre></p> <p>Resume: <pre><code>kubectl rollout resume deploy web-deployment\n</code></pre> Resuming a rollout continues the update process from where it was paused.</p> <p>Example output: <pre><code>deployment.apps/web-deployment resumed\n</code></pre></p> Rolling Back a Deployment <p>If an update causes issues, you can roll back to a previous version. Kubernetes retains old ReplicaSets for this purpose.</p> <p>To roll back to the previous version: <pre><code>kubectl rollout undo deploy web-deployment\n</code></pre> This command reverts the deployment to the last stable configuration.</p> <p>Example output: <pre><code>deployment.apps/web-deployment rolled back\n</code></pre></p> <p>For more control, you can specify a particular revision: <pre><code>kubectl rollout undo deploy web-deployment --to-revision=1\n</code></pre> This command rolls back the deployment to a specified revision, providing finer control over the rollback process.</p> <p>Example output: <pre><code>deployment.apps/web-deployment rolled back to revision 1\n</code></pre></p>"},{"location":"deployments/#advanced-features","title":"Advanced Features","text":"Autoscaling <p>Kubernetes supports various autoscalers:</p> <ul> <li>Horizontal Pod Autoscaler (HPA): Adjusts the number of Pods based on CPU/memory usage.</li> <li>Vertical Pod Autoscaler (VPA): Adjusts resource limits/requests for running Pods.</li> <li>Cluster Autoscaler (CA): Adjusts the number of nodes in the cluster.</li> </ul> <p>Example of HPA: <pre><code>kubectl autoscale deploy web-deployment --cpu-percent=50 --min=1 --max=10\n</code></pre> This command sets up an HPA for the <code>web-deployment</code>, adjusting the number of Pods to maintain average CPU usage at 50%, with a minimum of 1 Pod and a maximum of 10 Pods.</p> <p>Example output: <pre><code>horizontalpodautoscaler.autoscaling/web-deployment autoscaled\n</code></pre></p> Declarative vs. Imperative Management <p>Kubernetes prefers a declarative approach, where you define the desired state in YAML files, and Kubernetes manages the steps to achieve that state. This contrasts with the imperative approach, where you issue commands to achieve the desired state.</p> <p>Declarative Example: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web-container\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>Apply the file with: <pre><code>kubectl apply -f deployment.yaml\n</code></pre> This command posts the deployment configuration to the Kubernetes API server, which then ensures the desired state is maintained.</p> <p>Example output: <pre><code>deployment.apps/web-deployment created\n</code></pre></p>"},{"location":"deployments/#practical-exercise","title":"Practical Exercise","text":"Deploying a Sample Application <p>Create a YAML file (<code>sample-deployment.yaml</code>) with the following content: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample\n  template:\n    metadata:\n      labels:\n        app: sample\n    spec:\n      containers:\n      - name: sample-container\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>Deploy it: <pre><code>kubectl apply -f sample-deployment.yaml\n</code></pre> This command creates the sample deployment as specified in the YAML file.</p> <p>Example output: <pre><code>deployment.apps/sample-deployment created\n</code></pre></p> Scaling the Application <p>Scale the Deployment to 5 replicas: <pre><code>kubectl scale deploy sample-deployment --replicas=5\n</code></pre> This command scales the number of Pods in the <code>sample-deployment</code> Deployment to 5.</p> <p>Example output: <pre><code>deployment.apps/sample-deployment scaled\n</code></pre></p> <p>Verify the scaling: <pre><code>kubectl get deploy sample-deployment\n</code></pre> This command checks the status of the <code>sample-deployment</code> Deployment, confirming the number of running replicas.</p> <p>Example output: <pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE\nsample-deployment  5/5     5            5           5m\n</code></pre></p> Updating the Application <p>Update the image version to <code>nginx:1.16.0</code> in the YAML file and apply the changes: <pre><code>kubectl apply -f sample-deployment.yaml\n</code></pre> This command updates the deployment with the new image version, triggering a rolling update.</p> <p>Example output: <pre><code>deployment.apps/sample-deployment configured\n</code></pre></p> <p>Monitor the rollout: <pre><code>kubectl rollout status deploy sample-deployment\n</code></pre> This command provides real-time feedback on the status of</p> <p>the deployment rollout, ensuring that the update is proceeding as expected.</p> <p>Example output: <pre><code>deployment \"sample-deployment\" successfully rolled out\n</code></pre></p> Rolling Back the Application <p>Rollback to the previous version: <pre><code>kubectl rollout undo deploy sample-deployment\n</code></pre> This command reverts the deployment to the last stable configuration.</p> <p>Example output: <pre><code>deployment.apps/sample-deployment rolled back\n</code></pre></p>"},{"location":"deployments/#summary","title":"Summary","text":"<p>Deployments in Kubernetes offer a robust mechanism for managing stateless applications. By leveraging features like self-healing, scaling, rolling updates, and rollbacks, you can ensure your applications are resilient, scalable, and easy to maintain. Embracing the declarative model simplifies management and aligns with Kubernetes' principles of infrastructure as code.</p>"},{"location":"helm/","title":"Helm and Kubernetes Package Management","text":"<p>Helm is a powerful tool for managing Kubernetes applications. It simplifies application deployment and management by using packages called \"charts.\" This page will cover an introduction to Helm, its benefits, and how to create and use Helm charts effectively.</p>"},{"location":"helm/#introduction-to-helm","title":"Introduction to Helm","text":"What is Helm? <p>Helm is a package manager for Kubernetes that allows you to define, install, and upgrade complex Kubernetes applications. It uses a packaging format called charts, which are collections of files that describe a related set of Kubernetes resources.</p> Benefits of Using Helm <p>Helm provides several benefits for managing Kubernetes applications:</p> <ul> <li>Simplifies Deployment: Packages multiple Kubernetes resources into a single unit, making it easier to deploy complex applications.</li> <li>Versioning: Supports versioning of charts, enabling easy upgrades and rollbacks.</li> <li>Reuse: Allows you to reuse charts for different environments, reducing duplication.</li> <li>Customization: Supports customizable templates to adapt to different environments and configurations.</li> <li>Dependency Management: Manages dependencies between different charts.</li> </ul> How Helm Works <p>Helm operates with two main components:</p> <ol> <li>Helm Client: The command-line tool that you use to create, install, and manage Helm charts.</li> <li>Helm Server (Tiller): In Helm v2, Tiller runs inside the Kubernetes cluster and manages the deployment of charts. Note that Helm v3 has removed Tiller, and the client communicates directly with the Kubernetes API server.</li> </ol>"},{"location":"helm/#creating-and-using-helm-charts","title":"Creating and Using Helm Charts","text":"Creating a Helm Chart <p>To create a new Helm chart, use the following command: <pre><code>$ helm create my-chart\n</code></pre></p> <p>This command generates a directory structure with default files: <pre><code>my-chart/\n  Chart.yaml          # Chart metadata\n  values.yaml         # Default configuration values\n  charts/             # Dependency charts\n  templates/          # Kubernetes resource templates\n</code></pre></p> Example Chart.yaml <p><code>Chart.yaml</code> contains metadata about the chart: <pre><code>apiVersion: v2\nname: my-chart\ndescription: A Helm chart for Kubernetes\ntype: application\nversion: 0.1.0\nappVersion: 1.0.0\n</code></pre></p> Example values.yaml <p><code>values.yaml</code> contains default configuration values: <pre><code>replicaCount: 3\nimage:\n  repository: myimage\n  tag: latest\n  pullPolicy: IfNotPresent\nservice:\n  type: ClusterIP\n  port: 80\n</code></pre></p> Example Deployment Template <p><code>templates/deployment.yaml</code> defines a Kubernetes Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Chart.Name }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      app: {{ .Chart.Name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Chart.Name }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n        ports:\n        - containerPort: {{ .Values.service.port }}\n</code></pre></p> Using Helm Charts Installing a Chart <p>To install a chart, use the following command: <pre><code>$ helm install my-release my-chart/\n</code></pre></p> <p>This command installs the chart with the release name <code>my-release</code>.</p> Customizing Values <p>Override default values by specifying a custom <code>values.yaml</code> file: <pre><code>$ helm install my-release -f custom-values.yaml my-chart/\n</code></pre></p> <p>Or by using the <code>--set</code> flag: <pre><code>$ helm install my-release --set replicaCount=5 my-chart/\n</code></pre></p> Upgrading a Release <p>To upgrade an existing release with new values or chart versions: <pre><code>$ helm upgrade my-release my-chart/\n</code></pre></p> Rolling Back a Release <p>To roll back to a previous release: <pre><code>$ helm rollback my-release 1\n</code></pre></p> Customizing Helm Charts <p>You can customize Helm charts by modifying templates and values. Here are some tips:</p> Using Templates <p>Helm uses the Go templating language. You can define templates with placeholders for dynamic values. For example: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Release.Name }}-{{ .Chart.Name }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Release.Name }}-{{ .Chart.Name }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n</code></pre></p> Template Functions <p>Helm templates support functions for advanced customization. For example, you can use the <code>default</code> function to provide fallback values: <pre><code>image: \"{{ .Values.image.repository | default \"nginx\" }}\"\n</code></pre></p> Conditional Logic <p>Use conditional statements to include or exclude resources based on values: <pre><code>{{- if .Values.service.enabled }}\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-{{ .Chart.Name }}\nspec:\n  type: {{ .Values.service.type }}\n  ports:\n  - port: {{ .Values.service.port }}\n{{- end }}\n</code></pre></p>"},{"location":"helm/#practical-example","title":"Practical Example","text":"Creating and Deploying a Custom Helm Chart <ol> <li> <p>Create a new Helm chart: <pre><code>$ helm create custom-chart\n</code></pre></p> </li> <li> <p>Customize <code>values.yaml</code>: <pre><code>replicaCount: 2\nimage:\n  repository: nginx\n  tag: stable\n  pullPolicy: IfNotPresent\nservice:\n  type: NodePort\n  port: 80\n</code></pre></p> </li> <li> <p>Customize <code>templates/deployment.yaml</code>: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Release.Name }}-{{ .Chart.Name }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      app: {{ .Release.Name }}-{{ .Chart.Name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Release.Name }}-{{ .Chart.Name }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n        ports:\n        - containerPort: {{ .Values.service.port }}\n</code></pre></p> </li> <li> <p>Deploy the custom chart: <pre><code>$ helm install my-custom-release custom-chart/\n</code></pre></p> </li> <li> <p>Verify the deployment: <pre><code>$ kubectl get pods\n</code></pre></p> </li> </ol>"},{"location":"helm/#summary","title":"Summary","text":"<p>Helm simplifies the management of Kubernetes applications by using charts to package, deploy, and manage resources. By creating and using Helm charts, you can streamline the deployment process, manage configurations, and take advantage of Helm's powerful templating capabilities to customize your applications for different environments.</p>"},{"location":"ingress/","title":"Managing Ingress in Kubernetes","text":"<p>Ingress in Kubernetes allows you to manage external access to your services, typically HTTP. It provides features like load balancing, SSL termination, and name-based virtual hosting, enabling multiple services to be accessed through a single load balancer.</p>"},{"location":"ingress/#introduction-to-ingress","title":"Introduction to Ingress","text":"Why Use Ingress? <p>Ingress offers a way to expose multiple applications through a single load balancer, addressing the limitations of NodePort and LoadBalancer services:</p> <ul> <li>NodePort services use high port numbers and require clients to track node IP addresses.</li> <li>LoadBalancer services create a one-to-one mapping between internal services and cloud load balancers, which can be costly and limited by cloud provider quotas.</li> </ul> How Ingress Works <p>Ingress is defined in the <code>networking.k8s.io/v1</code> API group and operates at Layer 7 of the OSI model, allowing it to inspect HTTP headers and forward traffic based on hostnames and paths. It requires two main constructs:</p> <ol> <li>Ingress Resource: Defines routing rules.</li> <li>Ingress Controller: Implements the routing rules. Unlike other Kubernetes resources, Ingress controllers are not built-in and must be installed separately.</li> </ol> <p>11Cloud LB: Public IP &amp; DNS, low portCloud LB: Public IP &amp; DNS, low portIngress ruleseggs.food.com --&gt; svc-eggsham.food.com --&gt; svc-hamfood.com/eggs --&gt; svc-eggsfood.com/ham --&gt; svc-hamIngress rules...ingsvc-eggssvc-eggssvcsvc-hamsvc-hamsvcpod2233334444podpodpodpod</p> <p>In this example, the traffic flow is as follows:</p> <ol> <li>Client requests eggs.food.com or food.com/eggs and hits the public load-balancer.</li> <li>The request is forwarded to the Ingress controller.</li> <li>HTTP headers are inspected and Ingress rules trigger and route traffic to svc-eggs.</li> <li>The svc-eggs Service forwards the request to a healthy Pod listed in its EndpointSlice.</li> </ol>"},{"location":"ingress/#setting-up-ingress","title":"Setting Up Ingress","text":"Installing an Ingress Controller <p>To use Ingress, you need an Ingress controller. This example uses the NGINX Ingress controller:</p> <p>1. Install the NGINX Ingress Controller: <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.4/deploy/static/provider/cloud/deploy.yaml\n</code></pre></p> <p>2. Check the Ingress Controller Pod: <pre><code>kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx\n</code></pre></p> Configuring Ingress Class <p>Ingress classes allow multiple Ingress controllers to coexist in a single cluster:</p> <p>1. List Ingress Classes: <pre><code>kubectl get ingressclass\n</code></pre></p> <p>2. Describe Ingress Class: <pre><code>kubectl describe ingressclass nginx\n</code></pre></p>"},{"location":"ingress/#creating-and-managing-ingress-resources","title":"Creating and Managing Ingress Resources","text":"Deploying Sample Applications <p>1. Deploy Apps and Services: <pre><code>kubectl apply -f app.yml\n</code></pre></p> <p>app.yml: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-bacon\nspec:\n  selector:\n    app: bacon\n  ports:\n    - port: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-eggs\nspec:\n  selector:\n    app: eggs\n  ports:\n    - port: 8080\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: bacon\n  labels:\n    app: bacon\nspec:\n  containers:\n    - name: bacon\n      image: mybaconimage\n      ports:\n        - containerPort: 8080\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: eggs\n  labels:\n    app: eggs\nspec:\n  containers:\n    - name: eggs\n      image: myeggsimage\n      ports:\n        - containerPort: 8080\n</code></pre></p> Configuring Ingress Resource <p>2. Deploy Ingress Resource: <pre><code>kubectl apply -f ig-all.yml\n</code></pre></p> <p>ig-all.yml: <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: breakfast-all\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: bacon.breakfast.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: svc-bacon\n                port:\n                  number: 8080\n    - host: eggs.breakfast.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: svc-eggs\n                port:\n                  number: 8080\n    - host: breakfast.com\n      http:\n        paths:\n          - path: /bacon\n            pathType: Prefix\n            backend:\n              service:\n                name: svc-bacon\n                port:\n                  number: 8080\n          - path: /eggs\n            pathType: Prefix\n            backend:\n              service:\n                name: svc-eggs\n                port:\n                  number: 8080\n</code></pre></p> Verifying Ingress Setup <p>3. Check Ingress Resource: <pre><code>kubectl get ing\n</code></pre></p> <p>4. Describe Ingress Resource: <pre><code>kubectl describe ing breakfast-all\n</code></pre></p>"},{"location":"ingress/#configuring-dns-for-ingress","title":"Configuring DNS for Ingress","text":"<p>To route traffic correctly, configure DNS to point to the Ingress load balancer's IP:</p> <p>1. Edit /etc/hosts: <pre><code>212.2.246.150 bacon.breakfast.com\n212.2.246.150 eggs.breakfast.com\n212.2.246.150 breakfast.com\n</code></pre></p>"},{"location":"ingress/#testing-ingress","title":"Testing Ingress","text":"<p>Open a web browser and try accessing the following URLs:</p> <ul> <li><code>bacon.breakfast.com</code></li> <li><code>eggs.breakfast.com</code></li> <li><code>breakfast.com/bacon</code></li> <li><code>breakfast.com/eggs</code></li> </ul>"},{"location":"ingress/#advanced-ingress-concepts","title":"Advanced Ingress Concepts","text":"Session Affinity <p>Session Affinity ensures that requests from the same client go to the same Pod, which is useful for stateful applications.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-affinity-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  sessionAffinity: ClientIP\n</code></pre></p> External Traffic Policy <p>External Traffic Policy specifies whether traffic from outside the cluster is routed only to Pods on the same node or across all nodes.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  externalTrafficPolicy: Local\n</code></pre></p>"},{"location":"ingress/#troubleshooting-ingress","title":"Troubleshooting Ingress","text":"Common Issues and Solutions <p>1. Service Not Accessible:    - Check Service and Pod status:      <pre><code>kubectl get svc\nkubectl get pods\n</code></pre>    - Ensure selectors match Pod labels.</p> <p>2. DNS Resolution Fails:    - Verify cluster DNS is running:      <pre><code>kubectl get pods -n kube-system -l k8s-app=kube-dns\n</code></pre>    - Check <code>/etc/resolv.conf</code> in Pods.</p> Practical Tips <ul> <li>Use <code>kubectl logs</code> to inspect Ingress controller logs.</li> <li>Restart Ingress controller Pods if necessary:   <pre><code>kubectl delete pod -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx\n</code></pre></li> </ul>"},{"location":"ingress/#summary","title":"Summary","text":"<p>Ingress in Kubernetes provides a powerful way to manage external access to your services. By understanding and utilizing Ingress, you can efficiently route traffic to multiple services using a single load balancer, ensuring scalability and ease of management.</p>"},{"location":"kubernetes-api/","title":"Kubernetes API","text":""},{"location":"kubernetes-api/#mastering-the-kubernetes-api","title":"Mastering the Kubernetes API","text":"<p>Understanding the Kubernetes API is essential for mastering Kubernetes. It serves as the backbone of the platform, allowing you to manage resources programmatically and automate cluster operations.</p>"},{"location":"kubernetes-api/#overview-of-the-kubernetes-api","title":"Overview of the Kubernetes API","text":"The Big Picture <p>Kubernetes is an API-centric platform. All resources, such as Pods, Services, and StatefulSets, are defined through the API and managed by the API server. Administrators and clients interact with the cluster by sending requests to create, read, update, and delete these resources. Most interactions are done using <code>kubectl</code>, but they can also be crafted in code or generated through API development tools.</p> Key Concepts <ul> <li>API Server: The central component that exposes the API and handles requests.</li> <li>Resources and Objects: Resources like Pods and Services are defined in the API. When deployed to a cluster, these resources are often called objects.</li> <li>Serialization: The process of converting an object into a string or stream of bytes for transmission or storage. Kubernetes supports JSON and Protobuf for serialization.</li> </ul> How the API Works <p>The Kubernetes API server is the central hub through which all interactions in the cluster are routed, functioning as the front-end interface for Kubernetes' API. Picture it as the Grand Central Station of Kubernetes \u2014 every command, status update, and inter-service communication passes through the API server via RESTful calls over HTTPS. Here's a snapshot of how it operates:</p> <ul> <li><code>kubectl</code> commands are directed to the API server, whether it's for creating, retrieving, updating, or deleting Kubernetes objects.</li> <li>Node Kubelets keep an eye on the API server, picking up new tasks and sending back their statuses.</li> <li>The control plane services don't chat amongst themselves directly; they communicate through the API server.</li> </ul>"},{"location":"kubernetes-api/#understanding-serialization","title":"Understanding Serialization","text":"<p>Serialization is essential for transmitting and storing objects. Kubernetes typically uses JSON for communication with external clients and Protobuf for internal cluster traffic due to its efficiency.</p> Example: Serialization in Action <p>When a client like <code>kubectl</code> posts a request, it serializes the object as JSON. The API server then processes this request and sends back a serialized response.</p>"},{"location":"kubernetes-api/#the-api-server","title":"The API Server","text":"Role and Function <p>The API server is the front-end to the Kubernetes API, handling all RESTful HTTPS requests. It manages all interactions between internal components and external clients.</p> Components <ul> <li>Control Plane Service: Runs as a set of Pods in the <code>kube-system</code> Namespace.</li> <li>TLS and Authentication: Ensures secure communication and validates requests.</li> <li>RESTful Interface: Supports CRUD operations via standard HTTP methods (POST, GET, PUT, PATCH, DELETE).</li> </ul> Example: Using the API <p>A typical <code>kubectl</code> command translates into a REST request: <pre><code>kubectl get pods --namespace eggs\n</code></pre> This command converts to: <pre><code>GET /api/v1/namespaces/eggs/pods\n</code></pre></p>"},{"location":"kubernetes-api/#hands-on-with-the-api","title":"Hands-On with the API","text":"Exploring the API <p>1. Start a Proxy Session: <pre><code>kubectl proxy --port 9000 &amp;\n</code></pre>    This command starts a local proxy to the Kubernetes API server, allowing you to interact with the API using <code>curl</code> or other HTTP clients on <code>http://localhost:9000</code>.</p> <p>2. Using <code>curl</code> to Interact with the API: <pre><code>curl -X GET http://localhost:9000/api/v1/namespaces/eggs/pods\n</code></pre>    This command sends a GET request to the API server to retrieve information about Pods in the <code>eggs</code> Namespace.</p> <p>Example output: <pre><code>{\n  \"kind\": \"PodList\",\n  \"apiVersion\": \"v1\",\n  \"items\": []\n}\n</code></pre></p> Creating Resources <p>1. Define a Namespace:    Create a JSON file (<code>ns.json</code>):    <pre><code>{\n  \"kind\": \"Namespace\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"eggs\",\n    \"labels\": {\n      \"chapter\": \"api\"\n    }\n  }\n}\n</code></pre></p> <p>2. Post the Namespace: <pre><code>curl -X POST -H \"Content-Type: application/json\" --data-binary @ns.json http://localhost:9000/api/v1/namespaces\n</code></pre>    This command posts the JSON data to the API server, creating a new Namespace called <code>eggs</code>.</p> <p>Example output: <pre><code>{\n  \"kind\": \"Namespace\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"eggs\",\n    \"selfLink\": \"/api/v1/namespaces/eggs\",\n    \"uid\": \"abcd1234-5678-90ef-ghij-klmnopqrstuv\",\n    \"resourceVersion\": \"123456\",\n    \"creationTimestamp\": \"2024-06-07T12:34:56Z\",\n    \"labels\": {\n      \"chapter\": \"api\"\n    }\n  }\n}\n</code></pre></p> <p>3. Verify Creation: <pre><code>kubectl get namespaces\n</code></pre>    This command lists all Namespaces in the cluster, allowing you to verify the creation of the <code>eggs</code> Namespace.</p> <p>Example output: <pre><code>NAME          STATUS   AGE\ndefault       Active   84d\nkube-system   Active   84d\neggs          Active   1m\n</code></pre></p> <p>4. Delete the Namespace: <pre><code>curl -X DELETE http://localhost:9000/api/v1/namespaces/eggs\n</code></pre>    This command deletes the <code>eggs</code> Namespace.</p> <p>Example output: <pre><code>{\n  \"kind\": \"Namespace\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"eggs\",\n    \"deletionTimestamp\": \"2024-06-07T12:36:00Z\"\n  },\n  \"status\": {\n    \"phase\": \"Terminating\"\n  }\n}\n</code></pre></p>"},{"location":"kubernetes-api/#inspecting-the-api","title":"Inspecting the API","text":"Useful Commands <p>1. List All API Resources: <pre><code>kubectl api-resources\n</code></pre>    This command lists all available API resources in the cluster.</p> <p>Example output: <pre><code>NAME                  SHORTNAMES   APIGROUP                       NAMESPACED   KIND\npods                  po                                        true          Pod\nservices              svc                                       true          Service\ndeployments           deploy        apps                        true          Deployment\n...\n</code></pre></p> <p>2. List Supported API Versions: <pre><code>kubectl api-versions\n</code></pre>    This command lists all API versions supported by the cluster.</p> <p>Example output: <pre><code>v1\napps/v1\nbatch/v1\nextensions/v1beta1\n...\n</code></pre></p> <p>3. Inspect Specific Resources: <pre><code>kubectl explain pods\n</code></pre>    This command provides detailed information about the <code>Pod</code> resource, including its fields and their descriptions.</p> <p>Example output: <pre><code>KIND:     Pod\nVERSION:  v1\n\nDESCRIPTION:\n     Pod is a collection of containers that can run on a host. This resource\n     is created by clients and scheduled onto hosts.\n\nFIELDS:\n   apiVersion   &lt;string&gt;\n   kind         &lt;string&gt;\n   metadata     &lt;Object&gt;\n   spec         &lt;Object&gt;\n   status       &lt;Object&gt;\n</code></pre></p> <p>4. Using <code>curl</code> to Explore: <pre><code>curl http://localhost:9000/apis\n</code></pre>    This command lists all API groups and their versions available in the cluster.</p> <p>Example output: <pre><code>{\n  \"kind\": \"APIGroupList\",\n  \"apiVersion\": \"v1\",\n  \"groups\": [\n    {\n      \"name\": \"apps\",\n      \"versions\": [\n        {\n          \"groupVersion\": \"apps/v1\",\n          \"version\": \"v1\"\n        }\n      ],\n      \"preferredVersion\": {\n        \"groupVersion\": \"apps/v1\",\n        \"version\": \"v1\"\n      }\n    },\n    ...\n  ]\n}\n</code></pre></p>"},{"location":"kubernetes-api/#extending-the-api","title":"Extending the API","text":"Custom Resources <p>Kubernetes allows you to extend the API with CustomResourceDefinitions (CRDs). These custom resources behave like native Kubernetes resources, enabling you to manage new types of objects within your cluster.</p> Example CRD <p>crd.yml: <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: recipes.breakfast.com\nspec:\n  group: breakfast.com\n  scope: Cluster\n  names:\n    plural: recipes\n    singular: recipe\n    kind: Recipe\n    shortNames:\n    - rp\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                bookTitle:\n                  type: string\n                topic:\n                  type: string\n                edition:\n                  type: integer\n</code></pre></p> Deploying the CRD <p>1. Apply the CRD: <pre><code>kubectl apply -f crd.yml\n</code></pre>    This command creates the custom resource definition in the cluster.</p> <p>Example output: <pre><code>customresourcedefinition.apiextensions.k8s.io/recipes.breakfast.com created\n</code></pre></p> <p>2. Create an Instance:    Create a YAML file (<code>eggs.yml</code>) for the custom resource:    ```yaml    apiVersion:</p> <p>breakfast.com/v1    kind: Recipe    metadata:      name: scrambled    spec:      bookTitle: \"Breakfast Recipes\"      topic: Eggs      edition: 1    ```</p> <p>3. Apply the Instance: <pre><code>kubectl apply -f eggs.yml\n</code></pre>    This command creates an instance of the custom resource.</p> <p>Example output: <pre><code>recipe.breakfast.com/scrambled created\n</code></pre></p> <p>4. Verify Creation: <pre><code>kubectl get rp\n</code></pre>    This command lists all instances of the custom resource.</p> <p>Example output: <pre><code>NAME        AGE\nscrambled   1m\n</code></pre></p>"},{"location":"kubernetes-api/#summary","title":"Summary","text":"<p>The Kubernetes API is a powerful tool for managing your cluster. By understanding its structure and capabilities, you can leverage it to automate and streamline your operations. From creating resources to extending the API with custom definitions, mastering the API is key to unlocking Kubernetes' full potential.</p>"},{"location":"local-setup/","title":"Local Setup","text":""},{"location":"local-setup/#getting-a-local-kubernetes-cluster","title":"Getting a Local Kubernetes Cluster","text":"<p>For most users, setting up a local Kubernetes cluster using Docker Desktop or KinD (Kubernetes in Docker) is the best option when learning. It's free and allows you to quickly and easily get your hands on and start playing with Kubernetes.</p>"},{"location":"local-setup/#option-1-docker-desktop","title":"Option 1: Docker Desktop","text":"<p>Docker Desktop is a straightforward way to get Docker, Kubernetes, and <code>kubectl</code> on your computer, along with a user-friendly interface for managing your cluster contexts.</p> <p>1. Install Docker Desktop:</p> <ul> <li>Download and run the installer for your operating system from the Docker website.</li> <li>Follow the installation prompts. For Windows users, install the WSL 2 subsystem when prompted.</li> </ul> <p>2. Enable Kubernetes in Docker Desktop:</p> <ul> <li>Click the Docker icon in your menu bar or system tray and go to Settings.</li> <li>Select \"Kubernetes\" from the left navigation bar.</li> <li>Check \"Enable Kubernetes\" and click \"Apply &amp; restart.\"</li> <li>Wait a few minutes for Docker Desktop to pull the required images and start the cluster. The Kubernetes icon in the Docker Desktop window will turn green when the cluster is ready.</li> </ul> <p>3. Verify the Installation:</p> <ul> <li>Open a terminal and run the following commands to ensure Docker and <code>kubectl</code> are installed and working:      <pre><code>docker --version\nkubectl version --client=true -o yaml\n</code></pre></li> <li>Ensure the cluster is running with:      <pre><code>kubectl get nodes\n</code></pre>      This command lists all the nodes in your Kubernetes cluster. You should see at least one node listed, confirming your cluster is up and running.</li> </ul>"},{"location":"local-setup/#option-2-kind","title":"Option 2: KinD","text":"<p>KinD (Kubernetes in Docker) is an excellent tool for running local Kubernetes clusters using Docker containers. It\u2019s lightweight, flexible, and ideal for development and testing. It's my tool of choice for local development/experimentation.</p> <p>Steps to Set Up KinD:</p> <p>Install KinD:</p> <ul> <li>Follow the instructions on the KinD GitHub page to install KinD on your system.</li> <li>For macOS users, you can simply run <code>brew install kind</code> to get up and running quickly.</li> </ul> <p>Create a KinD Cluster: <pre><code>kind create cluster\n</code></pre>    This command sets up a new Kubernetes cluster locally using Docker containers. KinD creates a single-node cluster by default, which is sufficient for most development and testing needs.</p> <p>Verify your cluster is running: <pre><code>kubectl get nodes\n</code></pre>    This command lists all the nodes in your Kubernetes cluster. You should see the node created by KinD, confirming your cluster is up and running.</p>"},{"location":"local-setup/#working-with-kubectl","title":"Working with kubectl","text":"<p><code>kubectl</code> is the command-line tool used to interact with your Kubernetes clusters. It's essential for deploying applications, inspecting and managing cluster resources, and troubleshooting issues.</p> Installation <p>If you've followed the steps to set up either Docker Desktop or KinD, you should already have <code>kubectl</code> installed. If not, you can install it separately:</p> <p>Linux:</p> <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre> <p>Mac:</p> <pre><code>brew install kubectl\n</code></pre> <p>Windows:</p> <p>Download the executable from the official Kubernetes site and add it to your system PATH.</p> Using kubectl <p>Once installed, <code>kubectl</code> allows you to perform various operations on your Kubernetes cluster. Here are a few basic commands to get you started:</p> <ul> <li> <p>Check Cluster Nodes: <pre><code>kubectl get nodes\n</code></pre>   This command lists all nodes in the cluster, showing their status, roles, and other details.</p> </li> <li> <p>Get Cluster Info: <pre><code>kubectl cluster-info\n</code></pre>   This command displays information about the cluster, including the URL of the Kubernetes master and other components.</p> </li> <li> <p>Deploy an Application: <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre>   This command applies a configuration file to the cluster, creating or updating resources defined in the file.</p> </li> <li> <p>Inspect Resources: <pre><code>kubectl get pods\nkubectl describe pod &lt;pod-name&gt;\n</code></pre>   These commands list all pods in the cluster and provide detailed information about a specific pod, respectively.</p> </li> </ul> Setting an Alias for kubectl <p>Instead of typing out <code>kubectl</code> for every command, many Kubernetes users set an alias for it by adding the following to their shell profile:</p> <p><pre><code>alias k=kubectl\n</code></pre> This way, you can use <code>k</code> instead of <code>kubectl</code> in your commands, saving time and effort.</p> <p>Tip</p> <p>Using aliases can significantly speed up your workflow and reduce the chances of making typos in long commands.</p>"},{"location":"local-setup/#summary","title":"Summary","text":"<p>Setting up a local Kubernetes cluster using Docker Desktop or KinD is a great way to get hands-on experience with Kubernetes. Both tools provide an easy and quick way to start working with Kubernetes, allowing you to experiment and learn in a controlled environment. With <code>kubectl</code>, you can manage your cluster and deploy applications, making it an essential tool for any Kubernetes user.</p>"},{"location":"namespaces/","title":"Namespaces","text":""},{"location":"namespaces/#namespaces-in-kubernetes","title":"Namespaces in Kubernetes","text":"<p>Namespaces are a powerful feature in Kubernetes that allow you to segment your cluster into multiple groups, providing organization, isolation, and management capabilities.</p>"},{"location":"namespaces/#understanding-namespaces","title":"Understanding Namespaces","text":"What are Namespaces? <p>In Kubernetes, Namespaces provide a way to partition a single Kubernetes cluster into multiple virtual clusters. This is different from kernel namespaces, which isolate resources at the operating system level.</p> <ul> <li>Kernel Namespaces: Isolate operating system resources for containers.</li> <li>Kubernetes Namespaces: Segment a Kubernetes cluster into separate environments for different teams, projects, or applications.</li> </ul> Benefits of Using Namespaces <p>Namespaces offer several advantages, including:</p> <ul> <li>Organizational Segmentation: Separate environments for development, testing, and production.</li> <li>Resource Management: Apply different resource quotas and policies to each Namespace.</li> <li>Soft Isolation: Prevent resource conflicts and organize cluster resources logically.</li> </ul> <p>ham-svcham-svcnssvcbaconbaconsaltsaltpodpodperf Namespaceperf NamespaceforksforksdeployvinnyvinnysaCPU: 100Memory: 200 GiCPU: 100...quotaeggs-svceggs-svcnssvcfriedfriedpoachedpoachedpodpodqa Namespaceqa NamespacespoonsspoonsdeploysullivansullivansaCPU: 250Memory: 625 GiCPU: 250...quota</p>"},{"location":"namespaces/#practical-use-cases-for-namespaces","title":"Practical Use Cases for Namespaces","text":"<p>Namespaces are ideal for managing environments within a single organization, such as:</p> <ul> <li>Development Environments: Separate Namespaces for dev, test, and production environments.</li> <li>Team-Based Separation: Different teams (e.g., finance, HR, operations) each have their own Namespace.</li> <li>Project-Based Isolation: Isolate projects within the same cluster to avoid resource conflicts.</li> </ul> Limitations of Namespaces <p>Namespaces provide soft isolation, meaning they help organize resources but do not offer strong security isolation. For stronger isolation, consider using separate clusters.</p>"},{"location":"namespaces/#working-with-default-namespaces","title":"Working with Default Namespaces","text":"<p>Every Kubernetes cluster comes with some pre-defined Namespaces:</p> <ul> <li>default: The default Namespace for objects with no specified Namespace.</li> <li>kube-system: Contains system components like DNS and metrics server.</li> <li>kube-public: For resources that should be publicly accessible.</li> <li>kube-node-lease: Manages Node heartbeat and leases.</li> </ul> <p>Note</p> <p>Instead of typing out namespace each time, you can shorten it to ns in <code>kubectl</code> commands.</p> <p>To view the existing Namespaces, use: <pre><code>kubectl get namespaces\n</code></pre> This command lists all the Namespaces in the cluster, showing their status and age.</p> <p>Example output: <pre><code>NAME                 STATUS   AGE\ndefault              Active   84d\nkube-node-lease      Active   84d\nkube-public          Active   84d\nkube-system          Active   84d\nlocal-path-storage   Active   84d\n</code></pre></p> <p>To delete a Namespace: <pre><code>kubectl delete ns my-namespace\n</code></pre> This command deletes the specified Namespace and all the resources within it. Be cautious when deleting Namespaces, as this action is irreversible.</p> <p>Example output: <pre><code>namespace \"my-namespace\" deleted\n</code></pre></p>"},{"location":"namespaces/#creating-and-managing-namespaces","title":"Creating and Managing Namespaces","text":"Creating a Namespace <p>You can create a Namespace either imperatively or declaratively.</p> <p>Imperative Creation: <pre><code>kubectl create ns my-namespace\n</code></pre> This command immediately creates a Namespace named <code>my-namespace</code>. It's a quick and straightforward method for creating Namespaces.</p> <p>Example output: <pre><code>namespace/my-namespace created\n</code></pre></p> <p>Declarative Creation: Create a YAML file (<code>namespace.yaml</code>): <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n  labels:\n    environment: dev\n</code></pre> Apply the YAML file: <pre><code>kubectl apply -f namespace.yaml\n</code></pre> This approach allows you to manage your Namespace as code, making it easier to track changes and automate deployments. The labels can be used for organizational purposes or for applying specific policies.</p> <p>Example output: <pre><code>namespace/my-namespace created\n</code></pre></p> Configuring kubectl for a Specific Namespace <p>To avoid specifying the Namespace in every command, set your context to a specific Namespace: <pre><code>kubectl config set-context --current --namespace=my-namespace\n</code></pre> This command configures <code>kubectl</code> to use <code>my-namespace</code> as the default Namespace for the current context. It simplifies your workflow by eliminating the need to repeatedly specify the Namespace.</p> <p>Example output: <pre><code>Context \"your-current-context\" modified.\n</code></pre></p>"},{"location":"namespaces/#deploying-applications-in-namespaces","title":"Deploying Applications in Namespaces","text":"<p>You can deploy resources to a specific Namespace by specifying it in the YAML file or by using the <code>-n</code> flag with <code>kubectl</code> commands.</p> <p>Example YAML for Deployment: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: my-namespace # specified here\nspec:\n  containers:\n  - name: my-container\n    image: nginx\n    ports:\n    - containerPort: 80\n</code></pre> This YAML file specifies that the Pod <code>my-pod</code> should be deployed in the <code>my-namespace</code> Namespace. By including the <code>namespace</code> field, you ensure the resource is created in the correct Namespace.</p> <p>Apply the YAML file: <pre><code>kubectl apply -f deployment.yaml\n</code></pre> This command deploys the resources defined in <code>deployment.yaml</code> to the specified Namespace.</p> <p>Example output: <pre><code>pod/my-pod created\n</code></pre></p> <p>Using the <code>-n</code> Flag: <pre><code>kubectl get pods -n my-namespace\n</code></pre> The <code>-n</code> flag specifies the Namespace for the <code>kubectl</code> command. This command lists all Pods in the <code>my-namespace</code> Namespace.</p> <p>Example output: <pre><code>NAME        READY   STATUS    RESTARTS   AGE\nmy-pod      1/1     Running   0          1m\n</code></pre></p>"},{"location":"namespaces/#summary","title":"Summary","text":"<p>Namespaces in Kubernetes are an effective way to manage resources and organize environments within a cluster. While they provide soft isolation and ease of management, remember that they are not suitable for hard multi-tenancy. By using Namespaces, you can efficiently segment your cluster and apply different policies and resource quotas to each segment.</p>"},{"location":"networking/","title":"Networking in Kubernetes","text":"<p>Networking is a fundamental aspect of Kubernetes, enabling communication between various components within a cluster and with the outside world. This section covers the Container Network Interface (CNI) and popular CNI plugins, as well as network policies for controlling pod communication.</p>"},{"location":"networking/#container-network-interface-cni","title":"Container Network Interface (CNI)","text":"What is CNI? <p>The Container Network Interface (CNI) is a specification and a set of libraries for configuring network interfaces in Linux containers. It ensures that when a container is created or deleted, its network resources are allocated and cleaned up properly.</p> Role of CNI in Kubernetes <p>Kubernetes uses CNI to manage networking for Pods. When a Pod is created, the CNI plugin is responsible for assigning the Pod an IP address, setting up the network interface, and ensuring connectivity both within the cluster and externally.</p>"},{"location":"networking/#popular-cni-plugins","title":"Popular CNI Plugins","text":"<p>Several CNI plugins are widely used in Kubernetes environments. Each offers different features and capabilities.</p> Calico <p>Calico provides secure network connectivity for containers, virtual machines, and native host-based workloads. It supports a range of features, including:</p> <ul> <li>Network Policy Enforcement: Allows you to define and enforce network policies.</li> <li>BGP for Routing: Uses Border Gateway Protocol (BGP) for high-performance routing.</li> <li>IP-in-IP and VXLAN Encapsulation: Supports various encapsulation methods for different networking needs.</li> </ul>"},{"location":"networking/#understanding-overlay-networking-in-kubernetes","title":"Understanding Overlay Networking in Kubernetes","text":"<p>Overlay networking is a fundamental concept in Kubernetes that allows for the seamless communication of pods across different nodes within a cluster. This approach abstracts the underlying network infrastructure, providing a virtual network that connects all pods regardless of their physical location. The diagram below illustrates the high-level architecture of overlay networking in a Kubernetes cluster.</p> Key Components of the Overlay Network <p>1. Node Network:     - This is the physical network where the Kubernetes nodes (servers) are deployed. Each node in the cluster has a unique IP address within this network.     - Nodes can communicate with each other through this network, enabling pod-to-pod communication across different nodes.</p> <p>2. Pod Network:     - Pods are assigned IP addresses from a logically separate, private CIDR block, distinct from the node network.     - The pod network enables direct communication between pods across different nodes. This network is virtual and managed by Kubernetes through the use of CNI (Container Network Interface) plugins.</p> <p>3. Services Network:     - Kubernetes services provide stable endpoints for accessing pods. They use cluster-internal IP addresses and DNS names to facilitate communication.     - Services abstract the underlying pods, allowing for load balancing and failover.</p> How Overlay Networking Works <p>In overlay networking, each pod receives an IP address from a private CIDR block. This block is logically separate from the network where the nodes are deployed, allowing for greater scalability and simplified network management.</p> Intra-Cluster Communication <ul> <li>Pod-to-Pod Communication: Pods can communicate directly with each other using their assigned IP addresses. The overlay network ensures that these communications are routed correctly, regardless of the pods' physical locations.</li> <li>Service-to-Pod Communication: Services use label selectors to route traffic to the appropriate pods. The service network provides a stable IP and DNS name for accessing the pods, abstracting the complexity of pod IP management.</li> </ul> Exiting the Cluster <ul> <li>When traffic leaves the cluster, it undergoes Source Network Address Translation (SNAT), which translates the pod's IP address to the node's IP address. This process ensures that external systems see the traffic as coming from the node, not the individual pod.</li> <li>Inbound traffic destined for pods is routed through services like load balancers. These services manage the translation and routing of requests to the appropriate pods, hiding the pod IP addresses behind the node's IP address.</li> </ul> Benefits of Overlay Networking <ul> <li>Scalability: The separation of pod and node networks allows for easy scaling of the cluster. New pods and nodes can be added without reconfiguring the underlying network infrastructure.</li> <li>Flexibility: Overlay networks abstract the physical network, providing a consistent and flexible networking environment that can adapt to various underlying infrastructures.</li> <li>Simplicity: Managing a private CIDR block for pods simplifies IP address management and reduces the complexity of networking configurations.</li> </ul> <p>Overlay networking is a powerful model in Kubernetes, enabling seamless and scalable communication between pods and services. By abstracting the underlying network infrastructure, overlay networking allows for greater flexibility and simplicity in managing Kubernetes clusters. The diagram above encapsulates these concepts, illustrating the relationship between the node network, pod network, and services network in a Kubernetes environment.</p> <p>Kubernetes clusterKubernetes clusterServices networkServices networkPod networkPod networkNode networkNode networksvcsvcnodenodepodpodpodpodpod</p> <p>Installation Example: <pre><code>$ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n</code></pre></p> Flannel <p>Flannel is a simple and easy-to-configure CNI plugin designed to provide basic networking for Kubernetes clusters. It supports various backend options for network traffic encapsulation, including VXLAN and host-gw.</p> <p>Key Features:</p> <ul> <li>Simple Overlay Network: Uses a flat network to provide communication between nodes.</li> <li>Multiple Backends: Supports different backend options like VXLAN, host-gw, and UDP.</li> </ul> <p>Installation Example: <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n</code></pre></p> Weave <p>Weave Net is a CNI plugin that creates a virtual network allowing Pods to communicate with each other, regardless of the node they are on. It is designed for simplicity and ease of use.</p> <p>Key Features:</p> <ul> <li>Automatic Mesh Network: Automatically forms a mesh network for all nodes in the cluster.</li> <li>Encryption: Supports network traffic encryption.</li> <li>Network Policy: Integrates with Kubernetes Network Policies for traffic control.</li> </ul> <p>Installation Example: <pre><code>$ kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\n</code></pre></p>"},{"location":"networking/#network-policies","title":"Network Policies","text":"<p>Network policies in Kubernetes provide a way to control the communication between Pods. They use label selectors to define the source and destination Pods and specify the allowed protocols and ports.</p> Defining Network Policies <p>Network policies are defined using YAML files. Here\u2019s a basic example that allows incoming traffic to Pods with the label <code>app: my-app</code> only from Pods with the label <code>role: frontend</code>.</p> <p>Example Network Policy: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      app: my-app\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: frontend\n    ports:\n    - protocol: TCP\n      port: 80\n</code></pre></p> Enforcing Network Policies <p>To enforce network policies, ensure that your CNI plugin supports them. Most advanced CNI plugins, like Calico and Weave, provide full support for Kubernetes network policies.</p> <p>Applying a Network Policy: <pre><code>$ kubectl apply -f networkpolicy.yaml\n</code></pre></p> Key Concepts <ul> <li>PodSelector: Selects the Pods to which the policy applies.</li> <li>Ingress Rules: Define the allowed incoming traffic to the selected Pods.</li> <li>Egress Rules: Define the allowed outgoing traffic from the selected Pods.</li> </ul> Example Use Cases <ol> <li>Isolate Development and Production: Ensure that development Pods cannot communicate with production Pods.</li> <li>Allow Only Specific Services: Permit communication only between specific services, enhancing security.</li> <li>Restrict Traffic by Namespace: Control traffic between different namespaces for better segmentation and security.</li> </ol>"},{"location":"networking/#introduction-to-service-mesh","title":"Introduction to Service Mesh","text":"Concept and Benefits <p>A service mesh abstracts the complexity of service-to-service communication, providing a uniform and consistent way to secure, connect, and monitor microservices. Key benefits include:</p> <ul> <li>Traffic Management: Advanced routing, load balancing, and traffic splitting.</li> <li>Security: Mutual TLS (mTLS) for secure communication between services.</li> <li>Observability: Detailed telemetry and monitoring of service interactions.</li> <li>Resilience: Fault injection, retries, and circuit breaking.</li> </ul> Setting Up Istio <p>Istio is a popular open-source service mesh that provides advanced networking, security, and observability features for microservices.</p> <p>Installation: 1. Download the Istio CLI:    <pre><code>curl -L https://istio.io/downloadIstio | sh -\ncd istio-&lt;version&gt;\nexport PATH=$PWD/bin:$PATH\n</code></pre></p> <ol> <li> <p>Install Istio on your cluster:    <pre><code>istioctl install --set profile=demo -y\n</code></pre></p> </li> <li> <p>Label the namespace for automatic sidecar injection:    <pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre></p> </li> </ol> Using Istio <p>Deploy a sample application: <pre><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre></p> <p>Access the Istio dashboard: <pre><code>kubectl apply -f samples/addons\nistioctl dashboard kiali\n</code></pre></p> Using Linkerd Setting Up Linkerd <p>Linkerd is a lightweight service mesh that provides essential features like observability, security, and reliability.</p> <p>Installation: 1. Install the Linkerd CLI:    <pre><code>curl -sL https://run.linkerd.io/install | sh\nexport PATH=$PATH:$HOME/.linkerd2/bin\n</code></pre></p> <ol> <li> <p>Install Linkerd on your cluster:    <pre><code>linkerd install | kubectl apply -f -\nlinkerd check\n</code></pre></p> </li> <li> <p>Label the namespace for automatic sidecar injection:    <pre><code>kubectl annotate ns default linkerd.io/inject=enabled\n</code></pre></p> </li> </ol> <p>Deploy a sample application: <pre><code>kubectl apply -f https://run.linkerd.io/emojivoto.yml\n</code></pre></p> <p>Access the Linkerd dashboard: <pre><code>linkerd viz dashboard &amp;\n</code></pre></p>"},{"location":"networking/#summary","title":"Summary","text":"<p>Kubernetes networking, facilitated by CNI plugins, is crucial for seamless communication between Pods and external systems. Popular CNI plugins like Calico, Flannel, and Weave offer various features to meet different networking needs. Network policies provide fine-grained control over pod communication, enhancing the security and manageability of your Kubernetes clusters. By understanding and implementing these concepts, you can build a robust and secure networking environment for your applications.</p>"},{"location":"operators/","title":"Kubernetes Operators","text":"<p>Kubernetes Operators extend the functionality of Kubernetes by automating the management of complex applications. They provide a powerful way to manage application-specific logic and stateful workloads.</p>"},{"location":"operators/#what-are-operators","title":"What are Operators?","text":"Introduction to Operators <p>Kubernetes Operators are software extensions that use custom resources to manage applications and their components. They follow Kubernetes principles, notably the control loop, to automate tasks beyond the capabilities of standard Kubernetes resources.</p> Purpose of Operators <p>Operators are designed to automate the entire lifecycle of complex applications, including:</p> <ul> <li>Installation: Deploying and configuring applications.</li> <li>Management: Managing the application's runtime configuration.</li> <li>Scaling: Adjusting resources in response to changing workloads.</li> <li>Healing: Detecting and recovering from failures.</li> <li>Upgrades: Updating the application to new versions.</li> </ul> Key Concepts <p>Operators extend Kubernetes using the following components:</p> <ul> <li>Custom Resource Definitions (CRDs): Define new types of resources to represent the application and its components.</li> <li>Custom Controllers: Monitor the state of custom resources and take action to reconcile the actual state with the desired state.</li> </ul>"},{"location":"operators/#detailed-workflow","title":"Detailed Workflow","text":"<p>custom resourcecustom resourcemodifymodifyoperatoroperatorapichangeeventschange...watchwatchadjuststateadjust...</p>"},{"location":"operators/#user-modification","title":"User Modification","text":"<ul> <li>Action: A user modifies the custom resource to define or update the desired state of the application.</li> <li>Example: The user could be updating the version of the application or changing configuration parameters.</li> </ul> <pre><code># Example command to apply a change to a custom resource\nkubectl apply -f custom-resource.yaml\n</code></pre>"},{"location":"operators/#api-server-interaction","title":"API Server Interaction","text":"<ul> <li>Action: The Kubernetes API server receives the modification request and processes it.</li> <li>Purpose: The API server validates and stores the new state of the custom resource.</li> </ul>"},{"location":"operators/#operator-watches-custom-resource","title":"Operator Watches Custom Resource","text":"<ul> <li>Action: The Operator, running as a controller within the cluster, continuously watches for changes to the custom resources.</li> <li>Purpose: The Operator detects the change event and identifies that an action is needed to reconcile the desired state with the actual state.</li> </ul>"},{"location":"operators/#operator-takes-action","title":"Operator Takes Action","text":"<ul> <li>Action: The Operator performs the necessary operations to adjust the state of the application to match the desired state defined in the custom resource.</li> <li>Examples: This could include creating, updating, or deleting resources such as Pods, Services, or ConfigMaps. The Operator might also perform application-specific actions like running a database migration or initiating a backup.</li> </ul>"},{"location":"operators/#state-adjustment","title":"State Adjustment","text":"<ul> <li>Action: The Operator adjusts the state of the application.</li> <li>Purpose: Ensures the actual state of the cluster matches the desired state defined in the custom resource.</li> </ul>"},{"location":"operators/#continuous-monitoring-and-reconciliation","title":"Continuous Monitoring and Reconciliation","text":"<ul> <li>Action: The Operator continuously monitors the application and custom resources.</li> <li>Purpose: Automatically reconciles any discrepancies between the desired state and the actual state, ensuring the application runs as intended over time.</li> </ul>"},{"location":"operators/#building-and-using-operators","title":"Building and Using Operators","text":"Building an Operator <p>Building a Kubernetes Operator typically involves the following steps:</p> <ol> <li>Define a Custom Resource Definition (CRD): The CRD defines the schema for the custom resource that represents the application.</li> <li>Implement a Custom Controller: The controller monitors the custom resource and implements the logic to manage the application.</li> </ol> Example CRD <p>Define a CRD for a sample application (<code>sample-crd.yaml</code>): <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: samples.apps.example.com\nspec:\n  group: apps.example.com\n  versions:\n  - name: v1\n    served: true\n    storage: true\n    schema:\n      openAPIV3Schema:\n        type: object\n        properties:\n          spec:\n            type: object\n            properties:\n              replicas:\n                type: integer\n  scope: Namespaced\n  names:\n    plural: samples\n    singular: sample\n    kind: Sample\n    shortNames:\n    - smpl\n</code></pre></p> Example Custom Resource <p>Define a custom resource that uses the CRD (<code>sample-cr.yaml</code>): <pre><code>apiVersion: apps.example.com/v1\nkind: Sample\nmetadata:\n  name: my-sample\nspec:\n  replicas: 3\n</code></pre></p> Implementing the Controller <p>The controller is implemented using a programming language like Go. The Operator SDK provides tools and libraries to simplify this process.</p> Install Operator SDK <p>Install the Operator SDK CLI: <pre><code>$ curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v1.16.0/operator-sdk_linux_amd64\n$ chmod +x operator-sdk_linux_amd64\n$ sudo mv operator-sdk_linux_amd64 /usr/local/bin/operator-sdk\n</code></pre></p> Create a New Operator Project <p>Create a new project using the Operator SDK: <pre><code>$ operator-sdk init --domain example.com --repo github.com/example/my-operator\n$ operator-sdk create api --group apps --version v1 --kind Sample --resource --controller\n</code></pre></p> Implement Reconciliation Logic <p>In the generated controller, implement the reconciliation logic to manage the custom resource: <pre><code>func (r *SampleReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {\n    ctx := context.Background()\n    log := r.Log.WithValues(\"sample\", req.NamespacedName)\n\n    // Fetch the Sample instance\n    var sample appsV1.Sample\n    if err := r.Get(ctx, req.NamespacedName, &amp;sample); err != nil {\n        log.Error(err, \"unable to fetch Sample\")\n        return ctrl.Result{}, client.IgnoreNotFound(err)\n    }\n\n    // TODO: Add application management logic here\n\n    return ctrl.Result{}, nil\n}\n</code></pre></p> Deploying an Operator Build and Push the Operator Image <p>Build the Operator image and push it to a container registry: <pre><code>$ make docker-build docker-push IMG=&lt;your-image-registry&gt;/my-operator:v0.1.0\n</code></pre></p> Deploy the Operator <p>Deploy the Operator to the Kubernetes cluster: <pre><code>$ make deploy IMG=&lt;your-image-registry&gt;/my-operator:v0.1.0\n</code></pre></p> Using the Operator <p>Create and manage custom resources using the Operator:</p> <ol> <li> <p>Apply the CRD: <pre><code>$ kubectl apply -f sample-crd.yaml\n</code></pre></p> </li> <li> <p>Create a Custom Resource: <pre><code>$ kubectl apply -f sample-cr.yaml\n</code></pre></p> </li> <li> <p>Check the Status: <pre><code>$ kubectl get samples.apps.example.com\n</code></pre></p> </li> </ol> Advanced Operator Features <p>Operators can include advanced features such as:</p> <ul> <li>Leader Election: Ensures only one instance of the Operator is active at a time.</li> <li>Metrics and Monitoring: Collect and expose metrics for monitoring the Operator's performance.</li> <li>Webhooks: Implement validation and mutating webhooks to enforce policies.</li> </ul>"},{"location":"operators/#summary","title":"Summary","text":"<p>Kubernetes Operators provide a powerful way to manage complex applications by extending Kubernetes with custom resources and controllers. By building and using Operators, you can automate the entire lifecycle of applications, ensuring they are deployed, managed, and scaled efficiently. Operators are a key component in managing stateful and complex workloads in a Kubernetes-native way.</p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#introduction-to-kubernetes","title":"Introduction to Kubernetes","text":"<p>Kubernetes, often referred to as K8s, is an open-source platform designed to automate deploying, scaling, and operating application containers. It was originally developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). This section covers the essentials to get you up to speed with Kubernetes, its architecture, and its key features. Think of this as a one-pager or TLDR of Kubernetes.</p>"},{"location":"overview/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes is a container orchestrator, which means it manages the deployment and operation of containerized applications. Containers are lightweight, portable units that bundle an application and its dependencies, allowing them to run consistently across different environments. Kubernetes automates several tasks:</p> <ul> <li>Deployment: Seamlessly deploys applications by creating and managing containers.</li> <li>Scaling: Adjusts the number of application instances based on demand, ensuring efficient use of resources.</li> <li>Self-healing: Detects and replaces failed instances to maintain application availability.</li> <li>Rolling Updates and Rollbacks: Updates applications without downtime and rolls back if needed to a previous version if something goes wrong.</li> </ul> Declarative Model <p>Kubernetes operates on a declarative model, where you specify the desired state of the system in YAML or JSON configuration files. The system continuously works to ensure the observed state matches the desired state. This involves three key principles:</p> <ol> <li>Observed State: The current state of the system.</li> <li>Desired State: The state you want the system to achieve.</li> <li>Reconciliation: The process of adjusting the observed state to match the desired state.</li> </ol> <p>The declarative nature of Kubernetes is key to understanding its power. At a high level, here's how it works:</p> <p>controlplaneAPI ServerAPI Serverapietcdetcd5. take actionto reconcile5. take action...ControllerManagerController...SchedulerSchedulerc-msched1. declare desired state1. declare desired state2. persist desired state2. persist desired state3. check ifcurrent = desired3. check if...etcd4. no, itdoes not4. no, it...4. no, itdoes not4. no, it...3. check ifcurrent = desired3. check if...5. take actionto reconcile5. take action...</p> <ol> <li>You tell Kubernetes (typically via <code>kubectl</code>) how you want your application to look\u2014what image to use, how many replicas, ports to expose, etc.</li> <li>Kubernetes persists this desired state to the cluster store (etcd).</li> <li>A series of background controllers consistently check if the current state matches the desired state.</li> <li>If the current state does not equal the desired state (e.g., you desire 3 replicas but only 2 are currently running),</li> <li>Kubernetes initiates a series of actions to reconcile the two states. In this example, it would involve spinning up an additional replica.</li> </ol>"},{"location":"overview/#historical-background","title":"Historical Background","text":"<p>Kubernetes was born from Google's internal systems like Borg and Omega, which managed containerized applications like Search and Gmail at a massive scale. In 2014, Google open-sourced Kubernetes, and it quickly became the standard for container orchestration.</p>"},{"location":"overview/#kubernetes-architecture","title":"Kubernetes Architecture","text":"<p>From a 20K-foot level, Kubernetes clusters consist of two types of nodes - control plane nodes and worker nodes:</p> <ul> <li>Control Plane Nodes: These nodes run the Kubernetes control plane, which includes components like the API server, scheduler, and controllers. They manage the overall state of the cluster.</li> <li>Worker Nodes: These nodes run the applications and report back status to the control plane.</li> </ul> Components of the Control Plane <ul> <li>API Server: The front end of Kubernetes that exposes the Kubernetes API. All traffic within, to, and from various Kubernetes components flows through the API Server. It is the Grand Central Station or central nervous system of Kubernetes.</li> <li>Cluster Store: A distributed database (etcd) that stores the entire state of the cluster. When you define your desired application specifications, they are stored here. This is the only stateful core component of Kubernetes.</li> <li>Controllers: Ensure the cluster's desired state matches its observed state by running background watch loops on objects like Deployments, Pods, etc.</li> <li>Scheduler: Assigns tasks to worker nodes based on resource availability, application requirements, and other criteria.</li> </ul> Components of Worker Nodes <ul> <li>Kubelet: The agent that communicates with the API server and manages containers on the node. The kubelet communicates directly with the container runtime on the node, instructing it to pull images, and start/stop containers.</li> <li>Container Runtime: Executes container operations like starting and stopping containers. Common runtimes include containerd and CRI-O.</li> <li>Kube-proxy: Manages networking for containers, including load balancing.</li> </ul> <p>controlplaneretrieve statusstart/stop Podsretrieve status...create localIP rulescreate local...API ServerAPI Serverapipersist &amp;check statepersist &amp;...etcdetcdwatch statereconcile statewatch state...ControllerManagerController...schedule Podson Nodesschedule Pods...SchedulerSchedulerc-mschedetcdnodestart/stopcontainersstart/stop...kubeletkubeletkube-proxykube-proxycontainer runtimecontainer runtimekubeletk-proxy</p> <p>Note</p> <p>The API Server is the only component in Kubernetes that interacts directly with etcd.</p>"},{"location":"overview/#common-features-primer","title":"Common Features Primer","text":"Pods and Deployments <ul> <li>Pods: The smallest deployable units in Kubernetes, which can contain one or more containers. Containers within Pods share resources like network and storage.</li> <li>Deployments: Higher-level controllers that manage Pods, providing features like scaling, rolling updates, and rollbacks.</li> </ul> Services <p>Services provide stable networking endpoints for Pods, enabling reliable communication between different parts of an application. They abstract away the ephemeral nature of Pods, which can be created and destroyed dynamically, and give you a stable, long-lived connection point to the underlying Pods.</p> <p>Selector:project=abzone=prodSelector:...svcpodpodpodpodproject=abzone=prodproject=ab...project=abzone=prodproject=ab...project=abproject=abproject=abzone=prodrelease=24project=ab...</p> Self-Healing and Scaling <p>If deployed as part of a Deployment or StatefulSet, Kubernetes will automatically replace failed Pods and scale your application up or down based on traffic, load, or other custom thresholds. This ensures high availability and efficient resource utilization.</p> Rolling Updates and Rollbacks <p>By leveraging Deployments (via ReplicaSets), Kubernetes allows you to update your application without downtime by gradually replacing old Pods with new ones. If something goes wrong, Kubernetes can roll back to the previous version.</p>"},{"location":"overview/#summary","title":"Summary","text":"<p>Kubernetes is a powerful tool for managing containerized applications, offering automation, scalability, and reliability. By abstracting the underlying infrastructure, it simplifies application deployment and management across various environments. Whether you're running on-premises or in the cloud, Kubernetes provides a consistent and efficient platform for your applications. Before diving into some more details on these topics, let's first cover how you can quickly get your hands on a Kubernetes environment in the next section.</p>"},{"location":"pods/","title":"Pods","text":""},{"location":"pods/#introduction-to-kubernetes-pods","title":"Introduction to Kubernetes Pods","text":"<p>In Kubernetes, every application runs inside a Pod. Understanding how to work with Pods is crucial for deploying, scaling, and managing applications effectively.</p>"},{"location":"pods/#pod-fundamentals","title":"Pod Fundamentals","text":"<p>Pods are the smallest deployable units in Kubernetes and serve as an abstraction layer, allowing various types of workloads to run seamlessly. They enable resource sharing, advanced scheduling, health monitoring, and more.</p> Abstraction and Benefits <p>Pods abstract the complexities of different workload types, enabling Kubernetes to manage them without needing to understand the specifics of each workload. This abstraction allows for uniform deployment and management across heterogeneous environments.</p> <p>python apppython appJava appJava appML modelML modelMySQLdatabaseMySQL...</p> <p>In the image above, all four of those apps are vastly different but once containerized and wrapped in a Pod, Kubernetes treats them all the same and doesn't have to worry about the details of how each application is written or works.</p> Enhancements and Capabilities <p>Pods offer several enhancements for containers, including:</p> <ul> <li>Resource Sharing: Shared filesystem, network stack, memory, process tree, and hostname.</li> <li>Advanced Scheduling: Features like nodeSelectors, affinity rules, topology spread constraints, resource requests, and limits.</li> <li>Health Monitoring and Restart Policies: Probes for application health and policies for container restarts.</li> <li>Security and Termination Control: Enhanced security measures and graceful shutdown processes.</li> <li>Volumes: Shared storage among containers within a Pod.</li> </ul>"},{"location":"pods/#efficient-resource-utilization","title":"Efficient Resource Utilization","text":"Resource Sharing in Pods <p>Pods allow containers to share resources within the same execution environment:</p> <ul> <li>Filesystem and Volumes: Shared through the <code>mnt</code> Linux namespace.</li> <li>Network Stack: Shared via the <code>net</code> Linux namespace.</li> <li>Memory and Process Tree: Shared using the <code>ipc</code> and <code>pid</code> Linux namespaces.</li> <li>Hostname: Shared using the <code>uts</code> Linux namespace.</li> </ul> Scheduling Strategies <p>Kubernetes handles scheduling Pods to Nodes based on several different criteria. For multi-container Pods, Kubernetes ensures all containers within the same Pod are scheduled on the same Node. Kubernetes comes with sensible defaults for scheduling Pods, but some advanced scheduling techniques are available such as:</p> <ul> <li>nodeSelectors: Labels specifying Node requirements.</li> <li>Affinity Rules: Attract or repel Pods based on Node or Pod labels.</li> <li>Topology Spread Constraints: Distribute Pods across zones for high availability.</li> <li>Resource Requests and Limits: Define minimum and maximum resource requirements for Pods.</li> </ul>"},{"location":"pods/#lifecycle-and-management","title":"Lifecycle and Management","text":"Deploying and Managing Pods <p>Deploying a Pod involves several steps:</p> <ol> <li>Define the Pod in a YAML manifest: A YAML file specifying the desired state of the Pod, including containers, volumes, and other resources.</li> <li>Post the manifest to the API server: Using <code>kubectl apply -f &lt;filename&gt;.yaml</code>, the manifest is sent to the Kubernetes API server.</li> <li>API server authentication and authorization: The API server checks if the request is allowed.</li> <li>API server validation: The API server validates the Pod specification against the cluster's policies and configurations.</li> <li>Scheduler assigns the Pod to a Node: The scheduler determines the most suitable node based on resource availability and scheduling policies.</li> <li>Kubelet starts and monitors the Pod: The kubelet on the assigned node starts the containers and continuously monitors their status.</li> </ol> Pod Lifecycle and Immutability <p>Pods are designed to be ephemeral and immutable:</p> <ul> <li>Ephemeral: Pods are created, executed, and terminated without restarting. Pods are deleted upon completion or failure and are not intended to last forever.</li> <li>Immutable: Once deployed, Pods cannot be modified. To update, a new Pod must be created to replace the old one.</li> </ul> Restart Policies <p>Restart policies apply to individual containers within a Pod:</p> <ul> <li>Always: Always restart containers.</li> <li>Never: Never restart containers.</li> <li>OnFailure: Restart containers only if they fail.</li> </ul> <p>Again, those are policies for containers within the Pod - Pods themselves do not restart.</p>"},{"location":"pods/#multi-container-pods","title":"Multi-Container Pods","text":"<p>Multi-container Pods follow the single responsibility principle, where each container performs a distinct role. Some example use cases for this pattern include:</p> <ul> <li>Init Containers: Prepare the environment before application containers start.</li> <li>Sidecar Containers: Provide auxiliary services alongside the main application container.</li> </ul> <p>One common example is to use a multi-container Pod for service meshes. In these scenarios, a sidecar container acts as an SSL termination point for all traffic coming into the main Pod.</p> <p>main appcontainermain app...sidecarcontainersidecar...</p> <p>As mentioned above, multiple containers within a Pod share the same IP address, network stack, and filesystem. As such, to communicate with specific containers within a multi-container Pod, you have to leverage port addresses. The containers themselves, however, will be able to communicate with each other via localhost.</p> <p>main appcontainermain app...sidecarcontainersidecar...fsfs10.0.0.9:808010.0.0.9:808010.0.0.9:171710.0.0.9:171710.0.0.910.0.0.9localhostlocalhost</p>"},{"location":"pods/#using-kubectl-for-pod-management","title":"Using kubectl for Pod Management","text":"kubectl Basics <p><code>kubectl</code> is the command-line tool for interacting with Kubernetes clusters. Key operations include:</p> <p>Find Running Pods: <pre><code>$ kubectl get pods\nNAME        READY   STATUS    RESTARTS   AGE\nnginx-pod   1/1     Running   0          10s\n</code></pre></p> <p>View more details on a Pod using <code>describe</code>: <pre><code>$ kubectl describe pod nginx-pod\nName:             nginx-pod\nNamespace:        default\nPriority:         0\nService Account:  default\nNode:             kind-worker/172.18.0.3\nStart Time:       Thu, 06 Jun 2024 19:06:38 -0500\nLabels:           run=nginx-pod\nAnnotations:      &lt;none&gt;\nStatus:           Running\nIP:               10.244.2.2\nIPs:\n  IP:  10.244.2.2\nContainers:\n  nginx-pod:\n    Container ID:   containerd://92b7e2ac608fc5ad75c7196f69ae8695c93a9b9d5b9f1039b911e5ad65199b08\n    Image:          nginx\n    Image ID:       docker.io/library/nginx@sha256:0f04e4f646a3f14bf31d8bc8d885b6c951fdcf42589d06845f64d18aec6a3c4d\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Thu, 06 Jun 2024 19:06:46 -0500\n    Ready:          True\n    Restart Count:  0\n&lt;-- Rest of output trimmed --&gt;\n</code></pre></p> <p>View Pod Logs: <pre><code>$ kubectl logs nginx-pod\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh\n10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf\n10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf\n/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh\n/docker-entrypoint.sh: Configuration complete; ready for start up\n2024/06/07 00:06:46 [notice] 1#1: using the \"epoll\" event method\n2024/06/07 00:06:46 [notice] 1#1: nginx/1.27.0\n2024/06/07 00:06:46 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14)\n2024/06/07 00:06:46 [notice] 1#1: OS: Linux 6.6.26-linuxkit\n2024/06/07 00:06:46 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\n2024/06/07 00:06:46 [notice] 1#1: start worker processes\n2024/06/07 00:06:46 [notice] 1#1: start worker process 32\n</code></pre></p> Monitoring and Debugging <p>Use <code>kubectl</code> to monitor and debug Pods effectively:</p> <p>Detailed Pod Info: <pre><code>$ kubectl get pods -o wide\nNAME        READY   STATUS    RESTARTS   AGE    IP           NODE          NOMINATED NODE   READINESS GATES\nnginx-pod   1/1     Running   0          5m7s   10.244.2.2   kind-worker   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>Running a specific command in a running container:   ```</p> <p>sh   $ kubectl exec nginx-pod -- hostname   nginx-pod   ```</p> <p>Interactive Shell Session: <pre><code>$ kubectl exec -it nginx-pod -- sh\n# hostname\nnginx-pod\n# echo \"I am running this from the nginx-pod Pod!\"\nI am running this from the nginx-pod Pod!\n</code></pre></p>"},{"location":"pods/#summary","title":"Summary","text":"<p>Pods are the foundational units in Kubernetes, encapsulating applications and providing a robust execution environment. By leveraging Pods effectively, you can take full advantage of Kubernetes' capabilities for deploying, scaling, and managing applications.</p>"},{"location":"security/","title":"Securing Kubernetes: Authentication, Authorization, and Admission Control","text":"<p>Kubernetes security is a critical aspect of managing clusters, ensuring that only authorized users and processes can access and modify resources. This section covers API security, Role-Based Access Control (RBAC), and admission control.</p>"},{"location":"security/#overview-of-kubernetes-security","title":"Overview of Kubernetes Security","text":"The Big Picture <p>Kubernetes is API-centric, with the API server as its core component. Every interaction with the cluster, whether from users, Pods, or internal services, goes through the API server. This makes securing the API server paramount.</p> Typical API Request Flow <p>A typical API request, such as creating a Deployment, follows these steps:</p> <ol> <li>Authentication: Verifies the identity of the requester.</li> <li>Authorization: Checks if the authenticated user has permission to perform the action.</li> <li>Admission Control: Ensures the request complies with policies.</li> </ol> <p>apiauthNauthNauthZauthZAdmissionControlAdmission...user, group,service acct.user, group,...\"are you who you say you are?\"\"are you who you s...\"are you allowed to do what you're trying to do?\"\"are you allowed t...enforce policies, reject requests that do not adhereenforce policies,...TLSTLS</p>"},{"location":"security/#authentication-authn","title":"Authentication (AuthN)","text":"Understanding Authentication <p>Authentication (authN) is about proving your identity. Kubernetes does not have a built-in identity database; instead, it integrates with external identity management systems. Common methods include:</p> <ul> <li>Client Certificates: Signed by the cluster's Certificate Authority (CA).</li> <li>Webhook Token Authentication: Integrates with external systems.</li> <li>Service Accounts: For intra-cluster communication.</li> </ul> Checking Your Authentication Setup <p>Your cluster's details and user credentials are stored in a <code>kubeconfig</code> file, typically located at: <code>/home/&lt;user&gt;/.kube/config</code></p> <p>Example <code>kubeconfig</code> File: <pre><code>apiVersion: v1\nkind: Config\nclusters:\n- cluster:\n    name: prod-eggs\n    server: https://&lt;api-server-url&gt;:443\n    certificate-authority-data: LS0mRS1F...LS0tRj==\nusers:\n- name: vinny\n  user:\n    token: FfqwFGF1gASDF4...SZY3uUQ\ncontexts:\n- context:\n    name: eggs-admin\n    cluster: prod-eggs\n    user: vinny\ncurrent-context: eggs-admin\n</code></pre></p> Integrating with External IAM Systems <p>Most production clusters integrate with enterprise-grade Identity and Access Management (IAM) systems such as Active Directory or cloud-based IAM solutions, providing robust authentication mechanisms.</p>"},{"location":"security/#authorization-authz","title":"Authorization (AuthZ)","text":"Understanding Authorization <p>Authorization (authZ) determines what actions authenticated users can perform. Kubernetes uses a least-privilege model with deny-by-default, meaning you must explicitly grant permissions.</p> Role-Based Access Control (RBAC) <p>RBAC is the most common authorization module, using Roles and RoleBindings to define and assign permissions.</p> <p>Key Components:</p> <ul> <li>Roles: Define a set of permissions.</li> <li>RoleBindings: Assign roles to users or groups.</li> </ul> <p>Example Role: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: eggs\n  name: read-deployments\nrules:\n- verbs: [\"get\", \"watch\", \"list\"]\n  apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n</code></pre></p> <p></p> <p>Example RoleBinding: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-deployments\n  namespace: eggs\nsubjects:\n- kind: User\n  name: jambo\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: read-deployments\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p> ClusterRoles and ClusterRoleBindings <p>ClusterRoles apply to all Namespaces, allowing for broader permissions management.</p> <p>Example ClusterRole: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: read-deployments\nrules:\n- verbs: [\"get\", \"watch\", \"list\"]\n  apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n</code></pre></p> <p></p> <p>Example ClusterRoleBinding: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: read-deployments\nsubjects:\n- kind: User\n  name: jambo\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: read-deployments\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p>"},{"location":"security/#admission-control","title":"Admission Control","text":"Overview of Admission Controllers <p>Admission controllers enforce policies on requests after authentication and authorization but before they are persisted. They come in two types:</p> <ul> <li>Mutating Controllers: Modify requests to ensure compliance.</li> <li>Validating Controllers: Reject non-compliant requests.</li> </ul> Common Admission Controllers <ul> <li>NodeRestriction: Limits nodes to modifying their own objects.</li> <li>AlwaysPullImages: Ensures images are always pulled from the registry, preventing the use of cached images.</li> </ul> Example: NodeRestriction <p>To check admission controllers in your cluster: <pre><code>$ kubectl describe pod kube-apiserver-docker-desktop -n kube-system | grep admission\n--enable-admission-plugins=NodeRestriction\n</code></pre></p>"},{"location":"security/#certificates-and-service-accounts","title":"Certificates and Service Accounts","text":"Using Client Certificates <p>Client certificates authenticate users and services within the cluster. They are stored in the kubeconfig file and verified by the API server.</p> <p>Example of creating a client certificate: <pre><code>openssl genrsa -out client.key 2048\nopenssl req -new -key client.key -out client.csr -subj \"/CN=my-user\"\nopenssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365\n</code></pre></p> Service Accounts <p>Service Accounts provide identities for Pods and controllers, enabling secure intra-cluster communication. Unlike user accounts, which are meant for human users, service accounts are intended for processes that run in Pods.</p> <p>Example ServiceAccount: <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n  namespace: default\n</code></pre></p> Using a ServiceAccount in a Pod <p>To use a service account in a pod, specify the <code>serviceAccountName</code> field in the pod's spec. This binds the pod to the specified service account, allowing the pod to use the account's credentials to authenticate to the API server and other services.</p> <p>Example of a Pod using a ServiceAccount: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  serviceAccountName: my-service-account\n  containers:\n  - name: my-container\n    image: myimage\n</code></pre></p>"},{"location":"security/#practical-example","title":"Practical Example","text":"Deploying a \"Secure\" Application <p>1. Create a Namespace: <pre><code>$ kubectl create namespace secure-app\n</code></pre></p> <p>2. Create a ServiceAccount:</p> <pre><code># serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: secure-app-sa\n  namespace: secure-app\n</code></pre> <pre><code>$ kubectl apply -f serviceaccount.yaml\n</code></pre> <p>3. Deploy a Pod using the ServiceAccount:</p> <p><pre><code># pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-pod\n  namespace: secure-app\nspec:\n  serviceAccountName: secure-app-sa\n  containers:\n  - name: secure-container\n    image: nginx\n</code></pre> <pre><code>$ kubectl apply -f pod.yaml\n</code></pre></p> <p>4. Create a Role and RoleBinding:</p> <pre><code># role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: secure-app\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <pre><code># rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: secure-app\nsubjects:\n- kind: ServiceAccount\n  name: secure-app-sa\n  namespace: secure-app\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <pre><code>$ kubectl apply -f role.yaml\n$ kubectl apply -f rolebinding.yaml\n</code></pre>"},{"location":"security/#summary","title":"Summary","text":"<p>Securing a Kubernetes cluster involves multiple layers of authentication, authorization, and admission control. By understanding and implementing these mechanisms, you can ensure that your cluster is protected from unauthorized access and that all actions comply with defined policies.</p>"},{"location":"services/","title":"Services","text":""},{"location":"services/#understanding-and-utilizing-kubernetes-services","title":"Understanding and Utilizing Kubernetes Services","text":"<p>Kubernetes Services are essential for ensuring reliable communication between Pods. They abstract the complexities of networking and provide stable endpoints for applications.</p>"},{"location":"services/#introduction-to-kubernetes-services","title":"Introduction to Kubernetes Services","text":"Why Use Services? <p>Pods in Kubernetes are ephemeral; they can be created, destroyed, and rescheduled at any time due to various events such as scaling operations, rolling updates, rollbacks, and failures. This makes direct communication with Pods unreliable. Kubernetes Services address this issue by providing a stable endpoint for communication.</p> How Services Work <p>Services in Kubernetes provide a front end (DNS name, IP address, and port) that remains constant regardless of the state of the Pods behind it. They use label selectors to dynamically route traffic to healthy Pods that match the specified criteria.</p>"},{"location":"services/#types-of-kubernetes-services","title":"Types of Kubernetes Services","text":"ClusterIP <p>The default Service type, ClusterIP, exposes the Service on an internal IP within the cluster. This makes it accessible only within the cluster.</p> <p>Below is a high-level representation of how this looks:</p> <p>Selector:project=abzone=prodSelector:...svcpodpodpodpodproject=abzone=prodproject=ab...project=abzone=prodproject=ab...project=abproject=abproject=abzone=prodrelease=24project=ab...podto talk to one of the Pods tothe right, hit this Serviceto talk to one of the Pods to...project=hrzone=prodproject=hr...</p> <p>Key Points:</p> <ul> <li>Internal IP and DNS name are automatically created.</li> <li>Accessible only from within the cluster.</li> </ul> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-clusterip-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre></p> NodePort <p>NodePort Services extend ClusterIP Services by making them accessible from outside the cluster through a port on each node.</p> <p>Below is a high-level example of how the flow works:</p> <p>podnode:30050:30050node:30050:30050podnode:30050:30050podnode:30050:30050IP: 10.0.0.50DNS: turtlePort: 8080IP: 10.0.0.50...svcPod1 IP: xxxxPod 2 IP: xxxxPod 3 IP: xxxx...Pod1 IP: xxxx...11223344epepsliceep...</p> <ol> <li>External client hits node on NodePort.</li> <li>Node forwards request to the ClusterIP of the Service.</li> <li>The Service picks a Pod from the list of healthy Pods in the EndpointSlice.</li> <li>Forward request to the selected Pod.</li> </ol> <p>Key Points:</p> <ul> <li>Exposes the Service on a specific port on each node.</li> <li>External traffic can access the Service using <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>.</li> </ul> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n      nodePort: 30007\n</code></pre></p> LoadBalancer <p>LoadBalancer Services are built on top of NodePort and ClusterIP Services. They integrate with cloud provider load balancers to expose Services to the internet.</p> <p>Below is a high-level example of how the flow works:</p> <p>podnode:30050:30050node:30050:30050podnode:30050:30050podnode:30050:30050IP: 10.0.0.50DNS: turtlePort: 8080IP: 10.0.0.50...svcPod1 IP: xxxxPod 2 IP: xxxxPod 3 IP: xxxxPod1 IP: xxxx...4455epepsliceep...Cloud LB: Public IP &amp; DNS, low portCloud LB: Public IP &amp; DNS, low port112233</p> <ol> <li>External client hits LoadBalancer Service on friendly DNS name.</li> <li>LoadBalancer forwards request to a NodePort.</li> <li>Node forwards request to the ClusterIP of the Service.</li> <li>The Service picks a Pod from the EndpointSlice.</li> <li>Forwards request to the selected Pod.</li> </ol> <p>Key Points:</p> <ul> <li>Provides an external IP managed by the cloud provider.</li> <li>Simplifies access from outside the cluster.</li> </ul> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-loadbalancer-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre></p>"},{"location":"services/#detailed-service-theory","title":"Detailed Service Theory","text":"Labels and Selectors <p>Services use labels and selectors to determine which Pods receive traffic. This loose coupling allows Services to dynamically update the list of Pods they route to, maintaining high availability and load balancing.</p> <p>It should also be noted that Pods can still belong to a Service if they have extra labels, so long as they also contain all the labels that the Service is selecting on.</p> <p>Selector:project=abzone=prodSelector:...svcpodpodpodpodproject=abzone=prodproject=ab...project=abzone=prodproject=ab...project=abproject=abproject=abzone=prodrelease=24project=ab...</p> <p>Example: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-app-container\n        image: my-app-image\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre></p> EndpointSlices <p>As mentioned above, as Pods are spinning up and down, the Service will keep an updated list of Pods with the given labels and selectors. How it does this is through the use of EndpointSlices, which are effectively just dynamic lists of healthy Pods that match a given label selector.</p> <p>Any new Pods that are created on the cluster that match a Service's label selector will automatically be added to the given Service's EndpointSlice object. When a Pod disappears (fails, Node goes down, etc.) it will be removed from the EndpointSlice. The net result is that the Service's EndpointSlice should always be up to date with a list of healthy Pods that the Service can route to.</p>"},{"location":"services/#hands-on-with-services","title":"Hands-On with Services","text":"Creating and Managing Services Imperative Creation <p>Create a Service for an existing Deployment using <code>kubectl expose</code>: <pre><code>kubectl expose deployment my-app --type=LoadBalancer --name=my-service\n</code></pre></p> Declarative Creation <p>Define a Service in a YAML file and apply it: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre> <pre><code>kubectl apply -f my-service.yaml\n</code></pre></p> Inspecting Services <p>Check the status and details of Services: <pre><code>kubectl get svc\nkubectl describe svc my-service\n</code></pre></p>"},{"location":"services/#service-discovery","title":"Service Discovery","text":"How Service Discovery Works <p>Kubernetes uses an internal DNS to resolve Service names to IP addresses. Each Pod's <code>resolv.conf</code> is configured to use the cluster DNS, enabling seamless service discovery.</p> Service Registration <p>Service registration is the process of an app on Kubernetes providing its connection details to a registry in order for other apps on the cluster to be able to find it. This happens automatically when Services are created.</p> <p>High-level flow of Service registration:</p> <ol> <li>Post a Service manifest to the API server (via <code>kubectl</code>).</li> <li>The Service is given a stable IP address called a ClusterIP.</li> <li>EndpointSlices are created to maintain the list of healthy Pods which match the Service's label selector.</li> <li>The Service's name and IP are registered with the cluster DNS.</li> </ol> <p></p> <p>High-level flow of Service discovery:</p> <p>In terms of how an application then discovers other applications behind a Service, the flow looks like this:</p> <ol> <li>The new Service is registered with the cluster DNS (Service Registry).</li> <li>Your application wants to know the IP address of the Service so it provides the name to the cluster DNS for lookup.</li> <li>The cluster DNS returns the IP address of the Service.</li> <li>Your application now knows where to direct its request.</li> </ol> <p>foo-svc10.0.08foo-svc...svc11Service Registryfoo-svc = 10.0.0.8bar-svc = 10.0.0.9ham-svc = 10.0.0.10Service Registry...epappapp223344</p> Practical Example of Service Discovery <p>Assume we have two applications on the same cluster - <code>ham</code> and <code>eggs</code>. Each application has their Pods fronted by a Service, which in turn each have their own ClusterIP.</p> <p><pre><code>kubectl get svc\n</code></pre> Example output: <pre><code>NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nham-svc      ClusterIP   192.168.1.200               443/TCP   5d19h\neggs-svc     ClusterIP   192.168.1.208               443/TCP   5d19h\n</code></pre></p> <p>For <code>ham</code> to communicate with <code>eggs</code>, it needs to know two things: 1. The name of the <code>eggs</code> application's Service (<code>eggs-svc</code>). 2. How to convert that name to an IP address.</p> <p>Steps for Service Discovery:</p> <ol> <li>The application container's default gateway routes the traffic to the Node it is running on.</li> <li>The Node itself does not have a route to the Service network so it routes the traffic to the node kernel.</li> <li>The Node kernel recognizes traffic intended for the service network and routes the traffic to a healthy Pod that matches the label selector of the Service.</li> </ol>"},{"location":"services/#using-namespaces-with-services","title":"Using Namespaces with Services","text":"Role of Namespaces <p>Namespaces partition a cluster's address space, allowing you to create isolated environments within a single cluster.</p> <p>Example:</p> <ul> <li>Perf: <code>ham-svc.perf.svc.cluster.local</code></li> <li>QA: <code>ham-svc.qa.svc.cluster.local</code></li> </ul> <p>Objects within the same Namespace can connect to each other using short names. However, cross-Namespace communication must use the FQDN.</p> <p>ham-svcham-svcnssvcbaconbaconsaltsaltpodpodperf Namespaceperf Namespaceeggs-svceggs-svcnssvcfriedfriedpoachedpoachedpodpodqa Namespaceqa Namespaceham-svcham-svceggs-svc.qa.svc.cluster.localeggs-svc.qa.svc.cluster.local</p>"},{"location":"services/#advanced-concepts","title":"Advanced Concepts","text":"Session Affinity <p>Session Affinity allows you to configure Services to direct requests from the same client to the same Pod, useful for stateful applications.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n\n\n  name: my-affinity-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  sessionAffinity: ClientIP\n</code></pre></p> External Traffic Policy <p>External Traffic Policy controls whether traffic from outside the cluster is routed only to Pods on the same node (preserving client IP) or across all nodes.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  externalTrafficPolicy: Local\n</code></pre></p>"},{"location":"services/#troubleshooting-services","title":"Troubleshooting Services","text":"Common Issues and Solutions <p>Service Not Accessible:</p> <ul> <li>Check the status of the Service and Pods:   <pre><code>kubectl get svc\nkubectl get pods\n</code></pre></li> <li>Ensure the selectors match the Pod labels.</li> </ul> <p>DNS Resolution Fails:</p> <ul> <li>Verify the cluster DNS is running:   <pre><code>kubectl get pods -n kube-system -l k8s-app=kube-dns\n</code></pre></li> <li>Check the contents of <code>/etc/resolv.conf</code> in the Pods.</li> </ul> Practical Tips <ul> <li>Use <code>kubectl logs</code> to inspect logs of coreDNS Pods for DNS-related issues.</li> <li>Restart coreDNS Pods if necessary:   <pre><code>kubectl delete pod -n kube-system -l k8s-app=kube-dns\n</code></pre></li> </ul>"},{"location":"services/#summary","title":"Summary","text":"<p>Kubernetes Services are critical for managing and accessing applications within a cluster. They provide stable endpoints and load balancing, abstracting the dynamic nature of Pods. By understanding and utilizing different Service types and configurations, you can ensure robust and scalable application deployment.</p>"},{"location":"statefulsets/","title":"StatefulSets","text":""},{"location":"statefulsets/#managing-stateful-applications-with-kubernetes-statefulsets","title":"Managing Stateful Applications with Kubernetes StatefulSets","text":"<p>StatefulSets are essential for deploying and managing stateful applications on Kubernetes, which require persistent storage and stable network identities. This includes databases, key-value stores, and applications that maintain client session data.</p>"},{"location":"statefulsets/#introduction-to-statefulsets","title":"Introduction to StatefulSets","text":"What are StatefulSets? <p>StatefulSets are a Kubernetes resource designed to manage stateful applications. Unlike Deployments, StatefulSets provide:</p> <ul> <li>Predictable and persistent Pod names</li> <li>Persistent DNS hostnames</li> <li>Persistent volume bindings</li> </ul> <p>These features ensure that each Pod maintains a consistent identity, even across restarts, failures, and rescheduling.</p> Key Differences from Deployments <p>While both StatefulSets and Deployments are used to manage Pods, StatefulSets offer additional guarantees: - Ordered Creation and Deletion: Pods are created and deleted in a specific order. - Unique Network Identities: Each Pod gets a unique, stable network identity. - Stable Storage: Each Pod is associated with persistent storage that remains consistent across restarts.</p> <p>StatefulSets are Kubernetes constructs designed to manage stateful applications that require persistent data and identity across Pod restarts and deployments. Each Pod in a StatefulSet is given a stable and unique network identifier and persistent storage, which remains associated with the Pod, even when it is rescheduled to a different node within the cluster.</p> <p>StatefulSets are Kubernetes tools for running and managing applications that need to remember who they are and what they know\u2014think of them like memory keepers for your apps, such as databases that need to recall data after a reboot. Unlike Deployments that are more about stateless apps (think of them as forgetful but easily replaceable), StatefulSets make sure each of their Pods has a consistent name, network identity, and storage, even if they move around in the cluster. This makes StatefulSets perfect for when your app's individual identity and history are crucial for running smoothly.</p> <p>StatefulSets can guarantee Pod names, volume bindings, and DNS hostnames across reboots - whereas Deployments cannot. Below are two diagrams that illustrate this point:</p> <p>deploypod10.0.0.510.0.0.5 app dataapp data!!deploypod10.0.0.910.0.0.9 Pod/NodefailurePod/Node...pod10.0.0.510.0.0.5 app dataapp data !!Pod/NodefailurePod/Node...pod10.0.0.510.0.0.5 app dataapp data </p> <p>Notice how with a Deployment, when a Pod is replaced it comes up with a new name, IP address, and its volume is no longer bound to it. With StatefulSets, the new Pod comes up looking exactly the same as the previous failed one.</p>"},{"location":"statefulsets/#statefulset-theory","title":"StatefulSet Theory","text":"Pod Naming <p>Each Pod in a StatefulSet gets a predictable name, following the format <code>&lt;StatefulSetName&gt;-&lt;integer&gt;</code>. For example, a StatefulSet named <code>my-sts</code> with three replicas will have Pods named <code>my-sts-0</code>, <code>my-sts-1</code>, and <code>my-sts-2</code>.</p> Ordered Creation and Deletion <p>StatefulSets create and delete Pods in a specific order:</p> <ul> <li>Creation: Pods are created one at a time, waiting for each to be running and ready before creating the next.</li> <li>Deletion: Pods are deleted in reverse order, ensuring that the highest ordinal Pod is terminated first.</li> </ul> Volume Management <p>StatefulSets manage volumes through PersistentVolumeClaims (PVCs). Each Pod gets its own unique volume, which is preserved across restarts and reattachments:</p> <ul> <li>Volume Naming: Volumes are named based on the StatefulSet and Pod names, e.g., <code>vol-my-sts-0</code>, <code>vol-my-sts-1</code>.</li> <li>Persistence: Volumes remain attached to the same Pod, even if the Pod is rescheduled to a different node.</li> </ul>"},{"location":"statefulsets/#hands-on-with-statefulsets","title":"Hands-On with StatefulSets","text":"Deploying StatefulSets Example StatefulSet Configuration <p>Here\u2019s an example of a simple StatefulSet for a mysqlDB deployment:</p> <p>StatefulSet YAML: <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: my-sts\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  serviceName: \"my-sts\"\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: ctr-mysql\n        image: mysql:latest\n        ports:\n        - containerPort: 3306\n  volumeClaimTemplates:\n  - metadata:\n      name: mysql-data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: \"fast\"\n      resources:\n        requests:\n          storage: 15Gi\n</code></pre></p> Creating a StatefulSet <p>Deploy the StatefulSet using the following command: <pre><code>kubectl apply -f statefulset.yaml\n</code></pre> This command posts the StatefulSet configuration to the Kubernetes API server, which will create and manage the specified Pods and their associated storage.</p> <p>Example output: <pre><code>statefulset.apps/my-sts created\n</code></pre></p> Inspecting StatefulSet and Pods <p>Check the status of the StatefulSet and its Pods: <pre><code>kubectl get sts\n</code></pre> This command displays the status of the StatefulSets in your cluster.</p> <p>Example output: <pre><code>NAME     READY   AGE\nmy-sts   3/3     2m\n</code></pre></p> <p>To get detailed information about the Pods managed by the StatefulSet: <pre><code>kubectl get pods\n</code></pre> This command lists all Pods in your cluster, including those managed by the StatefulSet.</p> <p>Example output: <pre><code>NAME      READY   STATUS    RESTARTS   AGE\nmy-sts-0  1/1     Running   0          2m\nmy-sts-1  1/1     Running   0          1m\nmy-sts-2  1/1     Running   0          30s\n</code></pre></p> Scaling StatefulSets <p>StatefulSets can be scaled up or down, ensuring order and data integrity:</p> <ul> <li>Scaling Up: New Pods are created sequentially.</li> <li>Scaling Down: Pods are deleted in reverse order.</li> </ul> <p>To scale the StatefulSet: <pre><code>kubectl scale sts my-sts --replicas=4\n</code></pre> This command scales the StatefulSet to 4 replicas. Kubernetes will create the new Pod in order, ensuring consistency and stability.</p> <p>Example output: <pre><code>statefulset.apps/my-sts scaled\n</code></pre></p> Handling Failures <p>StatefulSets handle failures by automatically recreating Pods with the same identity and volume bindings:</p> <ul> <li>Pod Failure: A failed Pod is replaced with a new Pod with the same name and volume.</li> <li>Node Failure: Modern Kubernetes versions handle node failures more effectively, replacing Pods on failed nodes automatically.</li> </ul> Using Headless Services <p>StatefulSets often use headless Services to manage network identities:</p> <p>Headless Service YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-prod\nspec:\n  clusterIP: None\n  selector:\n    app: mysql\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sts-mysql\nspec:\n  serviceName: mysql-prod\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:latest\n        ports:\n        - containerPort: 3306\n</code></pre> The <code>clusterIP: None</code> in the Service configuration creates a headless Service, which means it does not get a ClusterIP address. Instead, it allows Pods to be addressed directly via their DNS names.</p>"},{"location":"statefulsets/#scaling-and-updating-statefulsets","title":"Scaling and Updating StatefulSets","text":"Scaling StatefulSets <p>Edit the StatefulSet YAML to change the replica count and apply the changes: <pre><code>kubectl apply -f statefulset.yaml\n</code></pre> This command updates the StatefulSet configuration, adjusting the number of replicas as specified.</p> <p>Example output: <pre><code>statefulset.apps/my-sts scaled\n</code></pre></p> Rolling Updates <p>Update the image version in the StatefulSet YAML and apply the changes to perform a rolling update: <pre><code>kubectl apply -f statefulset.yaml\n</code></pre> This command triggers a rolling update, where each Pod is updated one by one, ensuring that the application remains available during the update process.</p> <p>Example output: <pre><code>statefulset.apps/my-sts updated\n</code></pre></p>"},{"location":"statefulsets/#summary","title":"Summary","text":"<p>StatefulSets are crucial for managing stateful applications in Kubernetes. They provide stable network identities, persistent storage, and ordered Pod creation and deletion. By leveraging StatefulSets, you can ensure your stateful applications are robust, scalable, and resilient.</p>"},{"location":"storage/","title":"Mastering Kubernetes Storage","text":"<p>Storing and retrieving data is crucial for most real-world applications. Kubernetes' persistent volume subsystem allows you to connect to enterprise-grade storage systems that provide advanced data management services such as backup and recovery, replication, and snapshots.</p>"},{"location":"storage/#overview","title":"Overview","text":"<p>Kubernetes supports a variety of storage systems, including those from major cloud providers and enterprise-class solutions like EMC and NetApp. This section will cover:</p> <ul> <li>The big picture of Kubernetes storage</li> <li>Various storage providers</li> <li>The Container Storage Interface (CSI)</li> <li>Kubernetes persistent volume subsystem</li> <li>Dynamic provisioning with Storage Classes</li> <li>Hands-on examples</li> </ul>"},{"location":"storage/#the-big-picture","title":"The Big Picture","text":"<p>Kubernetes supports different types of storage, such as block, file, and object storage, from various external systems, either in the cloud or on-premises.</p> High-Level Architecture <p>Storage providers connect to Kubernetes through a plugin layer, often using the Container Storage Interface (CSI). This standardized interface simplifies integrating external storage resources with Kubernetes. </p> <p>pvpvccsicsiPV subsystemPV subsystemGCPGCPAzureAzureNetAppNetApp</p> Key Components <ul> <li>Storage Providers: External systems providing storage services, like EMC, NetApp, or cloud providers.</li> <li>Plugin Layer: Connects external storage systems with Kubernetes, typically using CSI plugins.</li> <li>Kubernetes Persistent Volume Subsystem: Standardized API objects that allow applications to consume storage easily.</li> </ul>"},{"location":"storage/#storage-providers","title":"Storage Providers","text":"<p>Kubernetes supports a wide range of external storage systems, each typically providing its own CSI plugin. These plugins are usually installed via Helm charts or YAML installers and run as Pods in the <code>kube-system</code> Namespace.</p> Restrictions <ul> <li>Cloud-Specific: You can't provision and mount GCP volumes if your cluster is on Microsoft Azure.</li> <li>Locality: Pods often need to be in the same region or zone as the storage backend.</li> </ul>"},{"location":"storage/#container-storage-interface-csi","title":"Container Storage Interface (CSI)","text":"<p>The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage systems to containerized workloads on Container Orchestration Systems (COS) like Kubernetes. CSI allows for the consistent configuration and management of storage solutions across various container orchestration systems.</p> <p>CSI enables storage providers to develop a standardized plugin once and have it work across a multitude of container orchestration systems without requiring changes. This simplifies the process of adding new storage capabilities to Kubernetes clusters and ensures compatibility and extendibility.</p> <p>While CSI is a critical piece of getting storage working in Kubernetes, unless you explicitly work on writing storage plugins you'll likely never interact with it directly. Most of your interaction with CSI will simply be referencing your relevant CSI plugin in YAML files.</p> Benefits of CSI <ul> <li>Decoupled Updates: CSI plugins can be updated independently of Kubernetes releases.</li> <li>Broad Compatibility: CSI plugins work across different orchestration platforms.</li> </ul> Installing CSI Plugins <p>Most cloud platforms pre-install CSI plugins for native storage services. Third-party storage systems require manual installation, often available as Helm charts or YAML files.</p>"},{"location":"storage/#kubernetes-persistent-volume-subsystem","title":"Kubernetes Persistent Volume Subsystem","text":"<p>The Persistent Volume Subsystem uses several key resources to manage storage:</p> <ul> <li>PersistentVolumes (PV): Represent external storage volumes.</li> <li>PersistentVolumeClaims (PVC): Requests for storage by applications.</li> <li>StorageClasses (SC): Define different classes of storage for dynamic provisioning.</li> </ul> Workflow Example <ol> <li>Pod Requests Storage: Via a PersistentVolumeClaim (PVC).</li> <li>PVC Requests Creation: PVC asks the StorageClass (SC) to create a new PV on the storage backend.</li> <li>CSI Plugin Interaction: The SC uses the CSI plugin to provision the volume.</li> <li>Volume Creation: The external volume is created and reported back to Kubernetes.</li> <li>PV and PVC Binding: The PV is mapped to the created volume, and the Pod mounts the PV.</li> </ol>"},{"location":"storage/#dynamic-provisioning-with-storage-classes","title":"Dynamic Provisioning with Storage Classes","text":"<p>StorageClasses (SCs) allow you to define different types of storage. How they are defined depends on the type of storage you're using. For example, if you're using Google Cloud Storage you have classes such as Standard, Nearline, Coldline, and Archive. You may also have simpler/more straightforward classes at your disposal such as SSD and HDD. When you create a SC you map both of those definitions so Pods in your cluster can use either or.</p> Example YAML for a StorageClass <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ssd\nprovisioner: pd.csi.storage.gke.io  # Google Cloud CSI plugin\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nparameters:\n  type: pd-ssd  # Google Cloud SSD drives\n  provisioned-iops-on-create: '10000'\n</code></pre> Key Points <ul> <li>Immutability: StorageClass objects cannot be modified once created.</li> <li>Meaningful Names: Use descriptive names for easy reference.</li> <li>Provisioner-Specific Parameters: The parameters block varies between different plugins.</li> </ul>"},{"location":"storage/#example","title":"Example","text":"<p>Example YAML: Below is the high-level flow for creating and using StorageClasses:</p> <ol> <li>Ensure you have a storage back-end (cloud, on-prem, etc.)</li> <li>Have a running Kubernetes cluster</li> <li>Install and setup the CSI storage plugin to connect to Kubernetes</li> <li>Create at least one StorageClass on Kubernetes</li> <li>Deploy Pods with PVCs that reference those Storage classes</li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  volumes:\n    - name: data\n      persistentVolumeClaim:\n        claimName: mypvc\n  containers:\n  - name: my-container\n    image: myimage\n    volumeMounts:\n    - name: data\n      mountPath: /data\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mypvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi\n  storageClassName: fast\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: fast\nprovisioner: pd.csi.storage.gke.io\nparameters:\n  type: pd-ssd\n</code></pre>"},{"location":"storage/#additional-volume-settings","title":"Additional Volume Settings","text":"Access Modes <ul> <li>ReadWriteOnce (RWO): Single PVC can bind to a volume in read-write mode.</li> <li>ReadWriteMany (RWM): Multiple PVCs can bind to a volume in read-write mode.</li> <li>ReadOnlyMany (ROM): Multiple PVCs can bind to a volume in read-only mode.</li> </ul> Reclaim Policy <ul> <li>Delete: Deletes PV and external storage when PVC is released.</li> <li>Retain: Keeps PV and external storage when PVC is deleted, requiring manual cleanup.</li> </ul>"},{"location":"storage/#summary","title":"Summary","text":"<p>Kubernetes provides a robust storage subsystem that allows applications to dynamically provision and manage storage from various external systems. By leveraging CSI plugins and StorageClasses, you can create flexible and scalable storage solutions tailored to your application's needs.</p>"}]}