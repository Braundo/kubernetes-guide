{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to K8s Guide","text":"<p>SITE IS STILL UNDER CONSTRUCTION!</p> <p>Content current as of Kubernetes verison 1.28</p> <p>Welcome to k8s.guide, your concise companion through the world of Kubernetes. My aim was to provide a simplified, digestible, and easy-to-navigate version of the official Kubernetes documentation.</p> <p></p> <p>Legal discalimer:  </p> <ul> <li> <p>\"Kubernetes\", \"K8s\" and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  </p> </li> <li> <p>Neither myself nor this site are officially associated with the Linux Foundation.  </p> </li> </ul>"},{"location":"architecture/cgroupv2/","title":"cgroup v2","text":"<p>On Linux, control groups constrain resources that are allocated to processes. The kubelet and the underlying container runtime need to interface with cgroups to enforce resource management for pods and containers which includes cpu/memory requests and limits for containerized workloads. There are two versions of cgroups in Linux: <code>cgroup v1</code> and <code>cgroup v2</code>. <code>cgroup v2</code> is the new generation of the cgroup API.</p>"},{"location":"architecture/cgroupv2/#what-is-cgroup-v2","title":"What is <code>cgroup v2</code>?","text":"<ul> <li><code>cgroup v2</code> is the next version of the Linux cgroup API.</li> <li>Provides a unified control system with enhanced resource management capabilities.</li> <li>Offers improvements like a single unified hierarchy design in API, safer sub-tree delegation to containers, and enhanced resource allocation management.</li> </ul>"},{"location":"architecture/cgroupv2/#using-cgroup-v2","title":"Using <code>cgroup v2</code>","text":"<ul> <li>Recommended to use a Linux distribution that enables and uses <code>cgroup v2</code> by default.</li> </ul>"},{"location":"architecture/cgroupv2/#requirements","title":"Requirements","text":"<ul> <li>OS distribution should enable <code>cgroup v2</code>.</li> <li>Linux Kernel version should be 5.8 or later.</li> <li>Container runtime should support <code>cgroup v2</code>, e.g., <code>containerd v1.4</code> and later, <code>cri-o v1.20</code> and later.</li> </ul>"},{"location":"architecture/cgroupv2/#linux-distribution-cgroup-v2-support","title":"Linux Distribution <code>cgroup v2</code> support","text":"<ul> <li>Linux distributions that support <code>cgroup v2</code>, such as Container Optimized OS, Ubuntu, Debian GNU/Linux, Fedora, Arch Linux, and RHEL.</li> </ul>"},{"location":"architecture/cgroupv2/#migrating-to-cgroup-v2","title":"Migrating to <code>cgroup v2</code>","text":"<ul> <li>Ensure you meet the requirements and then upgrade to a kernel version that enables <code>cgroup v2</code> by default.</li> <li>The kubelet automatically detects <code>cgroup v2</code> and performs accordingly.</li> </ul>"},{"location":"architecture/cgroupv2/#identify-the-cgroup-version-on-linux-nodes","title":"Identify the cgroup version on Linux Nodes","text":"<ul> <li>To check which cgroup version your distribution uses, you can run specific commands like: <pre><code>stat -fc %T /sys/fs/cgroup/.\n</code></pre></li> </ul>"},{"location":"architecture/cloud-controller-manager/","title":"Cloud Controller Manager","text":"<pre><code>graph TB\n    subgraph Kubernetes Control Plane\n        cm[Cloud Controller Manager]\n        api[API Server]\n        etcd[(ETCD)]\n        cm --&gt;|interacts with| api\n        api --&gt; etcd\n    end\n\n    subgraph Cloud Infrastructure\n        nodeController[Node Controller]\n        routeController[Route Controller]\n        serviceController[Service Controller]\n        cloudResources[Cloud Resources]\n        nodeController --&gt; cloudResources\n        routeController --&gt; cloudResources\n        serviceController --&gt; cloudResources\n    end\n\n    cm --&gt;|manages| nodeController\n    cm --&gt;|manages| routeController\n    cm --&gt;|manages| serviceController\n\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:2px;\n    class cm,nodeController,routeController,serviceController k8s;\n</code></pre>"},{"location":"architecture/cloud-controller-manager/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<ul> <li>A Kubernetes control plane component that embeds cloud-specific control logic.</li> <li>Decouples the interoperability logic between Kubernetes and underlying cloud infrastructure.</li> <li>Allows cloud providers to release features at a different pace compared to the main Kubernetes project.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#design","title":"Design","text":"<ul> <li>Runs in the control plane as a replicated set of processes, usually as containers in Pods.</li> <li>Implements multiple controllers in a single process.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#node-controller","title":"Node Controller","text":"<ul> <li>Updates Node objects when new servers are created in the cloud infrastructure.</li> <li>Annotates and labels the Node object with cloud-specific information.</li> <li>Verifies the node's health and deletes the Node object if the server has been deleted from the cloud.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#route-controller","title":"Route Controller","text":"<ul> <li>Configures routes in the cloud so that containers on different nodes can communicate.</li> <li>May also allocate blocks of IP addresses for the Pod network.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#service-controller","title":"Service Controller","text":"<ul> <li>Integrates with cloud infrastructure components like managed load balancers and IP addresses.</li> <li>Sets up load balancers and other infrastructure when a Service resource requires them.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#api-object-access","title":"API Object Access","text":"<ul> <li>Requires specific access levels to various API objects like Node, Service, and Endpoints.</li> <li>For example, full access to read and modify Node objects, and list and watch access to Service objects.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#rbac-clusterrole","title":"RBAC ClusterRole","text":"<ul> <li>Defines the permissions required for the cloud controller manager, such as creating events and service accounts.</li> </ul>"},{"location":"architecture/controllers/","title":"Controllers","text":""},{"location":"architecture/controllers/#control-loop","title":"Control Loop","text":"<ul> <li>A non-terminating loop that regulates the state of a system.</li> <li>Example: A thermostat in a room that adjusts the temperature to reach the desired state.</li> </ul>"},{"location":"architecture/controllers/#controller-pattern","title":"Controller Pattern","text":"<ul> <li>Tracks at least one Kubernetes resource type.</li> <li>Responsible for making the current state align with the desired state specified in the resource's spec field.</li> </ul>"},{"location":"architecture/controllers/#control-via-api-server","title":"Control via API Server","text":"<ul> <li>Controllers interact with the cluster API server to manage state.</li> <li>Example: The Job controller creates or removes Pods via the API server to complete a task.</li> </ul>"},{"location":"architecture/controllers/#direct-control","title":"Direct Control","text":"<ul> <li>Some controllers interact with external systems to achieve the desired state.</li> <li>Example: A controller that scales the number of nodes in a cluster by interacting with cloud provider APIs.</li> </ul>"},{"location":"architecture/controllers/#desired-vs-current-state","title":"Desired vs. Current State","text":"<ul> <li>Kubernetes aims for a cloud-native approach, handling constant change.</li> <li>Controllers work to bring the current state closer to the desired state, even if the cluster is never in a stable state.</li> </ul>"},{"location":"architecture/controllers/#design-principles","title":"Design Principles","text":"<ul> <li>Kubernetes uses multiple controllers for different aspects of cluster state.</li> <li>Allows for resilience, as one controller can take over if another fails.</li> </ul>"},{"location":"architecture/controllers/#ways-of-running-controllers","title":"Ways of Running Controllers","text":"<ul> <li>Built-in controllers run inside the kube-controller-manager.</li> <li>Custom controllers can run either inside or outside the Kubernetes cluster.</li> </ul> <pre><code>graph LR\n    subgraph Kubernetes Cluster\n        apiServer[API Server]\n        controller[Controller]\n        resource[Resource Spec]\n        actualState[Actual State]\n        desiredState[Desired State]\n\n        resource --&gt;|defines| desiredState\n        apiServer --&gt;|observes| actualState\n        actualState --&gt;|reported via kubelet| apiServer\n        desiredState --&gt; controller\n    end\n\n    apiServer --&gt;|notifies of state changes| controller\n    controller --&gt;|attempts to match| desiredState\n\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:2px;\n    class apiServer,controller k8s;\n</code></pre>"},{"location":"architecture/cri/","title":"Container Runtime Interface","text":""},{"location":"architecture/cri/#what-is-cri","title":"What is CRI?","text":"<ul> <li>CRI is a plugin interface that allows the kubelet to use various container runtimes.</li> <li>It eliminates the need to recompile cluster components for different runtimes.</li> <li>A working container runtime is required on each Node for the kubelet to launch Pods and their containers.</li> </ul>"},{"location":"architecture/cri/#protocol","title":"Protocol","text":"<ul> <li>CRI defines the main gRPC protocol for communication between the kubelet and the container runtime.</li> <li>The kubelet acts as a client when connecting to the container runtime via gRPC.</li> </ul>"},{"location":"architecture/cri/#api-feature-state","title":"API Feature State","text":"<ul> <li>As of Kubernetes v1.23, CRI is considered stable.</li> <li>The kubelet uses command-line flags like <code>--image-service-endpoint</code> to configure runtime and image service endpoints.</li> </ul>"},{"location":"architecture/cri/#cri-version-support","title":"CRI Version Support","text":"<ul> <li>For Kubernetes v1.28, the kubelet prefers to use <code>CRI v1</code>.</li> <li>If a runtime doesn't support v1, the kubelet negotiates an older supported version.</li> <li><code>CRI v1alpha2</code> is considered deprecated.</li> </ul>"},{"location":"architecture/cri/#upgrading","title":"Upgrading","text":"<ul> <li>During a Kubernetes upgrade, the kubelet tries to automatically select the latest CRI version.</li> <li>If that fails, fallback mechanisms are in place.</li> <li>If a gRPC re-dial is required due to a container runtime upgrade, the runtime must support the initially selected version, or the re-dial will fail.</li> </ul>"},{"location":"architecture/garbage-collection/","title":"Garbage Collection","text":""},{"location":"architecture/garbage-collection/#what-is-garbage-collection","title":"What is Garbage Collection?","text":"<ul> <li>Collective term for mechanisms that clean up cluster resources.</li> <li>Targets terminated pods, completed jobs, objects without owner references, unused containers and images, and more.</li> </ul>"},{"location":"architecture/garbage-collection/#owners-and-dependents","title":"Owners and Dependents","text":"<ul> <li>Objects in Kubernetes link to each other through owner references.</li> <li>Owner references help the control plane and other API clients clean up related resources before deleting an object.</li> </ul>"},{"location":"architecture/garbage-collection/#cascading-deletion","title":"Cascading Deletion","text":"<p>Two types: Foreground and Background. - Foreground: Owner object first enters a \"deletion in progress\" state, and dependents are deleted before the owner. - Background: Owner object is deleted immediately, and dependents are cleaned up in the background.</p>"},{"location":"architecture/garbage-collection/#orphaned-dependents","title":"Orphaned Dependents","text":"<ul> <li>Dependents left behind when an owner object is deleted are called orphan objects.</li> <li>By default, Kubernetes deletes dependent objects, but this behavior can be overridden.</li> </ul>"},{"location":"architecture/garbage-collection/#garbage-collection-of-unused-containers-and-images","title":"Garbage Collection of Unused Containers and Images","text":"<ul> <li>Kubelet performs garbage collection on unused images every five minutes and on unused containers every minute.</li> <li>Configurable options include <code>HighThresholdPercent</code> and <code>LowThresholdPercent</code> for disk usage.</li> </ul>"},{"location":"architecture/garbage-collection/#container-garbage-collection","title":"Container Garbage Collection","text":"<ul> <li>Variables like <code>MinAge</code>, <code>MaxPerPodContainer</code>, and <code>MaxContainers</code> can be defined to control container garbage collection.</li> <li>Kubelet adjusts <code>MaxPerPodContainer</code> if it conflicts with <code>MaxContainers</code>.</li> </ul>"},{"location":"architecture/garbage-collection/#configuring-garbage-collection","title":"Configuring Garbage Collection","text":"<ul> <li>Options specific to controllers managing resources can be configured for garbage collection.</li> </ul>"},{"location":"architecture/leases/","title":"Leases","text":""},{"location":"architecture/leases/#leases-in-distributed-systems","title":"Leases in Distributed Systems","text":"<ul> <li>Leases provide a mechanism to lock shared resources and coordinate activities.</li> <li>In Kubernetes, represented by Lease objects in the <code>coordination.k8s.io</code> API Group.</li> </ul>"},{"location":"architecture/leases/#node-heartbeats","title":"Node Heartbeats","text":"<ul> <li>The Lease API is used to communicate kubelet node heartbeats to the Kubernetes API server.</li> <li>Each Node has a corresponding Lease object in the <code>kube-node-lease</code> namespace.</li> <li>The <code>spec.renewTime</code> field is updated with each heartbeat, and the control plane uses this timestamp to determine Node availability.</li> </ul>"},{"location":"architecture/leases/#leader-election","title":"Leader Election","text":"<ul> <li>Leases ensure only one instance of a component runs at a given time.</li> <li>Used by control plane components like <code>kube-controller-manager</code> and <code>kube-scheduler</code> in HA configurations.</li> </ul>"},{"location":"architecture/leases/#api-server-identity","title":"API Server Identity","text":"<ul> <li>Starting in Kubernetes v1.26, each <code>kube-apiserver</code> uses the Lease API to publish its identity.</li> <li>Enables future capabilities that may require coordination between each <code>kube-apiserver</code>.</li> </ul>"},{"location":"architecture/leases/#workloads","title":"Workloads","text":"<ul> <li>Custom workloads can define their own use of Leases for leader election or coordination.</li> <li>Good practice to name the Lease in a way that is linked to the component or product.</li> </ul>"},{"location":"architecture/leases/#garbage-collection","title":"Garbage Collection","text":"<ul> <li>Expired leases from <code>kube-apiservers</code> that no longer exist are garbage-collected by new <code>kube-apiservers</code> after 1 hour.</li> </ul>"},{"location":"architecture/leases/#feature-gate","title":"Feature Gate","text":"<ul> <li>API server identity leases can be disabled by disabling the <code>APIServerIdentity</code> feature gate.</li> </ul>"},{"location":"architecture/mixed-version-proxy/","title":"Mixed Version Proxy","text":""},{"location":"architecture/mixed-version-proxy/#feature-state","title":"Feature State","text":"<ul> <li>As of Kubernetes v1.28, the Mixed Version Proxy is an alpha feature.</li> <li>Allows an API Server to proxy resource requests to other peer API servers running different Kubernetes versions.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#use-case","title":"Use Case","text":"<ul> <li>Useful for clusters with multiple API servers running different versions, especially during long-lived rollouts to new Kubernetes releases.</li> <li>Helps in directing resource requests to the correct <code>kube-apiserver</code>, preventing unexpected 404 errors during upgrades.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#enabling-mixed-version-proxy","title":"Enabling Mixed Version Proxy","text":"<ul> <li>Enable the UnknownVersionInteroperabilityProxy feature gate when starting the API Server.</li> <li>Requires specific command-line arguments like <code>--peer-ca-file</code>, <code>--proxy-client-cert-file</code>, <code>--proxy-client-key-file</code>, and <code>--requestheader-client-ca-file</code>.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#proxy-transport-and-authentication","title":"Proxy Transport and Authentication","text":"<ul> <li>Source <code>kube-apiserver</code> uses existing flags -proxy-client-cert-file and -proxy-client-key-file to present its identity.</li> <li>Destination <code>kube-apiserver</code> verifies the peer connection based on the -requestheader-client-ca-file argument.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#configuration-for-peer-api-server-connectivity","title":"Configuration for Peer API Server Connectivity","text":"<ul> <li>Use <code>--peer-advertise-ip</code> and <code>--peer-advertise-port</code> to set the network location for proxying requests.</li> <li>If unspecified, it defaults to the values from <code>--advertise-address</code> or <code>--bind-address</code>.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#mixed-version-proxying-mechanism","title":"Mixed Version Proxying Mechanism","text":"<ul> <li>Special filter in the aggregation layer identifies API groups/versions/resources that the local server doesn't recognize.</li> <li>Attempts to proxy those requests to a peer API server capable of handling them.</li> <li>If the peer API server fails to respond, a <code>503 (\"Service Unavailable\")</code> error is returned.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#how-it-works-under-the-hood","title":"How it Works Under the Hood","text":"<ul> <li>Uses the internal <code>StorageVersion</code> API to check which API servers can serve the requested resource.</li> <li>If no peer is known for that API group/version/resource, a <code>404 (\"Not Found\")</code> response is returned.</li> <li>If the selected peer fails to respond, a <code>503 (\"Service Unavailable\")</code> error is returned.</li> </ul>"},{"location":"architecture/node-communication/","title":"Node Communication","text":""},{"location":"architecture/node-communication/#node-to-control-plane","title":"Node to Control Plane","text":"<ul> <li>Follows a \"hub-and-spoke\" API pattern.</li> <li>All API usage from nodes or their pods terminates at the API server.</li> <li>API server listens on a secure HTTPS port, typically 443.</li> <li>Nodes should have the public root certificate and valid client credentials for secure connection.</li> </ul>"},{"location":"architecture/node-communication/#api-server-to-kubelet","title":"API Server to Kubelet","text":"<ul> <li>Used for fetching logs, attaching to running pods, and port-forwarding.</li> <li>Connections terminate at the kubelet's HTTPS endpoint.</li> <li>To secure the connection, use the <code>--kubelet-certificate-authority</code> flag for the API server.</li> <li>Kubelet authentication and/or authorization should be enabled.</li> </ul>"},{"location":"architecture/node-communication/#api-server-to-nodes-pods-and-services","title":"API Server to Nodes, Pods, and Services","text":"<ul> <li>Connections default to plain HTTP and are neither authenticated nor encrypted.</li> <li>Can be run over HTTPS but will not validate the certificate or provide client credentials.</li> <li>Not safe to run over untrusted or public networks.</li> </ul>"},{"location":"architecture/node-communication/#ssh-tunnels","title":"SSH Tunnels","text":"<ul> <li>Supports SSH tunnels to protect control plane to nodes communication.</li> <li>API server initiates an SSH tunnel to each node and passes all traffic through the tunnel.</li> <li>Ensures traffic is not exposed outside the nodes' network.</li> </ul>"},{"location":"architecture/node-communication/#konnectivity-service","title":"Konnectivity Service","text":"<ul> <li>Provides TCP level proxy for control plane to cluster communication.</li> <li>Consists of the Konnectivity server in the control plane network and agents in the nodes network.</li> <li>After enabling, all control plane to nodes traffic goes through these connections.</li> </ul>"},{"location":"architecture/nodes/","title":"Nodes","text":""},{"location":"architecture/nodes/#what-are-nodes","title":"What are nodes?","text":"<ul> <li>Worker machines in a Kubernetes cluster.</li> <li>Run containerized applications managed by the Control Plane.</li> <li>Equipped with a Kubelet agent that communicates with the master components.</li> </ul>"},{"location":"architecture/nodes/#control-plane-vs-worker-nodes","title":"Control Plane vs Worker Nodes","text":"<ul> <li>Nodes are managed by master components, primarily the Control Plane.</li> <li>Operations include adding nodes, updating node software, and decommissioning nodes. <pre><code>stateDiagram-v2\n    state control_plane {\n        kube_apiserver\n        kube_apiserver --&gt; etcd: stores data\n        kube_apiserver --&gt; kube_controller_manager: watches changes\n        kube_apiserver --&gt; scheduler: finds placement\n        scheduler --&gt; kube_apiserver: watches for pods needing scheduled\n        kube_controller_manager\n    }\n\n    state worker_nodes {\n        kubelet\n        kubelet --&gt; kube_proxy: configures networking\n        kubelet --&gt; container_runtime: runs containers\n        kube_proxy --&gt; iptables_BPF: manages networking rules\n        container_runtime\n    }\n\n    control_plane --&gt; worker_nodes: manages\n    worker_nodes --&gt; control_plane: reports\n</code></pre> <p>Don't worry if some of these components don't make sense - we'll get to them in later sections.</p> </li> </ul>"},{"location":"architecture/nodes/#node-name-uniqueness","title":"Node Name Uniqueness","text":"<ul> <li>Each node must have a unique identifier within the cluster.</li> <li>Ensures accurate scheduling and task allocation.</li> </ul>"},{"location":"architecture/nodes/#self-registration-of-nodes","title":"Self-registration of Nodes","text":"<ul> <li>Nodes can automatically register themselves upon joining the cluster.</li> <li>Facilitates dynamic scaling and resource allocation.</li> </ul>"},{"location":"architecture/nodes/#manual-node-administration","title":"Manual Node Administration","text":"<ul> <li>Admins can manually add or remove nodes using the Kubernetes API or CLI tools.</li> <li>Useful for fine-grained control over the cluster.</li> </ul>"},{"location":"architecture/nodes/#node-status","title":"Node Status","text":"<ul> <li>Provides detailed information about the node, including IP addresses, conditions (<code>Ready</code>, <code>OutOfDisk</code>, etc.), and resource capacity.</li> <li>Updated periodically by the node's Kubelet.</li> </ul>"},{"location":"architecture/nodes/#node-heartbeats","title":"Node Heartbeats","text":"<ul> <li>Regular signals sent from the Kubelet to the master to indicate the node's health.</li> <li>Failure to send a heartbeat within a certain time leads to node eviction.</li> </ul>"},{"location":"architecture/nodes/#node-controller","title":"Node Controller","text":"<ul> <li>A Control Plane component responsible for monitoring nodes.</li> <li>Handles node failures and triggers pod evictions if necessary.</li> </ul>"},{"location":"architecture/nodes/#rate-limits-on-eviction","title":"Rate Limits on Eviction","text":"<ul> <li>Configurable settings that control the speed at which pods are evicted from unhealthy nodes.</li> <li>Helps to avoid overwhelming the remaining healthy nodes.</li> </ul>"},{"location":"architecture/nodes/#resource-capacity-tracking","title":"Resource Capacity Tracking","text":"<ul> <li>Nodes report available resources like CPU, memory, and storage for better scheduling.</li> <li>Helps the scheduler in placing pods where resources are available.</li> </ul>"},{"location":"architecture/nodes/#node-topology","title":"Node Topology","text":"<ul> <li>Information about the physical or virtual layout of nodes in terms of regions, zones, and other cloud-provider specific metadata.</li> <li>Used for optimizing workload distribution and high availability.</li> </ul>"},{"location":"architecture/nodes/#graceful-node-shutdown","title":"Graceful Node Shutdown","text":"<ul> <li>A process that safely evicts pods before shutting down or rebooting a node.</li> <li>Ensures minimal impact on running applications and services.</li> </ul>"},{"location":"architecture/nodes/#pod-priority-based-graceful-node-shutdown","title":"Pod Priority based Graceful Node Shutdown","text":"<ul> <li>During a graceful shutdown, pods with higher priority are evicted last.</li> <li>Ensures that critical applications continue to run for as long as possible.</li> </ul>"},{"location":"architecture/nodes/#non-graceful-node-shutdown-handling","title":"Non-graceful Node Shutdown Handling","text":"<ul> <li>In cases of abrupt failures, all pods are immediately terminated.</li> <li>Risks include data loss and potential service disruption.</li> </ul>"},{"location":"architecture/nodes/#swap-memory-management","title":"Swap Memory Management","text":"<ul> <li>Kubernetes allows for the enabling or disabling of swap memory usage on nodes.</li> <li>Swap usage can impact application performance and pod scheduling decisions.</li> </ul>"},{"location":"containers/container-environment/","title":"Environment","text":""},{"location":"containers/container-environment/#container-environment","title":"Container Environment","text":"<ul> <li>The Kubernetes Container environment provides several important resources to Containers:<ul> <li>Filesystem: A combination of an image and one or more volumes.</li> <li>Container Information: Information about the Container itself.</li> <li>Cluster Information: Information about other objects in the cluster.</li> </ul> </li> </ul>"},{"location":"containers/container-environment/#container-information","title":"Container Information","text":"<ul> <li>The hostname of a Container is the name of the Pod in which the Container is running. This can be accessed through the hostname command or the gethostname function call in <code>libc</code>.</li> <li>The Pod name and namespace are available as environment variables through the downward API.</li> <li>User-defined environment variables from the Pod definition are also available to the Container, as are any environment variables specified statically in the container image.</li> </ul>"},{"location":"containers/container-environment/#cluster-information","title":"Cluster Information","text":"<ul> <li>A list of all services running when a Container was created is available to that Container as environment variables.</li> <li>This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.</li> <li>For a service named <code>foo</code> that maps to a Container named <code>bar</code>, variables like <code>FOO_SERVICE_HOST</code> and <code>FOO_SERVICE_PORT</code> are defined.</li> <li>Services have dedicated IP addresses and are available to the Container via DNS if the DNS addon is enabled.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/","title":"Lifecycle Hooks","text":"<p>Container lifecycle hooks allow containers to be aware of events in their management lifecycle and run specific code when these events occur.</p>"},{"location":"containers/container-lifecycle-hooks/#container-hooks","title":"Container Hooks","text":"<ul> <li>PostStart: This hook is executed immediately after a container is created. However, there's no guarantee that it will execute before the container's <code>ENTRYPOINT</code>. No parameters are passed to the handler.</li> <li>PreStop: This hook is called right before a container is terminated due to various reasons like API request, liveness/startup probe failure, etc. The hook must complete before the <code>TERM</code> signal to stop the container is sent.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/#hook-handler-implementations","title":"Hook Handler Implementations","text":"<ul> <li>Containers can implement two types of hook handlers:<ul> <li><code>Exec</code>: Executes a specific command inside the container's cgroups and namespaces.</li> <li><code>HTTP</code>: Executes an HTTP request against a specific endpoint on the container.</li> </ul> </li> </ul>"},{"location":"containers/container-lifecycle-hooks/#hook-handler-execution","title":"Hook Handler Execution","text":"<ul> <li>Hook calls are synchronous within the context of the Pod containing the container.</li> <li>For <code>PostStart</code> hooks, the Container <code>ENTRYPOINT</code> and hook fire asynchronously.</li> <li><code>PreStop</code> hooks must complete before the <code>TERM</code> signal can be sent.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/#hook-delivery-guarantees","title":"Hook Delivery Guarantees","text":"<ul> <li>Generally, hook delivery is intended to be at least once, meaning a hook may be called multiple times for any given event.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/#debugging-hook-handlers","title":"Debugging Hook Handlers","text":"<ul> <li>Logs for hook handlers are not exposed in Pod events. If a handler fails, it broadcasts an event like <code>FailedPostStartHook</code> or <code>FailedPreStopHook</code>.</li> </ul>"},{"location":"containers/images/","title":"Images","text":""},{"location":"containers/images/#image-pull-operations","title":"Image Pull Operations","text":"<ul> <li>Several methods to provide credentials, including node configuration and <code>imagePullSecrets</code>.</li> <li>Requires keys for access.</li> </ul>"},{"location":"containers/images/#private-registries","title":"Private Registries","text":"<ul> <li>Automatically set based on conditions like whether a tag or digest is specified.</li> </ul>"},{"location":"containers/images/#default-image-pull-policies","title":"Default Image Pull Policies","text":"<ul> <li><code>Never</code>: Never pulls image; uses local if available.</li> <li><code>Always</code>: Always pulls image.</li> <li><code>IfNotPresent</code>: Pulls image only if not present.</li> <li>By default, the pull policy is set to <code>IfNotPresent</code>, meaning the image is pulled only if not already present locally.</li> </ul>"},{"location":"containers/images/#updating-images","title":"Updating Images","text":"<ul> <li>Tags can be added to identify versions.</li> <li>They can include a registry hostname and port number.</li> </ul>"},{"location":"containers/images/#image-names","title":"Image Names","text":"<ul> <li>They are pushed to a registry and then referred to in a Pod.</li> <li>Container images encapsulate an application and its dependencies.</li> </ul>"},{"location":"containers/overview/","title":"Overview","text":""},{"location":"containers/overview/#what-are-containers","title":"What are Containers?","text":"<ul> <li>Technology for packaging an application along with its runtime dependencies.</li> <li>Containers are repeatable and standardized, ensuring the same behavior wherever you run them.</li> <li>They decouple applications from the underlying host infrastructure, making deployment easier across different cloud or OS environments.</li> <li>In a Kubernetes cluster, each node runs the containers that form the Pods assigned to that node.</li> </ul>"},{"location":"containers/overview/#container-images","title":"Container Images","text":"<ul> <li>A container image is a ready-to-run software package.</li> <li>It contains everything needed to run an application: the code, runtime, application and system libraries, and default settings.</li> <li>Containers are intended to be stateless and immutable. Changes should be made by building a new image and recreating the container.</li> </ul>"},{"location":"containers/overview/#container-runtimes","title":"Container Runtimes","text":"<ul> <li>A fundamental component in Kubernetes for running containers effectively.</li> <li>Manages the execution and lifecycle of containers within the Kubernetes environment.</li> <li>Kubernetes supports container runtimes like containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).</li> <li>You can allow your cluster to pick the default container runtime for a Pod or specify the RuntimeClass for different settings.</li> </ul> <p> A simple way to think about the relationship of containers and Kubernetes is that each node can run multiple pods, which in turn each run a single container (typically).</p> <pre><code>graph TD\n    Node[Node] --&gt; Pod1[Pod]\n    Node --&gt; Pod2[Pod]\n    Node --&gt; Pod3[Pod]\n    Node --&gt; Pod4[Pod]\n\n    Pod1 --&gt; Container1[Container]\n    Pod2 --&gt; Container2[Container]\n    Pod3 --&gt; Container3[Container]\n    Pod4 --&gt; Container4[Container]\n\n    style Node fill:#f9f,stroke:#333,stroke-width:4px\n    style Pod1 fill:#bbf,stroke:#333,stroke-width:2px\n    style Pod2 fill:#bbf,stroke:#333,stroke-width:2px\n    style Pod3 fill:#bbf,stroke:#333,stroke-width:2px\n    style Pod4 fill:#bbf,stroke:#333,stroke-width:2px\n    style Container1 fill:#88f,stroke:#333,stroke-width:1px\n    style Container2 fill:#88f,stroke:#333,stroke-width:1px\n    style Container3 fill:#88f,stroke:#333,stroke-width:1px\n    style Container4 fill:#88f,stroke:#333,stroke-width:1px\n</code></pre>"},{"location":"containers/runtime-class/","title":"Runtime Class","text":""},{"location":"containers/runtime-class/#motivation","title":"Motivation","text":"<ul> <li>You can set different RuntimeClasses for different Pods to balance performance and security.</li> <li>For example, you might use a runtime that employs hardware virtualization for Pods requiring high levels of information security.</li> </ul>"},{"location":"containers/runtime-class/#setup","title":"Setup","text":"<ol> <li>Configure the CRI implementation on nodes: This is runtime-dependent and involves setting up configurations that have a corresponding handler name.</li> <li>Create RuntimeClass resources: Each configuration set up in step 1 should have an associated handler name. For each handler, create a corresponding RuntimeClass object.</li> </ol>"},{"location":"containers/runtime-class/#usage","title":"Usage","text":"<ul> <li>You can specify a <code>runtimeClassName</code> in the Pod spec to use a particular RuntimeClass.</li> <li>If the specified RuntimeClass doesn't exist or the CRI can't run the corresponding handler, the Pod will enter a Failed state.</li> </ul>"},{"location":"containers/runtime-class/#scheduling","title":"Scheduling","text":"<ul> <li>You can set constraints to ensure that Pods running with this RuntimeClass are scheduled to nodes that support it.</li> <li>This is done through the scheduling field for a RuntimeClass.</li> </ul>"},{"location":"pods/disruptions/","title":"Disruptions","text":""},{"location":"pods/disruptions/#types-of-disruptions","title":"Types of Disruptions","text":"<ul> <li>Involuntary Disruptions: Unavoidable cases like hardware failure, cloud provider issues, etc.</li> <li>Voluntary Disruptions: Actions initiated by the application owner or cluster administrator, such as draining a node for repair or upgrade.</li> </ul>"},{"location":"pods/disruptions/#mitigating-disruptions","title":"Mitigating Disruptions","text":"<ul> <li>Request the resources your pod needs.</li> <li>Replicate your application for higher availability.</li> <li>Use anti-affinity to spread applications across racks or zones.</li> </ul>"},{"location":"pods/disruptions/#pod-disruption-budgets-pdb","title":"Pod Disruption Budgets (PDB)","text":"<ul> <li>Allows you to specify how many pods of a replicated application can be down simultaneously.</li> <li>Cluster managers should respect PDBs by calling the Eviction API.</li> </ul>"},{"location":"pods/disruptions/#pod-disruption-conditions","title":"Pod Disruption Conditions","text":"<ul> <li>A beta feature that adds a dedicated condition to indicate that the Pod is about to be deleted due to a disruption.</li> </ul>"},{"location":"pods/disruptions/#separation-of-roles","title":"Separation of Roles","text":"<ul> <li>Discusses the benefits of separating the roles of Cluster Manager and Application Owner.</li> </ul>"},{"location":"pods/disruptions/#options-for-cluster-administrators","title":"Options for Cluster Administrators","text":"<ul> <li>Accept downtime, failover to another cluster, or write disruption-tolerant applications and use PDBs.</li> </ul>"},{"location":"pods/downward-api/","title":"Downward API","text":""},{"location":"pods/downward-api/#understanding-the-downward-api","title":"Understanding the Downward API","text":"<p>The Downward API is a feature in Kubernetes that allows pods to retrieve information about themselves or the cluster, which can be exposed to containers within the pod. This mechanism enables containers to consume details about the pod or the cluster without direct interaction with the Kubernetes API server.</p>"},{"location":"pods/downward-api/#two-methods-of-exposure","title":"Two Methods of Exposure","text":"<ul> <li> <p>Environment Variables: Specific pod and container fields can be exposed to running containers as environment variables. This is defined in the pod's configuration file and allows a container to access information like its own name, namespace, or node details.</p> </li> <li> <p>Volume Files: Kubernetes can also expose the same information through files in a volume. This special volume type is called the \"downward API volume,\" and it presents information in a filesystem that the container can read, providing a more dynamic approach to accessing the data.</p> </li> </ul>"},{"location":"pods/downward-api/#benefits-of-low-coupling","title":"Benefits of Low Coupling","text":"<p>The downward API is particularly useful for legacy applications or third-party tools that expect certain information to be available in the environment but are not designed to interact with Kubernetes directly. It simplifies the process of adapting non-native Kubernetes applications to the platform.</p>"},{"location":"pods/downward-api/#available-fields-and-resources","title":"Available Fields and Resources","text":"<ul> <li>Containers can access a variety of information via the Downward API, including:</li> <li>Pod Metadata: Such as the pod's name, namespace, annotations, labels, and unique UID.</li> <li>Resource Requests and Limits: Information about the CPU and memory limits and requests that are set for the container.</li> </ul>"},{"location":"pods/downward-api/#fallback-for-resource-limits","title":"Fallback for Resource Limits","text":"<p>When a container's resource limits are not explicitly defined in the pod specification, the <code>kubelet</code> can expose the default limits as the maximum allocatable resources available on the node. This ensures that the container has some information about the resources it can use, which is critical for managing application performance and resource usage.</p>"},{"location":"pods/downward-api/#use-cases","title":"Use Cases","text":"<ul> <li>Configuration Files: Applications that configure themselves through external files can use the Downward API volume to generate those files.</li> <li>Self-Awareness: Containers that need to be aware of their metadata (for logging, monitoring, or other operational purposes) can use the Downward API to get that information.</li> <li>Resource Management: Containers can adjust their behavior based on the resources available to them, which is particularly useful in high-density multi-tenant environments where resource constraints are common.  The Downward API provides a powerful way to maintain the abstraction that Kubernetes offers while still giving containers the necessary information to operate correctly in a dynamic and distributed system. </li> </ul> <pre><code>graph TD\n    Pod[Pod]\n    EnvVars[Environment Variables]\n    DownwardAPIVolume[Downward API Volume]\n    Container[Container]\n\n    Pod --&gt;|Exposes info via| EnvVars\n    Pod --&gt;|Exposes info via| DownwardAPIVolume\n    EnvVars --&gt;|Accessed by| Container\n    DownwardAPIVolume --&gt;|Accessed by| Container\n\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:2px;\n    class Pod,Container,EnvVars,DownwardAPIVolume k8s;\n</code></pre>"},{"location":"pods/ephemeral-containers/","title":"Ephemeral Containers","text":""},{"location":"pods/ephemeral-containers/#purpose","title":"Purpose","text":"<p>Ephemeral containers are a special type of container designed for temporary tasks like troubleshooting. They are not meant for building applications.</p>"},{"location":"pods/ephemeral-containers/#immutability","title":"Immutability","text":"<p>Once a Pod is created, you can't add a container to it. Ephemeral containers offer a way to inspect the state of an existing Pod without altering it.</p>"},{"location":"pods/ephemeral-containers/#resource-allocation","title":"Resource Allocation","text":"<p>Ephemeral containers don't have guarantees for resources or execution. They will never be automatically restarted.</p>"},{"location":"pods/ephemeral-containers/#limitations","title":"Limitations","text":"<p>Many fields that are available for regular containers are disallowed for ephemeral containers, such as ports and resource allocations.</p>"},{"location":"pods/ephemeral-containers/#creation-method","title":"Creation Method","text":"<p>These containers are created using a special <code>ephemeralcontainers</code> handler in the API, not directly through <code>pod.spec</code>.</p>"},{"location":"pods/ephemeral-containers/#use-cases","title":"Use-Cases","text":"<p>Useful for interactive troubleshooting when <code>kubectl exec</code> is insufficient, especially with distroless images that lack debugging utilities.</p>"},{"location":"pods/ephemeral-containers/#process-namespace-sharing","title":"Process Namespace Sharing","text":"<p>Enabling this feature is helpful for viewing processes in other containers within the same Pod.</p>"},{"location":"pods/init-containers/","title":"Init Containers","text":""},{"location":"pods/init-containers/#what-are-init-containers","title":"What are Init Containers?","text":"<ul> <li>Specialized containers that run before the application containers in a Pod. They can contain utilities or setup scripts not present in the application image.</li> <li>Init containers always run to completion, and each must complete successfully before the next one starts. If an init container fails, it is restarted until it succeeds, depending on the Pod's <code>restartPolicy</code>.</li> </ul>"},{"location":"pods/init-containers/#configuration","title":"Configuration","text":"<ul> <li>Init containers are specified in the Pod specification under the <code>initContainers</code> field, similar to how application containers are defined.</li> </ul>"},{"location":"pods/init-containers/#differences-from-regular-containers","title":"Differences from Regular Containers","text":"<ul> <li>Init containers support all the features of application containers but do not support lifecycle hooks or probes like <code>livenessProbe</code>, <code>readinessProbe</code>, and <code>startupProbe</code>.</li> </ul>"},{"location":"pods/init-containers/#esource-handling","title":"esource Handling","text":"<ul> <li>Resource requests and limits for init containers are managed differently than for application containers.</li> </ul>"},{"location":"pods/init-containers/#sequential-execution","title":"Sequential Execution","text":"<ul> <li>If multiple init containers are specified, they are run sequentially, and each must succeed before the next can run.</li> </ul>"},{"location":"pods/init-containers/#use-cases","title":"Use Cases:","text":"<ul> <li>Running utilities or custom code for setup that are not present in the application image.</li> <li>Blocking or delaying application container startup until certain preconditions are met.</li> <li>Limiting the attack surface by keeping unnecessary tools separate.</li> <li>Examples: The documentation provides YAML examples to demonstrate how to define a Pod with init containers.</li> <li>Advanced features like <code>activeDeadlineSeconds</code> can be used to prevent init containers from failing forever.</li> <li>Starting from Kubernetes v1.28, a feature gate named <code>SidecarContainers</code> allows specifying a <code>restartPolicy</code> for init containers independent of the Pod and other init containers.</li> <li>Resource Sharing: The highest of any particular resource request or limit defined on all init containers is the effective init request/limit for the Pod.</li> <li>Pod Restart Reasons: A Pod can restart, causing re-execution of init containers, for various reasons like Pod infrastructure container restart or all containers in a Pod being terminated.</li> </ul>"},{"location":"pods/overview/","title":"Overview","text":"<p>Pods are the smallest deployable units of computing in Kubernetes. A Pod is a group of one or more containers with shared storage and network resources. They are always co-located and co-scheduled, running in a shared context. Pods model an application-specific \"logical host\" and can contain one or more application containers that are tightly coupled.</p>"},{"location":"pods/overview/#key-concepts","title":"Key Concepts:","text":"<ul> <li> <p>What is a Pod?: A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.</p> </li> <li> <p>Using Pods: Pods are generally not created directly but are created using workload resources like Deployment or Job.</p> </li> <li> <p>Workload Resources: These are resources that manage one or more Pods for you. Examples include Deployment, StatefulSet, and DaemonSet.</p> </li> <li> <p>Pod Templates: These are specifications for creating Pods and are included in workload resources.</p> </li> <li> <p>Pod Update and Replacement: When the Pod template for a workload resource is changed, new Pods are created based on the updated template.</p> </li> <li> <p>Resource Sharing and Communication: Pods enable data sharing and communication among their constituent containers.</p> </li> <li> <p>Storage in Pods: A Pod can specify a set of shared storage volumes that all containers in the Pod can access.</p> </li> <li> <p>Pod Networking: Each Pod is assigned a unique IP address. Containers in a Pod share the network namespace, including the IP address and network ports.</p> </li> <li> <p>Privileged Mode for Containers: Any container in a Pod can run in privileged mode to use operating system administrative capabilities.</p> </li> <li> <p>Static Pods: These are managed directly by the <code>kubelet</code> daemon on a specific node, without the API server observing them.</p> </li> <li> <p>Container Probes: These are diagnostics performed periodically by the <code>kubelet</code> on a container.</p> </li> </ul>"},{"location":"pods/pod-lifecycle/","title":"Pod Lifecycle","text":""},{"location":"pods/pod-lifecycle/#phases","title":"Phases","text":"<p>A Pod's life begins in <code>Pending</code> when it's accepted by the Kubernetes system, but the container images are not yet running. It moves to <code>Running</code> when its containers start, but may enter <code>Succeeded</code> or <code>Failed</code> if it completes its task or encounters an error, respectively. <code>Unknown</code> indicates that the cluster cannot determine the Pod's state, often due to communication problems.</p>"},{"location":"pods/pod-lifecycle/#container-states","title":"Container States","text":"<ul> <li>Containers within a Pod can be in different states:</li> <li><code>Waiting</code>: The container is not yet running its workload, typically because it's pulling its image or waiting for its command to start.</li> <li><code>Running</code>: The container is executing without issues.</li> <li><code>Terminated</code>: The container has stopped, either because it completed its task or due to an error. This state is often accompanied by exit codes and status messages that can be checked using <code>kubectl</code>.</li> </ul>"},{"location":"pods/pod-lifecycle/#container-restart-policy","title":"Container Restart Policy","text":"<ul> <li>The <code>restartPolicy</code> field within a Pod specification dictates the Kubelet's behavior when handling container terminations:</li> <li><code>Always</code>: Automatically restart the container if it stops.</li> <li><code>OnFailure</code>: Restart only if the container exits with a non-zero exit status (indicative of failure).</li> <li><code>Never</code>: Do not automatically restart the container.</li> </ul>"},{"location":"pods/pod-lifecycle/#pod-conditions","title":"Pod Conditions","text":"<ul> <li>These are flags set by the Kubelet to provide more granular status than the phase:<ul> <li><code>PodScheduled</code>: Indicates if the pod has been scheduled to a node.</li> <li><code>ContainersReady</code>: All containers in the Pod are ready.</li> <li><code>Initialized</code>: All init containers have started successfully.</li> <li><code>Ready</code>: The Pod is able to serve requests and should be added to the load balancing pools of all matching services.</li> </ul> </li> </ul>"},{"location":"pods/pod-lifecycle/#custom-readiness-checks","title":"Custom readiness checks","text":"<ul> <li>Can be configured via <code>readinessGates</code> in a Pod's specification, allowing you to define additional conditions to be evaluated before considering a Pod as ready.</li> <li><code>PodReadyToStartContainers</code> is a hypothetical condition that could be used to signify network readiness, implying the Pod's network setup is complete and it's ready to start containers.</li> </ul>"},{"location":"pods/pod-lifecycle/#container-probes","title":"Container Probes","text":"<ul> <li>These are diagnostic tools used by the Kubelet to assess the health and readiness of a container:</li> <li><code>livenessProbe</code>: Determines if a container is running. If this probe fails, the Kubelet kills the container which may be restarted depending on the pod's <code>restartPolicy</code>.</li> <li><code>readinessProbe</code>: Determines if a container is ready to respond to requests. Failing this probe means the container gets removed from service endpoints.</li> <li><code>startupProbe</code>: Used for containers that take a long time to start. If this probe fails, the Kubelet will not start the liveness or readiness probes, giving the container more time to initialize.</li> </ul>"},{"location":"pods/pod-lifecycle/#using-probes","title":"Using Probes","text":"<ul> <li>Liveness Probes: Implement if you need to handle the container's inability to recover from a deadlock or other runtime issues internally.</li> <li>Readiness Probes: Utilize when your container needs to perform certain actions such as warming a cache or migrating a database before it can serve requests.</li> <li>Startup Probes: Employ for slow-starting containers to ensure that Kubernetes doesn't kill them before they're up and running.</li> </ul>"},{"location":"pods/pod-lifecycle/#termination-of-pods","title":"Termination of Pods","text":"<ul> <li>Pods are terminated gracefully, allowing for cleanup and shutdown procedures to complete. The Kubelet sends a <code>SIGTERM</code> signal to the containers, indicating that they should shut down. You can specify the grace period during which a container should complete its shutdown before being forcibly killed.</li> </ul>"},{"location":"pods/qos-classes/","title":"QoS Classes","text":""},{"location":"pods/qos-classes/#scheduler-behavior","title":"Scheduler Behavior","text":"<p>The <code>kube-scheduler</code> does not consider QoS class when selecting which Pods to preempt.</p>"},{"location":"pods/qos-classes/#resource-management","title":"Resource Management","text":"<p>The resource request of a Pod is the sum of the resource requests of its containers, and similarly for the resource limit.</p>"},{"location":"pods/qos-classes/#behavior-independent-of-qos-class","title":"Behavior Independent of QoS Class","text":"<p>Any container exceeding a resource limit will be killed and restarted, and Pods exceeding resource requests become candidates for eviction.</p>"},{"location":"pods/qos-classes/#memory-qos-with-cgroup-v2","title":"Memory QoS with cgroup v2","text":"<p>This feature, in alpha stage as of Kubernetes v1.22, uses the memory controller of cgroup v2 to guarantee memory resources.</p>"},{"location":"pods/qos-classes/#types-of-qos-classes","title":"Types of QoS Classes","text":"<ul> <li>Guaranteed: These Pods have strict resource limits and are least likely to be evicted.</li> <li>Burstable: These Pods have some lower-bound resource guarantees but do not require a specific limit.</li> <li>BestEffort: These Pods can use any available node resources and are the first to be evicted under resource pressure.</li> </ul>"},{"location":"pods/user-namespaces/","title":"User Namespaces","text":""},{"location":"pods/user-namespaces/#feature-state","title":"Feature State","text":"<p>This is an alpha feature introduced in Kubernetes verison 1.25</p>"},{"location":"pods/user-namespaces/#purpose","title":"Purpose","text":"<p>User namespaces isolate the user running inside the container from the one in the host. This enhances security by limiting the damage a compromised container can do to the host or other pods.</p>"},{"location":"pods/user-namespaces/#linux-only-feature","title":"Linux-only Feature","text":"<p>This feature is specific to Linux and requires support for <code>idmap</code> mounts on the filesystems used.</p>"},{"location":"pods/user-namespaces/#container-runtime-support","title":"Container Runtime Support","text":"<p>CRI-O version 1.25 and later support this feature. <code>Containerd</code> v1.7 is not compatible with certain Kubernetes versions in terms of user namespace support.</p>"},{"location":"pods/user-namespaces/#uidgid-mapping","title":"UID/GID Mapping","text":"<p>The <code>kubelet</code> will assign unique host UIDs/GIDs to each pod to ensure no overlap.</p>"},{"location":"pods/user-namespaces/#capabilities","title":"Capabilities","text":"<p>Capabilities granted to a pod are limited to the pod's user namespace and are mostly invalid outside of it.</p>"},{"location":"pods/user-namespaces/#limitations","title":"Limitations","text":"<p>When using a user namespace, you cannot use other host namespaces like network, IPC, or PID.</p>"},{"location":"pods/user-namespaces/#security","title":"Security","text":"<p>The feature mitigates the impact of certain CVEs by ensuring that UIDs/GIDs used by the host's files and host's processes are in a specific range.</p>"}]}