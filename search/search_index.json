{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>NOTE - 5/26/2024</p> <p>I just overhauled most of the content in the guide. I will be doing the same with the diagrams here soon, so expect many missing diagrams until then.</p> <p> Welcome to the Kubernetes Guide, a quick and easy-to-digest summary of core Kubernetes concepts intended to help get you from zero to proficient! </p> <p>The initial focus of this guide is to cover topics to help you pass the Kubernetes Certified Administrator (CKA) exam. As time goes on, I will add more study content here to help prepare for other Kubernetes-related topics.</p> <p></p> <p>Feel free to pick and choose any section in any order, but you'll likely be best served by following along in the default order of the site.</p> <p></p> <p>One thing to note about text formatting in this guide: you'll notice some terms always start with a capital letter (i.e. Service, Pod, etc.). This is intentional and an attempt to adhere to standard formatting as laid out in the official Kubernetes documentation. Kubernetes API objects (like the ones just mentioned) should start with a capital letter.</p> <p></p> <p></p> <p>Legal disclaimer:  </p> <ul> <li> <p>\"Kubernetes\", \"K8s\" and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  </p> </li> <li> <p>Neither myself nor this site are officially associated with the Linux Foundation. </p> </li> </ul> <p></p> <p> Connect with me</p> <p> Suggest changes</p>"},{"location":"configmaps-secrets/","title":"Managing Configuration and Secrets in Kubernetes","text":"<p>Modern applications require dynamic configuration management and secure handling of sensitive data. Kubernetes offers ConfigMaps and Secrets to handle these requirements efficiently, allowing you to decouple configuration from application code and manage sensitive information securely.</p>"},{"location":"configmaps-secrets/#introduction","title":"Introduction","text":"<p>In the traditional monolithic application days, environment variables and configurations were bundled up with the application and deployed as one large object. However, in the cloud-native application model it's important to decouple these for many reasons:  </p> <ol> <li>Environment Flexibility: Decoupling allows the same application to run across different environments (development, staging, production) without code changes. Environment-specific configurations can be applied externally, improving the portability of the application.</li> <li>Scalability and Dynamic Management: When configuration is externalized, it's easier to scale applications horizontally since the configuration can be managed and applied independently. This allows for dynamic reconfiguration in response to changes in load or other factors without redeploying or restarting containers.</li> <li>Security and Sensitive Data Handling: Keeping sensitive configuration data, such as secrets and credentials, separate from the application codebase helps maintain security. It ensures that sensitive data is not exposed within the code and can be securely managed using secrets management tools.</li> <li>Continuous Deployment and Rollbacks: Decoupling facilitates continuous deployment practices by allowing configurations to be updated independently of the application. This separation also simplifies rollback procedures in case a configuration change needs to be reverted without affecting the application version that's running.</li> <li>Maintainability and Clarity: Keeping configuration separate from application code helps maintain a clean codebase and makes it clearer for developers to understand the application logic. It avoids cluttering the application with environment-specific conditionals and settings, making the code easier to maintain and evolve.  </li> </ol>"},{"location":"configmaps-secrets/#understanding-configmaps","title":"Understanding ConfigMaps","text":"What are ConfigMaps? <p>ConfigMaps store non-sensitive configuration data as key-value pairs. They are first-class objects in the Kubernetes API, making them stable and widely supported.</p> Use Cases for ConfigMaps <p>ConfigMaps are used to store:</p> <ul> <li>Environment variables</li> <li>Configuration files (e.g., web server configs)</li> <li>Hostnames</li> <li>Service ports</li> <li>Account names</li> </ul> <p>Avoid storing sensitive data in ConfigMaps; use Secrets for that purpose.</p> Creating ConfigMaps <p>ConfigMaps can be created imperatively or declaratively.</p> Imperative Creation <p>Create a ConfigMap with literal values: <pre><code>$ kubectl create configmap app-config --from-literal=env=prod --from-literal=debug=false\n</code></pre></p> <p>Create a ConfigMap from a file: <pre><code>$ kubectl create configmap app-config --from-file=config.properties\n</code></pre></p> Declarative Creation <p>Define a ConfigMap in a YAML file (<code>configmap.yaml</code>): <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  env: prod\n  debug: \"false\"\n</code></pre></p> <p>Apply the YAML file: <pre><code>$ kubectl apply -f configmap.yaml\n</code></pre></p> Using ConfigMaps <p>Inject ConfigMap data into Pods using environment variables, command arguments, or volumes.</p> As Environment Variables <p>Define environment variables in the Pod specification: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  containers:\n    - name: app-container\n      image: myapp:latest\n      env:\n        - name: ENV\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: env\n        - name: DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: debug\n</code></pre></p> As Volumes <p>Mount the ConfigMap as a volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  volumes:\n    - name: config-volume\n      configMap:\n        name: app-config\n  containers:\n    - name: app-container\n      image: myapp:latest\n      volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n</code></pre></p> Kubernetes-Native Applications <p>Kubernetes-native applications can access ConfigMap data directly via the API server, simplifying configuration management and reducing dependencies on environment variables or volumes.</p>"},{"location":"configmaps-secrets/#understanding-secrets","title":"Understanding Secrets","text":"What are Secrets? <p>Secrets store sensitive data such as passwords, tokens, and certificates. They are similar to ConfigMaps but are designed to handle sensitive information securely.</p> Are Kubernetes Secrets Secure? <p>By default, Kubernetes Secrets are not encrypted in the cluster store or in transit. They are base64-encoded, which is not secure. To enhance security, use additional tools like HashiCorp Vault for better encryption.</p> Creating Secrets <p>Secrets can also be created imperatively or declaratively.</p> Imperative Creation <p>Create a Secret with literal values: <pre><code>$ kubectl create secret generic db-credentials --from-literal=username=dbuser --from-literal=password=securepass\n</code></pre></p> Declarative Creation <p>Define a Secret in a YAML file (<code>secret.yaml</code>): <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\ndata:\n  username: dmlubnk=\n  password: UGFzc3dvcmQxMjM=\n</code></pre></p> <p>Apply the YAML file: <pre><code>$ kubectl apply -f secret.yaml\n</code></pre></p> Using Secrets <p>Inject Secret data into Pods using environment variables, command arguments, or volumes.</p> As Volumes <p>Mount the Secret as a volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  volumes:\n    - name: secret-volume\n      secret:\n        secretName: db-credentials\n  containers:\n    - name: app-container\n      image: myapp:latest\n      volumeMounts:\n        - name: secret-volume\n          mountPath: /etc/secret\n</code></pre></p>"},{"location":"configmaps-secrets/#hands-on-examples","title":"Hands-On Examples","text":"Example ConfigMap <p>Create a ConfigMap with configuration data: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: example-config\ndata:\n  APP_ENV: \"production\"\n  APP_DEBUG: \"false\"\n</code></pre></p> <p>Deploy and inspect the ConfigMap: <pre><code>$ kubectl apply -f example-config.yaml\n$ kubectl describe configmap example-config\n</code></pre></p> Example Secret <p>Create a Secret with sensitive data: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-secret\ntype: Opaque\ndata:\n  username: ZGJ1c2Vy\n  password: c2VjdXJlcGFzcw==\n</code></pre></p> <p>Deploy and inspect the Secret: <pre><code>$ kubectl apply -f db-secret.yaml\n$ kubectl describe secret db-secret\n</code></pre></p>"},{"location":"configmaps-secrets/#conclusion","title":"Conclusion","text":"<p>ConfigMaps and Secrets are essential tools in Kubernetes for managing application configuration and sensitive data. By decoupling configuration from application code and handling sensitive information securely, you can create more flexible, maintainable, and secure applications.</p>"},{"location":"deployments/","title":"Kubernetes Deployments","text":"<p>Deployments in Kubernetes provide powerful capabilities for managing stateless applications. They enable features like self-healing, scaling, rolling updates, and versioned rollbacks, making it easier to maintain robust and scalable applications.</p>"},{"location":"deployments/#key-concepts-of-deployments","title":"Key Concepts of Deployments","text":"What is a Deployment? <p>A Deployment in Kubernetes is a resource that manages a set of identical Pods, ensuring they are up and running as specified. Deployments provide a declarative way to manage updates and scaling of applications.</p> Why Use Deployments? <p>Deployments add several benefits to managing applications:</p> <ul> <li>Self-Healing: Automatically replaces failed Pods.</li> <li>Scaling: Adjusts the number of running Pods based on demand.</li> <li>Rolling Updates: Updates Pods without downtime.</li> <li>Rollbacks: Easily revert to previous versions if something goes wrong.</li> </ul>"},{"location":"deployments/#deployment-architecture","title":"Deployment Architecture","text":"Components <p>Deployments consist of two main components:</p> <ul> <li>Deployment Resource: Defines the desired state and configuration.</li> <li>Deployment Controller: Monitors the Deployment and ensures the current state matches the desired state through reconciliation.</li> </ul> Deployment and ReplicaSets <p>Deployments manage Pods indirectly through ReplicaSets. A ReplicaSet ensures a specified number of Pod replicas are running at any given time. The Deployment controller creates and manages ReplicaSets as needed to fulfill the Deployment's desired state.</p> <p>python apppython apprsdeploy</p> <p>That diagram may look overly complex and bloated with all of the layers of abstraction, but each layer provides powerful value-adds.</p>"},{"location":"deployments/#creating-and-managing-deployments","title":"Creating and Managing Deployments","text":"Creating a Deployment <p>You can create a Deployment using a YAML file that specifies the configuration.</p> <p>Example YAML for Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web-container\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>To apply this configuration, use the following command: <pre><code>$ kubectl apply -f deployment.yaml\n</code></pre></p> Scaling a Deployment <p>You can scale a Deployment either imperatively or declaratively.</p> <p>Imperative Scaling: <pre><code>$ kubectl scale deploy web-deployment --replicas=5\n</code></pre></p> <p>Declarative Scaling: Update the <code>replicas</code> field in your Deployment YAML file and apply the changes: <pre><code>spec:\n  replicas: 5\n</code></pre> <pre><code>$ kubectl apply -f deployment.yaml\n</code></pre></p> Rolling Updates <p>Rolling updates allow you to update your application without downtime. Kubernetes gradually replaces old Pods with new ones.</p> <p>To update the image version in your Deployment, modify the YAML file: <pre><code>spec:\n  template:\n    spec:\n      containers:\n      - name: web-container\n        image: nginx:1.16.0\n</code></pre> Apply the updated YAML file: <pre><code>$ kubectl apply -f deployment.yaml\n</code></pre></p> Monitoring Rollouts <p>You can monitor the status of a rollout using the following command: <pre><code>$ kubectl rollout status deploy web-deployment\n</code></pre></p> Pausing and Resuming Rollouts <p>If needed, you can pause and resume rollouts:</p> <p>Pause: <pre><code>$ kubectl rollout pause deploy web-deployment\n</code></pre> Resume: <pre><code>$ kubectl rollout resume deploy web-deployment\n</code></pre></p> Rolling Back a Deployment <p>If an update causes issues, you can roll back to a previous version. Kubernetes retains old ReplicaSets for this purpose.</p> <p>To roll back to the previous version: <pre><code>$ kubectl rollout undo deploy web-deployment\n</code></pre></p> <p>For more control, you can specify a particular revision: <pre><code>$ kubectl rollout undo deploy web-deployment --to-revision=1\n</code></pre></p>"},{"location":"deployments/#advanced-features","title":"Advanced Features","text":"Autoscaling <p>Kubernetes supports various autoscalers:</p> <ul> <li>Horizontal Pod Autoscaler (HPA): Adjusts the number of Pods based on CPU/memory usage.</li> <li>Vertical Pod Autoscaler (VPA): Adjusts resource limits/requests for running Pods.</li> <li>Cluster Autoscaler (CA): Adjusts the number of nodes in the cluster.</li> </ul> <p>Example of HPA: <pre><code>$ kubectl autoscale deploy web-deployment --cpu-percent=50 --min=1 --max=10\n</code></pre></p> Declarative vs. Imperative Management <p>Kubernetes prefers a declarative approach, where you define the desired state in YAML files, and Kubernetes manages the steps to achieve that state. This contrasts with the imperative approach, where you issue commands to achieve the desired state.</p> <p>Declarative Example: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web-container\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>Apply the file with: <pre><code>$ kubectl apply -f deployment.yaml\n</code></pre></p>"},{"location":"deployments/#practical-exercise","title":"Practical Exercise","text":"Deploying a Sample Application <p>Create a YAML file (<code>sample-deployment.yaml</code>) with the following content: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample\n  template:\n    metadata:\n      labels:\n        app: sample\n    spec:\n      containers:\n      - name: sample-container\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>Deploy it: <pre><code>$ kubectl apply -f sample-deployment.yaml\n</code></pre></p> Scaling the Application <p>Scale the Deployment to 5 replicas: <pre><code>$ kubectl scale deploy sample-deployment --replicas=5\n</code></pre></p> <p>Verify the scaling: <pre><code>$ kubectl get deploy sample-deployment\n</code></pre></p> Updating the Application <p>Update the image version to <code>nginx:1.16.0</code> in the YAML file and apply the changes: <pre><code>$ kubectl apply -f sample-deployment.yaml\n</code></pre></p> <p>Monitor the rollout: <pre><code>$ kubectl rollout status deploy sample-deployment\n</code></pre></p> Rolling Back the Application <p>Rollback to the previous version: <pre><code>$ kubectl rollout undo deploy sample-deployment\n</code></pre></p>"},{"location":"deployments/#conclusion","title":"Conclusion","text":"<p>Deployments in Kubernetes offer a robust mechanism for managing stateless applications. By leveraging features like self-healing, scaling, rolling updates, and rollbacks, you can ensure your applications are resilient, scalable, and easy to maintain. Embracing the declarative model simplifies management and aligns with Kubernetes' principles of infrastructure as code.</p>"},{"location":"ingress/","title":"Managing Ingress in Kubernetes","text":"<p>Ingress in Kubernetes allows you to manage external access to your services, typically HTTP. It provides features like load balancing, SSL termination, and name-based virtual hosting, enabling multiple services to be accessed through a single load balancer.</p>"},{"location":"ingress/#introduction-to-ingress","title":"Introduction to Ingress","text":"Why Use Ingress? <p>Ingress offers a way to expose multiple applications through a single load balancer, addressing the limitations of NodePort and LoadBalancer services:</p> <ul> <li>NodePort services use high port numbers and require clients to track node IP addresses.</li> <li>LoadBalancer services create a one-to-one mapping between internal services and cloud load balancers, which can be costly and limited by cloud provider quotas.</li> </ul> How Ingress Works <p>Ingress is defined in the <code>networking.k8s.io/v1</code> API group and operates at Layer 7 of the OSI model, allowing it to inspect HTTP headers and forward traffic based on hostnames and paths. It requires two main constructs:</p> <ol> <li>Ingress Resource: Defines routing rules.</li> <li>Ingress Controller: Implements the routing rules. Unlike other Kubernetes resources, Ingress controllers are not built-in and must be installed separately.</li> </ol>"},{"location":"ingress/#setting-up-ingress","title":"Setting Up Ingress","text":"Installing an Ingress Controller <p>To use Ingress, you need an Ingress controller. This example uses the NGINX Ingress controller:</p> <ol> <li> <p>Install the NGINX Ingress Controller: <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.4/deploy/static/provider/cloud/deploy.yaml\n</code></pre></p> </li> <li> <p>Check the Ingress Controller Pod: <pre><code>$ kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx\n</code></pre></p> </li> </ol> Configuring Ingress Class <p>Ingress classes allow multiple Ingress controllers to coexist in a single cluster:</p> <ol> <li> <p>List Ingress Classes: <pre><code>$ kubectl get ingressclass\n</code></pre></p> </li> <li> <p>Describe Ingress Class: <pre><code>$ kubectl describe ingressclass nginx\n</code></pre></p> </li> </ol>"},{"location":"ingress/#creating-and-managing-ingress-resources","title":"Creating and Managing Ingress Resources","text":"Deploying Sample Applications <ol> <li>Deploy Apps and Services: <pre><code>$ kubectl apply -f app.yml\n</code></pre></li> </ol> <p>app.yml: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-bacon\nspec:\n  selector:\n    app: bacon\n  ports:\n    - port: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-eggs\nspec:\n  selector:\n    app: eggs\n  ports:\n    - port: 8080\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: bacon\n  labels:\n    app: bacon\nspec:\n  containers:\n    - name: bacon\n      image: mybaconimage\n      ports:\n        - containerPort: 8080\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: eggs\n  labels:\n    app: eggs\nspec:\n  containers:\n    - name: eggs\n      image: myeggsimage\n      ports:\n        - containerPort: 8080\n</code></pre></p> Configuring Ingress Resource <ol> <li>Deploy Ingress Resource: <pre><code>$ kubectl apply -f ig-all.yml\n</code></pre></li> </ol> <p>ig-all.yml: <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: breakfast-all\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: bacon.breakfast.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: svc-bacon\n                port:\n                  number: 8080\n    - host: eggs.breakfast.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: svc-eggs\n                port:\n                  number: 8080\n    - host: breakfast.com\n      http:\n        paths:\n          - path: /bacon\n            pathType: Prefix\n            backend:\n              service:\n                name: svc-bacon\n                port:\n                  number: 8080\n          - path: /eggs\n            pathType: Prefix\n            backend:\n              service:\n                name: svc-eggs\n                port:\n                  number: 8080\n</code></pre></p> Verifying Ingress Setup <ol> <li> <p>Check Ingress Resource: <pre><code>$ kubectl get ing\n</code></pre></p> </li> <li> <p>Describe Ingress Resource: <pre><code>$ kubectl describe ing breakfast-all\n</code></pre></p> </li> </ol>"},{"location":"ingress/#configuring-dns-for-ingress","title":"Configuring DNS for Ingress","text":"<p>To route traffic correctly, configure DNS to point to the Ingress load balancer's IP:</p> <ol> <li>Edit /etc/hosts: <pre><code>212.2.246.150 bacon.breakfast.com\n212.2.246.150 eggs.breakfast.com\n212.2.246.150 breakfast.com\n</code></pre></li> </ol>"},{"location":"ingress/#testing-ingress","title":"Testing Ingress","text":"<p>Open a web browser and try accessing the following URLs:</p> <ul> <li><code>bacon.breakfast.com</code></li> <li><code>eggs.breakfast.com</code></li> <li><code>breakfast.com/bacon</code></li> <li><code>breakfast.com/eggs</code></li> </ul>"},{"location":"ingress/#advanced-ingress-concepts","title":"Advanced Ingress Concepts","text":"Session Affinity <p>Session Affinity ensures that requests from the same client go to the same Pod, which is useful for stateful applications.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-affinity-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  sessionAffinity: ClientIP\n</code></pre></p> External Traffic Policy <p>External Traffic Policy specifies whether traffic from outside the cluster is routed only to Pods on the same node or across all nodes.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  externalTrafficPolicy: Local\n</code></pre></p>"},{"location":"ingress/#troubleshooting-ingress","title":"Troubleshooting Ingress","text":"Common Issues and Solutions <ol> <li>Service Not Accessible:</li> <li>Check Service and Pod status:      <pre><code>$ kubectl get svc\n$ kubectl get pods\n</code></pre></li> <li> <p>Ensure selectors match Pod labels.</p> </li> <li> <p>DNS Resolution Fails:</p> </li> <li>Verify cluster DNS is running:      <pre><code>$ kubectl get pods -n kube-system -l k8s-app=kube-dns\n</code></pre></li> <li>Check <code>/etc/resolv.conf</code> in Pods.</li> </ol> Practical Tips <ul> <li>Use <code>kubectl logs</code> to inspect Ingress controller logs.</li> <li>Restart Ingress controller Pods if necessary:   <pre><code>$ kubectl delete pod -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx\n</code></pre></li> </ul>"},{"location":"ingress/#conclusion","title":"Conclusion","text":"<p>Ingress in Kubernetes provides a powerful way to manage external access to your services. By understanding and utilizing Ingress, you can efficiently route traffic to multiple services using a single load balancer, ensuring scalability and ease of management.</p>"},{"location":"kubernetes-api/","title":"Mastering the Kubernetes API","text":"<p>Understanding the Kubernetes API is essential for mastering Kubernetes. It serves as the backbone of the platform, allowing you to manage resources programmatically and automate cluster operations.</p>"},{"location":"kubernetes-api/#overview-of-the-kubernetes-api","title":"Overview of the Kubernetes API","text":"The Big Picture <p>Kubernetes is an API-centric platform. All resources, such as Pods, Services, and StatefulSets, are defined through the API and managed by the API server. Administrators and clients interact with the cluster by sending requests to create, read, update, and delete these resources. Most interactions are done using <code>kubectl</code>, but they can also be crafted in code or generated through API development tools.</p> Key Concepts <ul> <li>API Server: The central component that exposes the API and handles requests.</li> <li>Resources and Objects: Resources like Pods and Services are defined in the API. When deployed to a cluster, these resources are often called objects.</li> <li>Serialization: The process of converting an object into a string or stream of bytes for transmission or storage. Kubernetes supports JSON and Protobuf for serialization.</li> </ul> How the API Works <p>The Kubernetes API server is the central hub through which all interactions in the cluster are routed, functioning as the front-end interface for Kubernetes' API. Picture it as the Grand Central Station of Kubernetes \u2014 every command, status update, and inter-service communication passes through the API server via RESTful calls over HTTPS. Here's a snapshot of how it operates:  </p> <ul> <li><code>kubectl</code> commands are directed to the API server, whether it's for creating, retrieving, updating, or deleting Kubernetes objects.</li> <li>Node Kubelets keep an eye on the API server, picking up new tasks and sending back their statuses.</li> <li>The control plane services don't chat amongst themselves directly; they communicate through the API server.  </li> </ul>"},{"location":"kubernetes-api/#understanding-serialization","title":"Understanding Serialization","text":"<p>Serialization is essential for transmitting and storing objects. Kubernetes typically uses JSON for communication with external clients and Protobuf for internal cluster traffic due to its efficiency.</p> Example: Serialization in Action <p>When a client like <code>kubectl</code> posts a request, it serializes the object as JSON. The API server then processes this request and sends back a serialized response.</p>"},{"location":"kubernetes-api/#the-api-server","title":"The API Server","text":"Role and Function <p>The API server is the front-end to the Kubernetes API, handling all RESTful HTTPS requests. It manages all interactions between internal components and external clients.</p> Components <ul> <li>Control Plane Service: Runs as a set of Pods in the <code>kube-system</code> Namespace.</li> <li>TLS and Authentication: Ensures secure communication and validates requests.</li> <li>RESTful Interface: Supports CRUD operations via standard HTTP methods (POST, GET, PUT, PATCH, DELETE).</li> </ul> Example: Using the API <p>A typical <code>kubectl</code> command translates into a REST request: <pre><code>$ kubectl get pods --namespace eggs\n</code></pre> This command converts to: <pre><code>GET /api/v1/namespaces/eggs/pods\n</code></pre></p>"},{"location":"kubernetes-api/#hands-on-with-the-api","title":"Hands-On with the API","text":"Exploring the API <ol> <li> <p>Start a Proxy Session: <pre><code>$ kubectl proxy --port 9000 &amp;\n</code></pre></p> </li> <li> <p>Using <code>curl</code> to Interact with the API: <pre><code>$ curl -X GET http://localhost:9000/api/v1/namespaces/eggs/pods\n</code></pre></p> </li> </ol> Creating Resources <ol> <li> <p>Define a Namespace: ns.json: <pre><code>{\n  \"kind\": \"Namespace\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"eggs\",\n    \"labels\": {\n      \"chapter\": \"api\"\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Post the Namespace: <pre><code>$ curl -X POST -H \"Content-Type: application/json\" \\\n--data-binary @ns.json http://localhost:9000/api/v1/namespaces\n</code></pre></p> </li> <li> <p>Verify Creation: <pre><code>$ kubectl get namespaces\n</code></pre></p> </li> <li> <p>Delete the Namespace: <pre><code>$ curl -X DELETE -H \"Content-Type: application/json\" \\\nhttp://localhost:9000/api/v1/namespaces/eggs\n</code></pre></p> </li> </ol>"},{"location":"kubernetes-api/#inspecting-the-api","title":"Inspecting the API","text":"Useful Commands <ol> <li> <p>List All API Resources: <pre><code>$ kubectl api-resources\n</code></pre></p> </li> <li> <p>List Supported API Versions: <pre><code>$ kubectl api-versions\n</code></pre></p> </li> <li> <p>Inspect Specific Resources: <pre><code>$ kubectl explain pods\n</code></pre></p> </li> <li> <p>Using <code>curl</code> to Explore: <pre><code>$ curl http://localhost:9000/apis\n</code></pre></p> </li> </ol>"},{"location":"kubernetes-api/#extending-the-api","title":"Extending the API","text":"Custom Resources <p>Kubernetes allows you to extend the API with CustomResourceDefinitions (CRDs). These custom resources behave like native Kubernetes resources, enabling you to manage new types of objects within your cluster.</p> Example CRD <p>crd.yml: <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: eggs.breakfast.com\nspec:\n  group: breakfast.com\n  scope: Cluster\n  names:\n    plural: recipes\n    singular: recipe\n    kind: Recipe\n    shortNames:\n    - rp\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                bookTitle:\n                  type: string\n                topic:\n                  type: string\n                edition:\n                  type: integer\n</code></pre></p> Deploying the CRD <ol> <li> <p>Apply the CRD: <pre><code>$ kubectl apply -f crd.yml\n</code></pre></p> </li> <li> <p>Create an Instance: eggs.yml: <pre><code>apiVersion: breakfast.com/v1\nkind: Recipe\nmetadata:\n  name: scrambled\nspec:\n  topic: Eggs\n  edition: 1\n</code></pre></p> </li> <li> <p>Apply the Instance: <pre><code>$ kubectl apply -f eggs.yml\n</code></pre></p> </li> <li> <p>Verify Creation: <pre><code>$ kubectl get rp\n</code></pre></p> </li> </ol>"},{"location":"kubernetes-api/#conclusion","title":"Conclusion","text":"<p>The Kubernetes API is a powerful tool for managing your cluster. By understanding its structure and capabilities, you can leverage it to automate and streamline your operations. From creating resources to extending the API with custom definitions, mastering the API is key to unlocking Kubernetes' full potential.</p>"},{"location":"local-setup/","title":"Getting a Local Kubernetes Cluster","text":"<p>For most users, setting up a local Kubernetes cluster using Docker Desktop or KinD (Kubernetes in Docker) is the best option when learning. It's free and allows you to quickly and easily get your hands on and start playing with Kubernetes.</p>"},{"location":"local-setup/#option-1-docker-desktop","title":"Option 1: Docker Desktop","text":"<p>Docker Desktop is a straightforward way to get Docker, Kubernetes, and <code>kubectl</code> on your computer, along with a user-friendly interface for managing your cluster contexts.</p> <p>Steps to Set Up Docker Desktop:</p> <ol> <li>Install Docker Desktop:</li> <li>Search for \"Docker Desktop\" online.</li> <li>Download and run the installer for your operating system (Linux, Mac, or Windows).</li> <li> <p>Follow the installation prompts. For Windows users, install the WSL 2 subsystem when prompted.</p> </li> <li> <p>Enable Kubernetes in Docker Desktop:</p> </li> <li>Click the Docker icon in your menu bar or system tray and go to Settings.</li> <li>Select \"Kubernetes\" from the left navigation bar.</li> <li>Check \"Enable Kubernetes\" and click \"Apply &amp; restart.\"</li> <li> <p>Wait a few minutes for Docker Desktop to pull the required images and start the cluster. The Kubernetes icon in the Docker Desktop window will turn green when the cluster is ready.</p> </li> <li> <p>Verify the Installation:</p> </li> <li>Open a terminal and run the following commands to ensure Docker and kubectl are installed and working:      <pre><code>$ docker --version\n$ kubectl version --client=true -o yaml\n</code></pre></li> <li>Ensure the cluster is running with:      <pre><code>$ kubectl get nodes\n</code></pre></li> </ol>"},{"location":"local-setup/#option-2-kind-kubernetes-in-docker","title":"Option 2: KinD (Kubernetes in Docker)","text":"<p>KinD is an excellent tool for running local Kubernetes clusters using Docker containers. It\u2019s lightweight, flexible, and ideal for development and testing. It's my tool of choice for local development/experimentation.</p> <p>Steps to Set Up KinD:</p> <p>Install KinD:</p> <ul> <li>Follow the instructions on the KinD GitHub page to install KinD on your system.</li> <li>For macOS users, you can simply run <code>brew install kind</code> to get up and running quickly.</li> </ul> <p>Create a KinD Cluster: <pre><code>$ kind create cluster\n</code></pre> Verify your cluster is running: <pre><code>$ kubectl get nodes\n</code></pre></p>"},{"location":"local-setup/#working-with-kubectl","title":"Working with kubectl","text":"<p><code>kubectl</code> is the command-line tool used to interact with your Kubernetes clusters. It's essential for deploying applications, inspecting and managing cluster resources, and troubleshooting issues.</p>"},{"location":"local-setup/#installation","title":"Installation","text":"<p>If you've followed the steps to set up either Docker Desktop or KinD, you should already have kubectl installed. If not, you can install it separately:</p> <p>Linux:</p> <p><pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre> Mac:</p> <pre><code>brew install kubectl\n</code></pre> <p>Windows:</p> <p>Download the executable from the official Kubernetes site and add it to your system PATH.</p>"},{"location":"namespaces/","title":"Namespaces in Kubernetes","text":"<p>Namespaces are a powerful feature in Kubernetes that allow you to segment your cluster into multiple groups, providing organization, isolation, and management capabilities.</p>"},{"location":"namespaces/#understanding-namespaces","title":"Understanding Namespaces","text":"What are Namespaces? <p>In Kubernetes, Namespaces provide a way to partition a single Kubernetes cluster into multiple virtual clusters. This is different from kernel namespaces, which isolate resources at the operating system level.</p> <p>Kernel Namespaces: Isolate operating system resources for containers. Kubernetes Namespaces: Segment a Kubernetes cluster into separate environments for different teams, projects, or applications.</p> Benefits of Using Namespaces <p>Namespaces offer several advantages, including:</p> <ul> <li>Organizational Segmentation: Separate environments for development, testing, and production.</li> <li>Resource Management: Apply different resource quotas and policies to each Namespace.</li> <li>Soft Isolation: Prevent resource conflicts and organize cluster resources logically.</li> </ul>"},{"location":"namespaces/#practical-use-cases-for-namespaces","title":"Practical Use Cases for Namespaces","text":"<p>Namespaces are ideal for managing environments within a single organization, such as:</p> <ul> <li>Development Environments: Separate Namespaces for dev, test, and production environments.</li> <li>Team-Based Separation: Different teams (e.g., finance, HR, operations) each have their own Namespace.</li> <li>Project-Based Isolation: Isolate projects within the same cluster to avoid resource conflicts.</li> </ul> Limitations of Namespaces <p>Namespaces provide soft isolation, meaning they help organize resources but do not offer strong security isolation. For stronger isolation, consider using separate clusters.</p>"},{"location":"namespaces/#working-with-default-namespaces","title":"Working with Default Namespaces","text":"<p>Every Kubernetes cluster comes with some pre-defined Namespaces:</p> <ul> <li>default: The default Namespace for objects with no specified Namespace.</li> <li>kube-system: Contains system components like DNS and metrics server.</li> <li>kube-public: For resources that should be publicly accessible.</li> <li>kube-node-lease: Manages Node heartbeat and leases.</li> </ul> <p>To view the existing Namespaces, use: <pre><code>$ kubectl get namespaces\n</code></pre></p> <p>To delete a Namespace: <pre><code>$ kubectl delete ns my-namespace\n</code></pre></p>"},{"location":"namespaces/#creating-and-managing-namespaces","title":"Creating and Managing Namespaces","text":"Creating a Namespace <p>You can create a Namespace either imperatively or declaratively.</p> <p>Imperative Creation: <pre><code>$ kubectl create ns my-namespace\n</code></pre></p> <p>Declarative Creation: Create a YAML file (<code>namespace.yaml</code>): <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n  labels:\n    environment: dev\n</code></pre> Apply the YAML file: <pre><code>$ kubectl apply -f namespace.yaml\n</code></pre></p> Configuring kubectl for a Specific Namespace <p>To avoid specifying the Namespace in every command, set your context to a specific Namespace: <pre><code>$ kubectl config set-context --current --namespace=my-namespace\n</code></pre></p>"},{"location":"namespaces/#deploying-applications-in-namespaces","title":"Deploying Applications in Namespaces","text":"<p>You can deploy resources to a specific Namespace by specifying it in the YAML file or by using the <code>-n</code> flag with <code>kubectl</code> commands.</p> <p>Example YAML for Deployment: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: my-namespace # specified here\nspec:\n  containers:\n  - name: my-container\n    image: nginx\n    ports:\n    - containerPort: 80\n</code></pre></p> <p>Apply the YAML file: <pre><code>$ kubectl apply -f deployment.yaml\n</code></pre></p> <p>Using the <code>-n</code> Flag: <pre><code>$ kubectl get pods -n my-namespace\n</code></pre></p>"},{"location":"namespaces/#conclusion","title":"Conclusion","text":"<p>Namespaces in Kubernetes are an effective way to manage resources and organize environments within a cluster. While they provide soft isolation and ease of management, remember that they are not suitable for hard multi-tenancy. By using Namespaces, you can efficiently segment your cluster and apply different policies and resource quotas to each segment.</p>"},{"location":"overview/","title":"Introduction to Kubernetes","text":"<p>Kubernetes, often referred to as K8s, is an open-source platform designed to automate deploying, scaling, and operating application containers. It was originally developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). This sections covers the essentials to get you up to speed with Kubernetes, its architecture, and its key features. Think of this as a one-pager or TLDR of Kubernetes.</p>"},{"location":"overview/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes is a container orchestrator, which means it manages the deployment and operation of containerized applications. Containers are lightweight, portable units that bundle an application and its dependencies, allowing them to run consistently across different environments. Kubernetes automates several tasks:</p> <ul> <li>Deployment: Deploys applications seamlessly.</li> <li>Scaling: Adjusts the number of application instances based on demand.</li> <li>Self-healing: Detects and replaces failed instances.</li> <li>Rolling Updates and Rollbacks: Updates applications without downtime and rolls back if needed.</li> </ul> Declarative Model <p>The declarative nature of Kubernetes is key to understanding the power of it. At a super high level, this is how Kubernetes operates:</p> <ol> <li>You tell Kubernetes (typically via <code>kubectl</code>) how you want your application to look. What image to use, how many replicas, ports to expose, etc.</li> <li>Kubernetes persists this desired state to the cluster store (etcd)</li> <li>A series of background controllers consistently check if current state matches desired state.</li> <li>If current state does not equal desired state (i.e. we desire 3 replicas but only 2 are currently running),</li> <li>Kubernetes kicks off a series of actions to reconcile the two states. In the example above, this would involve spinning up an extra replica.</li> </ol> <p>2. persist desired state2. persist desired state4. no, itdoes not4. no, it...API ServerAPI Server1. declare desiredstate1. declare desired...4. no, itdoes not4. no, it...etcdetcd3. check if currentstate = desired state3. check if current...Controller ManagerController Man...3. check if currentstate = desired3. check if current...5. takeaction5. take...controlplane</p>"},{"location":"overview/#historical-background","title":"Historical Background","text":"<p>Kubernetes was born from Google's internal systems like Borg and Omega, which managed containerized applications like Search and Gmail at massive scale. In 2014, Google open-sourced Kubernetes, and it quickly became the standard for container orchestration.</p>"},{"location":"overview/#kubernetes-architecture","title":"Kubernetes Architecture","text":"<p>Kubernetes clusters consist of two types of nodes - control plane nodes and worker nodes:</p> <ul> <li>Control Plane Nodes: These nodes run the Kubernetes control plane, which includes components like the API server, scheduler, and controllers. They manage the overall state of the cluster.</li> <li>Worker Nodes: These nodes run the applications and report back status to the control plane.</li> </ul> Components of the Control Plane <ul> <li>API Server: The front end of Kubernetes that exposes the Kubernetes API. All traffic within, to, and from various Kubernetes components flows through the ARI Server. It is the Grand Central Station or central nervous system of Kubernetes.</li> <li>Cluster Store: A distributed database (etcd) that stores the entire state of the cluster. When your define your desired application specifications, they are stored here. This is the only stateful core component of Kubernetes.</li> <li>Controllers: Ensure the cluster's desired state matches its observed state by running background watch loops on objects like Deployments, Pods, etc.</li> <li>Scheduler: Assigns tasks to worker nodes based on resource availability, application requirements and other criteria.</li> </ul> Components of Worker Nodes <ul> <li>Kubelet: The agent that communicates with the API server and manages containers on the node. The kubelet also communicates directly with the container runtime on the node, instructing it to pull images, and start/stop containers.</li> <li>Runtime: Executes container operations like starting and stopping containers. Common runtimes include containerd and CRI-O.</li> <li>Kube-proxy: Manages networking for containers, including load balancing.</li> </ul> <p>API ServerAPI ServeretcdetcdController ManagerController Man...controlplanec-metcdapiSchedulerSchedulerschedcontainer runtimecontainer runtimekubeletkubeletkube-proxykube-proxynodekubeletk-proxy</p> <p>Note</p> <p>The API Server is the only component in Kubernetes that interacts directly with etcd.</p>"},{"location":"overview/#kubernetes-in-action","title":"Kubernetes in Action","text":"The Declarative Approach <p>Kubernetes operates on a declarative model, where you specify the desired state of the system in YAML or JSON configuration files. The system continuously works to ensure the observed state matches the desired state. This involves three key principles:</p> <ol> <li>Observed State: The current state of the system.</li> <li>Desired State: The state you want the system to achieve.</li> <li>Reconciliation: The process of adjusting the observed state to match the desired state.</li> </ol> Pods and Deployments <ul> <li>Pods: The smallest deployable units in Kubernetes, which can contain one or more containers. Containers within Pods share resources like network and storage.</li> <li>Deployments: Higher-level controllers that manage Pods, providing features like scaling, rolling updates, and rollbacks.</li> </ul> Services <p>Services provide stable networking endpoints for Pods, enabling reliable communication between different parts of an application. They abstract away the ephemeral nature of Pods, which can be created and destroyed dynamically.</p>"},{"location":"overview/#advanced-features","title":"Advanced Features","text":"Self-Healing and Scaling <p>Kubernetes automatically replaces failed Pods and scales the application up or down based on traffic and load. This ensures high availability and efficient resource utilization.</p> Rolling Updates and Rollbacks <p>Kubernetes allows you to update your application without downtime by gradually replacing old Pods with new ones. If something goes wrong, Kubernetes can roll back to the previous version.</p>"},{"location":"overview/#conclusion","title":"Conclusion","text":"<p>Kubernetes is a powerful tool for managing containerized applications, offering automation, scalability, and reliability. By abstracting the underlying infrastructure, it simplifies application deployment and management across various environments. Whether you're running on-premises or in the cloud, Kubernetes provides a consistent and efficient platform for your applications.</p>"},{"location":"pods/","title":"Introduction to Kubernetes Pods","text":"<p>In Kubernetes, every application runs inside a Pod. Understanding how to work with Pods is crucial for deploying, scaling, and managing applications effectively.</p>"},{"location":"pods/#pod-fundamentals","title":"Pod Fundamentals","text":"<p>Pods are the smallest deployable units in Kubernetes and serve as an abstraction layer, allowing various types of workloads to run seamlessly. They enable resource sharing, advanced scheduling, health monitoring, and more.</p> Abstraction and Benefits <p>Pods abstract the complexities of different workload types, enabling Kubernetes to manage them without needing to understand the specifics of each workload. This abstraction allows for uniform deployment and management across heterogeneous environments.</p> <p>python apppython appJava appJava appML modelML modelMySQLdatabaseMySQL...</p> <p>In the image above, all four of those apps are vastly different but once containerized and wrapped in a Pod, Kubernetes treats them all the same and doesn't have to worry about the details of how each application is written or works.</p> Enhancements and Capabilities <p>Pods offer several enhancements for containers, including:</p> <ul> <li>Resource Sharing: Shared filesystem, network stack, memory, process tree, and hostname.</li> <li>Advanced Scheduling: Features like nodeSelectors, affinity rules, topology spread constraints, resource requests, and limits.</li> <li>Health Monitoring and Restart Policies: Probes for application health and policies for container restarts.</li> <li>Security and Termination Control: Enhanced security measures and graceful shutdown processes.</li> <li>Volumes: Shared storage among containers within a Pod.</li> </ul>"},{"location":"pods/#efficient-resource-utilization","title":"Efficient Resource Utilization","text":"Resource Sharing in Pods <p>Pods allow containers to share resources within the same execution environment:</p> <ul> <li>Filesystem and Volumes: Shared through the <code>mnt</code> Linux namespace.</li> <li>Network Stack: Shared via the <code>net</code> Linux namespace.</li> <li>Memory and Process Tree: Shared using the <code>ipc</code> and <code>pid</code> Linux namespaces.</li> <li>Hostname: Shared using the <code>uts</code> Linux namespace.</li> </ul> Scheduling Strategies <p>Kubernetes ensures all containers within a Pod are scheduled on the same Node. Advanced scheduling techniques include:</p> <ul> <li>nodeSelectors: Labels specifying Node requirements.</li> <li>Affinity Rules: Attract or repel Pods based on Node or Pod labels.</li> <li>Topology Spread Constraints: Distribute Pods across zones for high availability.</li> <li>Resource Requests and Limits: Define minimum and maximum resource requirements for Pods.</li> </ul>"},{"location":"pods/#lifecycle-and-management","title":"Lifecycle and Management","text":"Deploying and Managing Pods <p>Deploying a Pod involves several steps:</p> <ol> <li>Define the Pod in a YAML manifest.</li> <li>Post the manifest to the API server.</li> <li>API server will authenticate and authorize the request.</li> <li>API server will validate the Pod specification.</li> <li>The scheduler assigns the Pod to a Node.</li> <li>The <code>kubelet</code> starts and monitors the Pod.</li> </ol> Pod Lifecycle and Immutability <p>Pods are designed to be ephemeral and immutable:</p> <ul> <li>Ephemeral: Created, executed, and terminated without restarting. Pods are deleted upon completion or failure - they are not intended to last forever.</li> <li>Immutable: Once deployed, Pods cannot be modified. To update, a new Pod must be created to replace the old one.</li> </ul> Restart Policies <p>Restart policies apply to individual containers within a Pod:</p> <ul> <li>Always: Always restart containers.</li> <li>Never: Never restart containers.</li> <li>OnFailure: Restart containers only if they fail.</li> </ul> <p>Again, those are policies for containers within the Pod - Pods themselves do not restart.</p>"},{"location":"pods/#practical-examples","title":"Practical Examples","text":"Multi-Container Pods <p>Multi-container Pods follow the single responsibility principle, where each container performs a distinct role. Some example use cases for this pattern include:</p> <ul> <li>Init Containers: Prepare the environment before application containers start.</li> <li>Sidecar Containers: Provide auxiliary services alongside the main application container.</li> </ul> <p>One common example is to use a multi-container Pod for service meshes. In these scenarios, a sidecar container acts as an SSL termination point for all traffic coming into the main Pod.</p> <p>main appcontainermain app...sidecarcontainersidecar...</p> <p>As mentioned above, multiple containers within a Pod share the IP address, network stack, and filesystem. As such, in order to communicate to specific containers within a multi-container Pod, you have to leverage port addresses. The containers themselves however will be able to communicate to each other via localhost.</p> <p>main appcontainermain app...sidecarcontainersidecar...fsfs10.0.0.9:808010.0.0.9:808010.0.0.9:171710.0.0.9:171710.0.0.910.0.0.9localhostlocalhost</p>"},{"location":"pods/#using-kubectl-for-pod-management","title":"Using kubectl for Pod Management","text":"kubectl Basics <p><code>kubectl</code> is the command-line tool for interacting with Kubernetes clusters. Key operations include:</p> <p>Get Pod Info: <pre><code>$ kubectl get pods\n</code></pre> Describe a Pod: <pre><code>$ kubectl describe pod hello-pod\n</code></pre> View Pod Logs: <pre><code>$ kubectl logs hello-pod\n</code></pre></p> Monitoring and Debugging <p>Use <code>kubectl</code> to monitor and debug Pods effectively:</p> <p>Detailed Pod Info: <pre><code>$ kubectl get pods -o wide\n$ kubectl get pods -o yaml\n</code></pre> Pod Logs: <pre><code>$ kubectl logs hello-pod\n</code></pre> Remote Command Execution: <pre><code>$ kubectl exec hello-pod -- ps\n</code></pre> Interactive Shell Session: <pre><code>$ kubectl exec -it hello-pod -- sh\n</code></pre></p>"},{"location":"pods/#conclusion","title":"Conclusion","text":"<p>Pods are the foundational units in Kubernetes, encapsulating applications and providing a robust execution environment. By leveraging Pods effectively, you can take full advantage of Kubernetes' capabilities for deploying, scaling, and managing applications.</p>"},{"location":"security/","title":"Securing Kubernetes: Authentication, Authorization, and Admission Control","text":"<p>Kubernetes security is a critical aspect of managing clusters, ensuring that only authorized users and processes can access and modify resources. This section covers API security, Role-Based Access Control (RBAC), and admission control.</p>"},{"location":"security/#overview-of-kubernetes-security","title":"Overview of Kubernetes Security","text":"The Big Picture <p>Kubernetes is API-centric, with the API server as its core component. Every interaction with the cluster, whether from users, Pods, or internal services, goes through the API server. This makes securing the API server paramount.</p> Typical API Request Flow <p>A typical API request, such as creating a Deployment, follows these steps:</p> <ol> <li>Authentication: Verifies the identity of the requester.</li> <li>Authorization: Checks if the authenticated user has permission to perform the action.</li> <li>Admission Control: Ensures the request complies with policies.</li> </ol>"},{"location":"security/#authentication-authn","title":"Authentication (AuthN)","text":"Understanding Authentication <p>Authentication (authN) is about proving your identity. Kubernetes does not have a built-in identity database; instead, it integrates with external identity management systems. Common methods include:</p> <ul> <li>Client Certificates: Signed by the cluster's Certificate Authority (CA).</li> <li>Webhook Token Authentication: Integrates with external systems.</li> <li>Service Accounts: For intra-cluster communication.</li> </ul> Checking Your Authentication Setup <p>Your cluster's details and user credentials are stored in a <code>kubeconfig</code> file, typically located at: <code>/home/&lt;user&gt;/.kube/config</code></p> <p>Example <code>kubeconfig</code> File: <pre><code>apiVersion: v1\nkind: Config\nclusters:\n- cluster:\n    name: prod-eggs\n    server: https://&lt;api-server-url&gt;:443\n    certificate-authority-data: LS0mRS1F...LS0tRj==\nusers:\n- name: vinny\n  user:\n    token: FfqwFGF1gASDF4...SZY3uUQ\ncontexts:\n- context:\n    name: eggs-admin\n    cluster: prod-eggs\n    user: vinny\ncurrent-context: eggs-admin\n</code></pre></p> Integrating with External IAM Systems <p>Most production clusters integrate with enterprise-grade Identity and Access Management (IAM) systems such as Active Directory or cloud-based IAM solutions, providing robust authentication mechanisms.</p>"},{"location":"security/#authorization-authz-with-rbac","title":"Authorization (AuthZ) with RBAC","text":"Understanding Authorization <p>Authorization (authZ) determines what actions authenticated users can perform. Kubernetes uses a least-privilege model with deny-by-default, meaning you must explicitly grant permissions.</p> Role-Based Access Control (RBAC) <p>RBAC is the most common authorization module, using Roles and RoleBindings to define and assign permissions.</p> <p>Key Components:</p> <ul> <li>Roles: Define a set of permissions.</li> <li>RoleBindings: Assign roles to users or groups.</li> </ul> <p>Example Role: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: eggs\n  name: read-deployments\nrules:\n- verbs: [\"get\", \"watch\", \"list\"]\n  apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n</code></pre></p> <p>Example RoleBinding: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-deployments\n  namespace: eggs\nsubjects:\n- kind: User\n  name: jambo\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: read-deployments\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p> ClusterRoles and ClusterRoleBindings <p>ClusterRoles apply to all Namespaces, allowing for broader permissions management.</p> <p>Example ClusterRole: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: read-deployments\nrules:\n- verbs: [\"get\", \"watch\", \"list\"]\n  apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n</code></pre></p> <p>Example ClusterRoleBinding: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: read-deployments\nsubjects:\n- kind: User\n  name: jambo\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: read-deployments\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p>"},{"location":"security/#admission-control","title":"Admission Control","text":"Overview of Admission Controllers <p>Admission controllers enforce policies on requests after authentication and authorization but before they are persisted. They come in two types:</p> <ul> <li>Mutating Controllers: Modify requests to ensure compliance.</li> <li>Validating Controllers: Reject non-compliant requests.</li> </ul> Common Admission Controllers <ul> <li>NodeRestriction: Limits nodes to modifying their own objects.</li> <li>AlwaysPullImages: Ensures images are always pulled from the registry, preventing the use of cached images.</li> </ul> Example: NodeRestriction <p>To check admission controllers in your cluster: <pre><code>$ kubectl describe pod kube-apiserver-docker-desktop -n kube-system | grep admission\n--enable-admission-plugins=NodeRestriction\n</code></pre></p>"},{"location":"security/#certificates-and-service-accounts","title":"Certificates and Service Accounts","text":"Using Client Certificates <p>Client certificates authenticate users and services within the cluster. They are stored in the <code>kubeconfig</code> file and verified by the API server.</p> Service Accounts <p>Service Accounts provide identities for Pods and controllers, enabling secure intra-cluster communication.</p> <p>Example ServiceAccount: <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n  namespace: default\n</code></pre></p> <p>Using a ServiceAccount in a Pod: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  serviceAccountName: my-service-account\n  containers:\n  - name: my-container\n    image: myimage\n</code></pre></p>"},{"location":"security/#practical-example","title":"Practical Example","text":"Deploying a Secure Application <ol> <li> <p>Create a Namespace: <pre><code>$ kubectl create namespace secure-app\n</code></pre></p> </li> <li> <p>Create a ServiceAccount: <pre><code>$ kubectl apply -f serviceaccount.yaml\n</code></pre></p> </li> </ol> <p>serviceaccount.yaml: <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: secure-app-sa\n  namespace: secure-app\n</code></pre></p> <ol> <li>Deploy a Pod using the ServiceAccount: <pre><code>$ kubectl apply -f pod.yaml\n</code></pre></li> </ol> <p>pod.yaml: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-pod\n  namespace: secure-app\nspec:\n  serviceAccountName: secure-app-sa\n  containers:\n  - name: secure-container\n    image: nginx\n</code></pre></p> <ol> <li>Create a Role and RoleBinding: <pre><code>$ kubectl apply -f role.yaml\n$ kubectl apply -f rolebinding.yaml\n</code></pre></li> </ol> <p>role.yaml: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: secure-app\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre></p> <p>rolebinding.yaml: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: secure-app\nsubjects:\n- kind: ServiceAccount\n  name: secure-app-sa\n  namespace: secure-app\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p>"},{"location":"security/#conclusion","title":"Conclusion","text":"<p>Securing a Kubernetes cluster involves multiple layers of authentication, authorization, and admission control. By understanding and implementing these mechanisms, you can ensure that your cluster is protected from unauthorized access and that all actions comply with defined policies.</p>"},{"location":"services/","title":"Understanding and Utilizing Kubernetes Services","text":"<p>Kubernetes Services are essential for ensuring reliable communication between Pods. They abstract the complexities of networking and provide stable endpoints for applications.</p>"},{"location":"services/#introduction-to-kubernetes-services","title":"Introduction to Kubernetes Services","text":"Why Use Services? <p>Pods in Kubernetes are ephemeral; they can be created, destroyed, and rescheduled at any time due to various events such as scaling operations, rolling updates, rollbacks, and failures. This makes direct communication with Pods unreliable. Kubernetes Services address this issue by providing a stable endpoint for communication.</p> How Services Work <p>Services in Kubernetes provide a front end (DNS name, IP address, and port) that remains constant regardless of the state of the Pods behind it. They use label selectors to dynamically route traffic to healthy Pods that match the specified criteria.</p>"},{"location":"services/#types-of-kubernetes-services","title":"Types of Kubernetes Services","text":"ClusterIP <p>The default Service type, ClusterIP, exposes the Service on an internal IP within the cluster. This makes it accessible only within the cluster.</p> <p>Key Points:</p> <ul> <li>Internal IP and DNS name are automatically created.</li> <li>Accessible only from within the cluster.</li> </ul> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-clusterip-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre></p> NodePort <p>NodePort Services extend ClusterIP Services by making them accessible from outside the cluster through a port on each node.</p> <p>Key Points:</p> <ul> <li>Exposes the Service on a specific port on each node.</li> <li>External traffic can access the Service using <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>.</li> </ul> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n      nodePort: 30007\n</code></pre></p> LoadBalancer <p>LoadBalancer Services are built on top of NodePort and ClusterIP Services. They integrate with cloud provider load balancers to expose Services to the internet.</p> <p>Key Points:</p> <ul> <li>Provides an external IP managed by the cloud provider.</li> <li>Simplifies access from outside the cluster.</li> </ul> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-loadbalancer-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre></p>"},{"location":"services/#detailed-service-theory","title":"Detailed Service Theory","text":"Labels and Selectors <p>Services use labels and selectors to determine which Pods receive traffic. This loose coupling allows Services to dynamically update the list of Pods they route to, maintaining high availability and load balancing.</p> <p>my-deploymy-deploydeploypodpodpodpodsvcenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopenv=devenv=devapp=shopapp=shop</p> <p>It should also be noted that Pods can still belong to a Service if they have extra labels, so long as they also contain all the labels that the Service is selecting on. Below is an example of that:</p> <p>my-deploymy-deploydeploypodpodsvcenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopcur=usdcur=usd</p> <p>Example: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-app-container\n        image: my-app-image\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre></p> EndpointSlices <p>As mentioned above, as Pods are spinning up and down, the Service will keep an updated list of Pods with the given labels and selectors. How it does this is through the use of EndpointSlices, which are effectively just dynamic lists of healthy Pods that match a given label selector.</p> <p>Any new Pods that are created on the cluster that match a Service's label selector will automatically be added to the given Service's EndpointSlice object. When a Pod disappears (fails, Node goes down, etc.) it will be removed from the EndpointSlice. The net result is that the Service's EndpointSlice should always be up to date with a list of healthy Pods that the Service can route to.</p>"},{"location":"services/#hands-on-with-services","title":"Hands-On with Services","text":"Creating and Managing Services Imperative Creation <p>Create a Service for an existing Deployment using <code>kubectl expose</code>: <pre><code>$ kubectl expose deployment my-app --type=LoadBalancer --name=my-service\n</code></pre></p> Declarative Creation <p>Define a Service in a YAML file and apply it: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre> <pre><code>$ kubectl apply -f my-service.yaml\n</code></pre></p> Inspecting Services <p>Check the status and details of Services: <pre><code>$ kubectl get svc\n$ kubectl describe svc my-service\n</code></pre></p>"},{"location":"services/#service-discovery","title":"Service Discovery","text":"How Service Discovery Works <p>Kubernetes uses an internal DNS to resolve Service names to IP addresses. Each Pod's <code>resolv.conf</code> is configured to use the cluster DNS, enabling seamless service discovery.</p> Service Registration <p>Service registration is the process of an app on Kubernetes providing its connection details to a registry in order for other apps on the cluster to be able to find it. This happens automatically when Services are created. </p> <p>High-level flow of Service registration:</p> <ol> <li>Post a Service manifest to the API server (via <code>kubectl</code>).</li> <li>The Service is given a stable IP address called a ClusterIP.</li> <li>EndpointSlices are created to maintain the list of healthy Pods which match the Service's label selector.</li> <li>The Service's name and IP are registered with the cluster DNS.</li> </ol> <p>1. register newservice1. register new...foo-svc10.0.0.8foo-svc...3. at 10.0.0.83. at 10.0.0.8Service Registryfoo-svc = 10.0.0.8bar-svc = 10.0.0.9ham-svc = 10.0.0.10Service Registry...2. where can I findfoo-svc?2. where can I find...4. contact foo-svc4. contact foo-svcappappsvc</p> Practical Example of Service Discovery <p>Assume we have two applications on the same cluster - <code>ham</code> and <code>eggs</code>. Each application has their Pods fronted by a Service, which in turn each have their own ClusterIP.</p> <pre><code>$ kubectl get svc\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nham-svc      ClusterIP   192.168.1.200               443/TCP   5d19h\neggs-svc     ClusterIP   192.168.1.208               443/TCP   5d19h\n</code></pre> <p>For <code>ham</code> to communicate with <code>eggs</code>, it needs to know two things: 1. The name of the <code>eggs</code> application's Service (<code>eggs-svc</code>). 2. How to convert that name to an IP address.</p> <p>Steps for Service Discovery:</p> <ol> <li>The application container's default gateway routes the traffic to the Node it is running on.</li> <li>The Node itself does not have a route to the Service network so it routes the traffic to the node kernel.</li> <li>The Node kernel recognizes traffic intended for the service network and routes the traffic to a healthy Pod that matches the label selector of the Service.</li> </ol>"},{"location":"services/#using-namespaces-with-services","title":"Using Namespaces with Services","text":"Role of Namespaces <p>Namespaces partition a cluster's address space, allowing you to create isolated environments within a single cluster.</p> <p>Example:</p> <ul> <li>Perf: <code>ham-svc.perf.svc.cluster.local</code></li> <li>QA: <code>ham-svc.qa.svc.cluster.local</code></li> </ul> <p>Objects within the same Namespace can connect to each other using short names. However, cross-Namespace communication must use the FQDN.</p> <p>perf Namespaceperf Namespacensham-svcham-svcsvcsaltsaltpodbaconbaconpodham-svcham-svcqa Namespaceqa Namespacenseggs-svceggs-svcsvcpoachedpoachedpodfriedfriedpodeggs-svc.qa.svc.cluster.localeggs-svc.qa.svc.cluster.local</p>"},{"location":"services/#advanced-concepts","title":"Advanced Concepts","text":"Session Affinity <p>Session Affinity allows you to configure Services to direct requests from the same client to the same Pod, useful for stateful applications.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-affinity-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  sessionAffinity: ClientIP\n</code></pre></p> External Traffic Policy <p>External Traffic Policy controls whether traffic from outside the cluster is routed only to Pods on the same node (preserving client IP) or across all nodes.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  externalTrafficPolicy: Local\n</code></pre></p>"},{"location":"services/#troubleshooting-services","title":"Troubleshooting Services","text":"Common Issues and Solutions <p>Service Not Accessible:</p> <ul> <li>Check the status of the Service and Pods:      <pre><code>$ kubectl get svc\n$ kubectl get pods\n</code></pre></li> <li>Ensure the selectors match the Pod labels.</li> </ul> <p>DNS Resolution Fails:</p> <ul> <li>Verify the cluster DNS is running:      <pre><code>$ kubectl get pods -n kube-system -l k8s-app=kube-dns\n</code></pre></li> <li>Check the contents of <code>/etc/resolv.conf</code> in the Pods.</li> </ul> Practical Tips <ul> <li>Use <code>kubectl logs</code> to inspect logs of coreDNS Pods for DNS-related issues.</li> <li>Restart coreDNS Pods if necessary:   <pre><code>$ kubectl delete pod -n kube-system -l k8s-app=kube-dns\n</code></pre></li> </ul>"},{"location":"services/#summary","title":"Summary","text":"<p>Kubernetes Services are critical for managing and accessing applications within a cluster. They provide stable endpoints and load balancing, abstracting the dynamic nature of Pods. By understanding and utilizing different Service types and configurations, you can ensure robust and scalable application deployment.</p>"},{"location":"statefulsets/","title":"Managing Stateful Applications with Kubernetes StatefulSets","text":"<p>StatefulSets are essential for deploying and managing stateful applications on Kubernetes, which require persistent storage and stable network identities. This includes databases, key-value stores, and applications that maintain client session data.</p>"},{"location":"statefulsets/#introduction-to-statefulsets","title":"Introduction to StatefulSets","text":"What are StatefulSets? <p>StatefulSets are a Kubernetes resource designed to manage stateful applications. Unlike Deployments, StatefulSets provide:</p> <ul> <li>Predictable and persistent Pod names</li> <li>Persistent DNS hostnames</li> <li>Persistent volume bindings</li> </ul> <p>These features ensure that each Pod maintains a consistent identity, even across restarts, failures, and rescheduling.</p> Key Differences from Deployments <p>While both StatefulSets and Deployments are used to manage Pods, StatefulSets offer additional guarantees: - Ordered Creation and Deletion: Pods are created and deleted in a specific order. - Unique Network Identities: Each Pod gets a unique, stable network identity. - Stable Storage: Each Pod is associated with persistent storage that remains consistent across restarts.</p> <p>StatefulSets are Kubernetes constructs designed to manage stateful applications that require persistent data and identity across Pod restarts and deployments. Each Pod in a StatefulSet is given a stable and unique network identifier and persistent storage, which remains associated with the Pod, even when it is rescheduled to a different node within the cluster. </p> <p>StatefulSets are Kubernetes tools for running and managing applications that need to remember who they are and what they know\u2014think of them like memory keepers for your apps, such as databases that need to recall data after a reboot. Unlike Deployments that are more about stateless apps (think of them as forgetful but easily replaceable), StatefulSets make sure each of their Pods has a consistent name, network identity, and storage, even if they move around in the cluster. This makes StatefulSets perfect for when your app's individual identity and history are crucial for running smoothly.  </p> <p>StatefulSets can guarantee Pod names, volume bindings, and DNS hostnames across reboots - whereas Deployments cannot. Below are two diagrams that illustrate this point:  </p> Node/Pod Failure with Deployments <p>deploypodvol10.0.0.510.0.0.5deploypod10.0.0.910.0.0.9Pod/NodefailurePod/Node... </p> Node/Pod Failure with StatefulSets <p>podvol10.0.0.510.0.0.5pod10.0.0.510.0.0.5Pod/NodefailurePod/Node...stsstsvol</p> <p>Notice how with a Deployment, when a Pod is replaced it comes up with a new name, IP address, and its volume is no longer bound to it. With StatefulSets, the new Pod comes up looking exactly the same as the previous failed one.  </p>"},{"location":"statefulsets/#statefulset-theory","title":"StatefulSet Theory","text":"Pod Naming <p>Each Pod in a StatefulSet gets a predictable name, following the format <code>&lt;StatefulSetName&gt;-&lt;integer&gt;</code>. For example, a StatefulSet named <code>my-sts</code> with three replicas will have Pods named <code>my-sts-0</code>, <code>my-sts-1</code>, and <code>my-sts-2</code>.</p> Ordered Creation and Deletion <p>StatefulSets create and delete Pods in a specific order:</p> <ul> <li>Creation: Pods are created one at a time, waiting for each to be running and ready before creating the next.</li> <li>Deletion: Pods are deleted in reverse order, ensuring that the highest ordinal Pod is terminated first.</li> </ul> Volume Management <p>StatefulSets manage volumes through PersistentVolumeClaims (PVCs). Each Pod gets its own unique volume, which is preserved across restarts and reattachments:</p> <ul> <li>Volume Naming: Volumes are named based on the StatefulSet and Pod names, e.g., <code>vol-my-sts-0</code>, <code>vol-my-sts-1</code>.</li> <li>Persistence: Volumes remain attached to the same Pod, even if the Pod is rescheduled to a different node.</li> </ul>"},{"location":"statefulsets/#hands-on-with-statefulsets","title":"Hands-On with StatefulSets","text":"Deploying StatefulSets Example StatefulSet Configuration <p>Here\u2019s an example of a simple StatefulSet for a mysqlDB deployment:</p> <p>StatefulSet YAML: <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: my-sts\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  serviceName: \"my-sts\"\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: ctr-mysql\n        image: mysql:latest\n        ports:\n        - containerPort: 27017\n  volumeClaimTemplates:\n  - metadata:\n      name: mysql-data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: \"fast\"\n      resources:\n        requests:\n          storage: 15Gi\n</code></pre></p> Creating a StatefulSet <p>Deploy the StatefulSet using the following command: <pre><code>$ kubectl apply -f statefulset.yaml\n</code></pre></p> Inspecting StatefulSet and Pods <p>Check the status of the StatefulSet and its Pods: <pre><code>$ kubectl get sts\n$ kubectl get pods\n</code></pre></p> Scaling StatefulSets <p>StatefulSets can be scaled up or down, ensuring order and data integrity:</p> <ul> <li>Scaling Up: New Pods are created sequentially.</li> <li>Scaling Down: Pods are deleted in reverse order.</li> </ul> <p>To scale the StatefulSet: <pre><code>$ kubectl scale sts my-sts --replicas=4\n</code></pre></p> Handling Failures <p>StatefulSets handle failures by automatically recreating Pods with the same identity and volume bindings:</p> <ul> <li>Pod Failure: A failed Pod is replaced with a new Pod with the same name and volume.</li> <li>Node Failure: Modern Kubernetes versions handle node failures more effectively, replacing Pods on failed nodes automatically.</li> </ul> Using Headless Services <p>StatefulSets often use headless Services to manage network identities:</p> <p>Headless Service YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-prod\nspec:\n  clusterIP: None\n  selector:\n    app: mysql\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sts-mysql\nspec:\n  serviceName: mysql-prod\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:latest\n        ports:\n        - containerPort: 28018\n</code></pre></p>"},{"location":"statefulsets/#scaling-and-updating-statefulsets","title":"Scaling and Updating StatefulSets","text":"Scaling StatefulSets <p>Edit the StatefulSet YAML to change the replica count and apply the changes: <pre><code>$ kubectl apply -f statefulset.yaml\n</code></pre></p> Rolling Updates <p>Update the image version in the StatefulSet YAML and apply the changes to perform a rolling update: <pre><code>$ kubectl apply -f statefulset.yaml\n</code></pre></p>"},{"location":"statefulsets/#conclusion","title":"Conclusion","text":"<p>StatefulSets are crucial for managing stateful applications in Kubernetes. They provide stable network identities, persistent storage, and ordered Pod creation and deletion. By leveraging StatefulSets, you can ensure your stateful applications are robust, scalable, and resilient.</p>"},{"location":"storage/","title":"Mastering Kubernetes Storage","text":"<p>Storing and retrieving data is crucial for most real-world applications. Kubernetes' persistent volume subsystem allows you to connect to enterprise-grade storage systems that provide advanced data management services such as backup and recovery, replication, and snapshots.</p>"},{"location":"storage/#overview","title":"Overview","text":"<p>Kubernetes supports a variety of storage systems, including those from major cloud providers and enterprise-class solutions like EMC and NetApp. This section will cover:</p> <ul> <li>The big picture of Kubernetes storage</li> <li>Various storage providers</li> <li>The Container Storage Interface (CSI)</li> <li>Kubernetes persistent volume subsystem</li> <li>Dynamic provisioning with Storage Classes</li> <li>Hands-on examples</li> </ul>"},{"location":"storage/#the-big-picture","title":"The Big Picture","text":"<p>Kubernetes supports different types of storage, such as block, file, and object storage, from various external systems, either in the cloud or on-premises.</p> High-Level Architecture <p>Storage providers connect to Kubernetes through a plugin layer, often using the Container Storage Interface (CSI). This standardized interface simplifies integrating external storage resources with Kubernetes. </p> <p>CSICSIPVsubsystemPV...AzureAzureNetAppNetAppGCPGCP</p> Key Components <ul> <li>Storage Providers: External systems providing storage services, like EMC, NetApp, or cloud providers.</li> <li>Plugin Layer: Connects external storage systems with Kubernetes, typically using CSI plugins.</li> <li>Kubernetes Persistent Volume Subsystem: Standardized API objects that allow applications to consume storage easily.</li> </ul>"},{"location":"storage/#storage-providers","title":"Storage Providers","text":"<p>Kubernetes supports a wide range of external storage systems, each typically providing its own CSI plugin. These plugins are usually installed via Helm charts or YAML installers and run as Pods in the <code>kube-system</code> Namespace.</p> Restrictions <ul> <li>Cloud-Specific: You can't provision and mount GCP volumes if your cluster is on Microsoft Azure.</li> <li>Locality: Pods often need to be in the same region or zone as the storage backend.</li> </ul>"},{"location":"storage/#container-storage-interface-csi","title":"Container Storage Interface (CSI)","text":"<p>The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage systems to containerized workloads on Container Orchestration Systems (COS) like Kubernetes. CSI allows for the consistent configuration and management of storage solutions across various container orchestration systems.</p> <p>CSI enables storage providers to develop a standardized plugin once and have it work across a multitude of container orchestration systems without requiring changes. This simplifies the process of adding new storage capabilities to Kubernetes clusters and ensures compatibility and extendibility.</p> <p>While CSI is a critical piece of getting storage working in Kubernetes, unless you explicitly work on writing storage plugins you'll likely never interact with it directly. Most of your interaction with CSI will simply be referencing your relevant CSI plugin in YAML files.</p> Benefits of CSI <ul> <li>Decoupled Updates: CSI plugins can be updated independently of Kubernetes releases.</li> <li>Broad Compatibility: CSI plugins work across different orchestration platforms.</li> </ul> Installing CSI Plugins <p>Most cloud platforms pre-install CSI plugins for native storage services. Third-party storage systems require manual installation, often available as Helm charts or YAML files.</p>"},{"location":"storage/#kubernetes-persistent-volume-subsystem","title":"Kubernetes Persistent Volume Subsystem","text":"<p>The Persistent Volume Subsystem uses several key resources to manage storage:</p> <ul> <li>PersistentVolumes (PV): Represent external storage volumes.</li> <li>PersistentVolumeClaims (PVC): Requests for storage by applications.</li> <li>StorageClasses (SC): Define different classes of storage for dynamic provisioning.</li> </ul> Workflow Example <ol> <li>Pod Requests Storage: Via a PersistentVolumeClaim (PVC).</li> <li>PVC Requests Creation: PVC asks the StorageClass (SC) to create a new PV on the storage backend.</li> <li>CSI Plugin Interaction: The SC uses the CSI plugin to provision the volume.</li> <li>Volume Creation: The external volume is created and reported back to Kubernetes.</li> <li>PV and PVC Binding: The PV is mapped to the created volume, and the Pod mounts the PV.</li> </ol>"},{"location":"storage/#dynamic-provisioning-with-storage-classes","title":"Dynamic Provisioning with Storage Classes","text":"<p>StorageClasses (SCs) allow you to define different types of storage. How they are defined depends on the type of storage you're using. For example, if you're using Google Cloud Storage you have classes such as Standard, Nearline, Coldline, and Archive. You may also have simpler/more straightforward classes at your disposal such as SSD and HDD. When you create a SC you map both of those definitions so Pods in your cluster can use either or.</p> Example YAML for a StorageClass <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ssd\nprovisioner: pd.csi.storage.gke.io  # Google Cloud CSI plugin\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nparameters:\n  type: pd-ssd  # Google Cloud SSD drives\n  provisioned-iops-on-create: '10000'\n</code></pre> Key Points <ul> <li>Immutability: StorageClass objects cannot be modified once created.</li> <li>Meaningful Names: Use descriptive names for easy reference.</li> <li>Provisioner-Specific Parameters: The parameters block varies between different plugins.</li> </ul>"},{"location":"storage/#example","title":"Example","text":"<p>Example YAML: Below is the high-level flow for creating and using StorageClasses:</p> <ol> <li>Ensure you have a storage back-end (cloud, on-prem, etc.)</li> <li>Have a running Kubernetes cluster</li> <li>Install and setup the CSI storage plugin to connect to Kubernetes</li> <li>Create at least one StorageClass on Kubernetes</li> <li>Deploy Pods with PVCs that reference those Storage classes</li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  volumes:\n    - name: data\n      persistentVolumeClaim:\n        claimName: mypvc\n  containers:\n  - name: my-container\n    image: myimage\n    volumeMounts:\n    - name: data\n      mountPath: /data\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mypvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi\n  storageClassName: fast\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: fast\nprovisioner: pd.csi.storage.gke.io\nparameters:\n  type: pd-ssd\n</code></pre>"},{"location":"storage/#additional-volume-settings","title":"Additional Volume Settings","text":"Access Modes <ul> <li>ReadWriteOnce (RWO): Single PVC can bind to a volume in read-write mode.</li> <li>ReadWriteMany (RWM): Multiple PVCs can bind to a volume in read-write mode.</li> <li>ReadOnlyMany (ROM): Multiple PVCs can bind to a volume in read-only mode.</li> </ul> Reclaim Policy <ul> <li>Delete: Deletes PV and external storage when PVC is released.</li> <li>Retain: Keeps PV and external storage when PVC is deleted, requiring manual cleanup.</li> </ul>"},{"location":"storage/#conclusion","title":"Conclusion","text":"<p>Kubernetes provides a robust storage subsystem that allows applications to dynamically provision and manage storage from various external systems. By leveraging CSI plugins and StorageClasses, you can create flexible and scalable storage solutions tailored to your application's needs.</p>"}]}