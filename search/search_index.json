{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Currently reworking the content and organization of the site.</p> <p> Welcome to the Kubernetes Guide, a quick summary of core Kubernetes concepts intended to help get you from zero to proficient!  Feel free to pick and choose any section in any order, but you'll likely be best served by following along in the default order of the site.</p> <p></p> <p>Legal discalimer:  </p> <ul> <li> <p>\"Kubernetes\", \"K8s\" and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  </p> </li> <li> <p>Neither myself nor this site are officially associated with the Linux Foundation.  </p> </li> </ul>"},{"location":"config-maps-secrets/","title":"ConfigMaps and Secrets","text":"<p>test</p>"},{"location":"deployments/","title":"Deployments","text":""},{"location":"deployments/#overview","title":"Overview","text":"<p>The main idea behind Deployments is that you tell Kubernetes the desired state of your application a looping controller watches your app and continuously attempts to reconcile the actual state of your app with the desired state.</p>"},{"location":"deployments/#deployment-spec","title":"Deployment Spec","text":"<p>The way you tell Kubernetes how you want your application to look is through the use of a YAML file (Deployment spec). When you POST the Deployment spec (via <code>kubectl</code>) to the API server, Kubernetes goes through the process of deploying your application to match the desired state and leverages a Deployment controller to continuously watch your application state.  </p> <p>It should be noted that every Deployment object will only manage a single Pod object. If you have an application with more than one Pod, you will need more than one Deployment object. But, a single Deployment object can manage any number of replicas of a given Pod.</p>"},{"location":"deployments/#replicasets","title":"ReplicaSets","text":"<p>Under the covers, Deployments actually leverage a different Kubernetes object to handle Pod scaling and reboots - the RelpicaSet. You should never be managing ReplicaSets directly, but it's good to know they exist and understand the hierarchy of control here. Containers will be wrapped in Pods, which have their scaling and self-healing managed by ReplicaSets, which in turn are managed by Deployments.</p> <pre><code>flowchart TB\n    subgraph Deployment\n        subgraph ReplicaSet\n            Pod1[Pod]\n            Pod2[Pod]\n        end\n    end</code></pre>"},{"location":"deployments/#scaling-and-self-healing","title":"Scaling and Self-Healing","text":"<p>If you deploy a pod by itself (either via a YAML file or <code>kubectl</code>), if it dies or fails, the Pod is lost forever.</p> <p>We never \"revive\" Pods; the appropriate way to \"revive\" a failed Pod is to create a new one to replace it.</p> <p>However, with the magic of Deployments, if a Pod that was created via Deployment fails, it will be replaced. Remember that Deployment controllers continuously watch for deviations from your desired state; so if you specified that your application should run 3 Pods and one of the Pods fails, the controller will recognize that actual state (2 Pods) no longer matches desired state (3 Pods), and it will kick off a series of actions to deploy another Pod.</p>"},{"location":"deployments/#rolling-updates","title":"Rolling Updates","text":"<p>This same logic allows seamless, zero-downtime updates for your applications. Let's say you defined your application to have 5 Pods running and labeled it as being <code>v1.2</code>. Your team introduces some new features or implements some bug fixes and creates <code>v1.3</code> of your application. Your next step will be to go in and update your desired state (Deployment spec) from <code>v1.2</code> <code>v1.3</code>. The Deployment controller will then recognize that the actual state (<code>v1.2</code>) no longer matches the desired state (<code>v1.3</code>) and begin the process of spinning down outdated Pods and spinning up new Pods with the new version.  </p> <p>Some things to keep in mind for this to work: your application(s) need(s) to maintain loose coupling and maintain backwards and forwards compatability (cloud native application design pattern).  </p> <p>There are different rolling update strategies you can employ that specify how to handle rollouts/rollbacks, how many can be deploye or spun down at once, etc. For more in-depth information on these strategies, refer to the official Kubernetes documentation.</p>"},{"location":"deployments/#rollbacks","title":"Rollbacks","text":"<p>Rollbacks work in the same manner as rolling updates from above but in reverse. Essentially, imagine you had an issue with <code>v1.3</code> and need to roll back to <code>v1.2</code>. It's as simple as updating your Deployment spec and letting the Deployment controller notice this change and begin that reconcilliation process. Kubernetes let you specify how many revisions (old versions) of your Deployments should be maintained for the purposes of rollbacks. In your Deployment spec, this is defined by the <code>revisionHistoryLimit</code> block.  </p> <p>You can view the update history of a deployment by running the following command: <pre><code>kubectl rollout history deployment/&lt;deployment-name&gt;\n</code></pre></p>"},{"location":"deployments/#scaling","title":"Scaling","text":"<p>Performing manual scaling operations with Deployments is also super straightforward and can be done in a similar manner to the one above. If you decide you actually want 10 Pods instead of 5, it's as simple as updating your Deployment spec and updating the <code>Replicas</code> block to the desired amount of Pods. Once again, the Deployment controller will notice the variation in states and begin reconciliation.</p>"},{"location":"dns/","title":"DNS","text":"<p>test</p>"},{"location":"ingress/","title":"Ingress","text":"<p>test</p>"},{"location":"namespaces/","title":"Namespaces","text":""},{"location":"namespaces/#overview","title":"Overview","text":"<p>Namespaces are used to partition Kubernetes clusters and provide easy ways to apply policies and quotas at a more granular level.  </p> <p>Namespaces are not intended to be used for secure isolation</p> <p>If you need secure isolation, the best practice is to use multiple clusters.</p> <p>Namespaces can be a useful construct for partitioning a single cluster among various environments for teams. For instance, the a single cluster might have development and production environments partitioned by Namespace.  </p> <p>Kubernetes comes with a number of Namspaces already created. You can run the following command to view all namespaces on a cluster:  </p> <pre><code>$ kubectl get namespaces\n    NAME              STATUS   AGE\n    default           Active   22h\n    gmp-public        Active   22h\n    gmp-system        Active   22h\n    kube-node-lease   Active   22h\n    kube-public       Active   22h\n    kube-system       Active   22h\n</code></pre> <p>Your output will vary based on your environment.  </p>"},{"location":"namespaces/#deploying-objects-to-namespaces","title":"Deploying Objects to Namespaces","text":"<p>When deploying objects on Kubernetes you can specify the target Namespace imperatively by adding the <code>-n &lt;Namespace&gt;</code> flag to your command, or declaratively by specifying the Namespace in your YAML file.  </p> <p>If you don't explicitly define a Namespace, objects will be deployed to the <code>default</code> Namespace.</p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#history","title":"History","text":"<p>Kubernetes itself was born out of Google's experience running billions of containers at scale and managing them with proprietary systems called Borg and Omega. In 2014 Google donated Kubernetes as an open-source project to the Cloud Native Computing Foundation (CNCF).</p>"},{"location":"overview/#20k-foot-view","title":"20K-foot View","text":"<p>At a high-level, Kubernetes is responsible for deploying your applications and dynamically responding to changes to keep your applications running how you intended. Kubernetes runs on any cloud or on-premise datacenter, abstracting away all of the underlying infrastructure and letting you focus on application development. All applications running on Kubernetes must be containerized, and those containers must be running inside of a Pod.  </p> <p>Fundamentally, Kubernetes is a cluster - a group of machines, so to speak. These machines are called nodes in the Kubernetes world and can be cloud instances, virtual machines, physical servers, your laptop, etc.  </p> <p>A Kubernetes cluster consists of a control plane and any number of worker nodes. The control plane is the \"brain\" of Kubernets and handles things such as scheduling workloads to nodes, implenting the API, and watching for changes that need to be responded to. The worker nodes handle the leg-work of actually running applications.</p>"},{"location":"overview/#api-server","title":"API Server","text":"<p>Speaking of, the API server is the central component for all communication for all components in Kubernetes.  </p> <p>Any communication inbound or outbound to/from the Kubernetes cluster must be routed through the API server.</p>"},{"location":"overview/#cluster-store","title":"Cluster Store","text":"<p>The control plane, like many aspects of Kubernetes, exists in a stateless manner. However, the clsuter store does not - it persistently stores the state of the cluster and other configuration data. As of Kubernetes v1.28, <code>etcd</code> is the distributed databse that Kubernetes leverages for it's cluster store.  </p> <p><code>etcd</code> is installed on every control plane node by default for high-availability. However, it does not tolerate split-brain scenarios and will prevent updates to the cluster in such states - but it will still allow applications to run in those scenarios.</p>"},{"location":"overview/#controller-manager","title":"Controller Manager","text":"<p>Kubernetes consists of many different controllers, which are essentially background loops that watch for changes to the cluster (and alert when things don't match up so other components can take action). All controllers are managed and implemented by a higher-level component called the controller manager. </p> <p>The following logic is at the core of what Kubernetes is and how it works:  </p> <pre><code>flowchart TD\n    A(Obtain desired state) --&gt; B(Observe current state)\n    B --&gt; C{desired = current?}\n    C --&gt;|Yes| B\n    C --&gt;|No| E[Take action]</code></pre>"},{"location":"overview/#declarative-model","title":"Declarative Model","text":"<p>At the core of Kubernetes is the concept of the declarative model. You tell Kubernetes how you want your application to look and run (how many replicas, which image to use, network settings, commands to run, how to perform updates, etc.), and it's Kubernetes job to ensure that happens. You \"tell\" Kubernetes through the use of manifest files written in YAML.  </p> <p>You take those manifest files and <code>POST</code> them to the Kubernetes API server (typically through the use of <code>kubectl</code> commands). The API server will then authenticate the request, inspect the manifest for formatting, route the request to the appropriate controller (i.e. if you've defined a manifest file for a Deployment, it will send the request to the Deployments controller), and then it will record your desired state in the cluster store (remember, <code>etcd</code>). After this, the relevant controller will get started on performing any tasks necessary to get your application into it's desired state.  </p> <p>After your application is up and running, controllers begin monitoring it's state in the background and ensuring it matches the desired state in <code>etcd</code> (see simple logic diagram above).</p>"},{"location":"pods/","title":"Pods","text":"<p>Pods are the atomic unit of scheduling in Kubernetes. As virtual machines were in the VMware world, so are Pods in the world of Kubernetes. Every container running on Kubernetes must be wrapped up in a Pod. The most simple implementation of this are single-container Pods - one container inside one Pod. However there are certain instances where multi-container Pods make sense.</p> <p>It's important to note that when you scale up/down application in Kubernetes, you're not doing so by adding/removing containers directly - you do so by adding/removing Pods.</p>"},{"location":"pods/#atomic","title":"Atomic","text":"<p>Pod deployment is atomic in nature - a Pod is only considered Ready when all of its containers are up and running. Either the entire Pod comes up successfully and is running, or the entire thing doesn't. - there are no partial states.</p>"},{"location":"pods/#lifecycle","title":"Lifecycle","text":"<p>Pods are designed to be ephemeral in nature. Once a Pod dies, it's not meant to be restarted or revived. Instead, the intent to spin up a brand new Pod in the failed ones place (based off of your defined Manifest). Further, Pods are immutable and should not be changed once running. If you need to chance your application, you update the configuration via the manifest and deploy a new Pod.</p>"},{"location":"pods/#multi-container-pods","title":"Multi-container Pods","text":"<p>As mentioend above, the simplest way to run an app on Kubernetes is to run a single container inside of a single Pod. However, in situations where you need to tightly couple two or more functions you can co-locate multiple containers inside of the same pod. One such example would be leveraging the sidecar pattern for logging wherein the main container dumps logs to a supporting container that can sanitize and format the logs for consumption. This frees up the main container from having to worry about formatting logs.</p>"},{"location":"security/","title":"Security","text":"<p>test</p>"},{"location":"services/","title":"Services","text":"<p>test</p>"},{"location":"statefulsets/","title":"StatefulSets","text":"<p>test</p>"},{"location":"storage/","title":"Storage","text":"<p>test</p>"}]}