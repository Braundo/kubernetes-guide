{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Kubernetes Guide, a quick and easy-to-digest summary of core Kubernetes concepts intended to help get you from zero to proficient! </p> <p>The initial focus of this guide is to cover topics to help you pass the Kubernetes Certified Administrator (CKA) exam. As time goes on, I will add more study content here to help prepare for other Kubernetes-related topics.</p> <p></p> <p>Feel free to pick and choose any section in any order, but you'll likely be best served by following along in the default order of the site. </p> <p></p> <p>Legal disclaimer:  </p> <ul> <li> <p>\"Kubernetes\", \"K8s\" and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  </p> </li> <li> <p>Neither myself nor this site are officially associated with the Linux Foundation. </p> </li> </ul> <p></p> <p> Connect with me</p> <p> Suggest changes</p>"},{"location":"core-concepts/config-maps-secrets/","title":"ConfigMaps & Secrets","text":""},{"location":"core-concepts/config-maps-secrets/#background","title":"Background","text":"<p>Alongside the ability to store and retrieve data, another key capability for applications is the ability to be configured or instantiated via environment variables or commands.  </p> <p>In the traditional monolithic application days, environment variables and configurations were bundled up with the application and deployed as one large object. However, in the cloud-native application model it's important to decouple these for many reasons:  </p> <ol> <li>Environment Flexibility: Decoupling allows the same application to run across different environments (development, staging, production) without code changes. Environment-specific configurations can be applied externally, improving the portability of the application.</li> <li>Scalability and Dynamic Management: When configuration is externalized, it's easier to scale applications horizontally since the configuration can be managed and applied independently. This allows for dynamic reconfiguration in response to changes in load or other factors without redeploying or restarting containers.</li> <li>Security and Sensitive Data Handling: Keeping sensitive configuration data, such as secrets and credentials, separate from the application codebase helps maintain security. It ensures that sensitive data is not exposed within the code and can be securely managed using secrets management tools.</li> <li>Continuous Deployment and Rollbacks: Decoupling facilitates continuous deployment practices by allowing configurations to be updated independently of the application. This separation also simplifies rollback procedures in case a configuration change needs to be reverted without affecting the application version that's running.</li> <li>Maintainability and Clarity: Keeping configuration separate from application code helps maintain a clean codebase and makes it clearer for developers to understand the application logic. It avoids cluttering the application with environment-specific conditionals and settings, making the code easier to maintain and evolve.  </li> </ol> <p>Let's take a look at an example from point #1 there. Imagine you have an application that runs in 3 different environments: <code>dev</code>, <code>perf</code>, and <code>prod</code>. Each environment has different configurations such as credentials, network policies, security policies, etc. In the old world, if you were to package those configurations with the application, you'd end up with three separate images stored in three separate repositories. Any time a developer needs to make an update to the application, they must ensure they update it across all three repos, rebuild all three images, and redeploy all three images.</p> <p>dev repo- dev credentials- dev network policy- dev security policydev repo...perf repo- perf credentials- perf network policy- perf security policyperf repo...prod repo- prod credentials- prod network policy- prod security policyprod repo...deploys todeploys todev appdev appdeploys todeploys toperf appperf appdeploys todeploys toprod appprod appbuildsbuildsbuildsbuildsbuildsbuildsdevenvironmentdev...perfenvironmentperf...prodenvironmentprod... </p> <p>A better way to handle this is by decoupling those configuration values from your application. You build and maintain a single application repository and build &amp; run that single image in all environments. For this to be possible, your applications should be built as plain as possible with as little configuration necessary embedded. The configurations for each environment are then stored separately and applied to the various environments at runtime. </p> <p>In this manner, application code is updated in one repository, one image is used, and configurations are independently managed.</p> <p>dev CM- dev credentials- dev network policy- dev security policydev CM...perf CM- perf credentials- perf network policy- perf security policyperf CM...prod CM- prod credentials- prod network policy- prod security policyprod CM...deploys todeploys todev appdev appdeploys todeploys toperf appperf appdeploys todeploys toprod appprod appapplyCMsapply...devenvironmentdev...perfenvironmentperf...prodenvironmentprod...app repoapp source codeapp repo...appimageapp...</p>"},{"location":"core-concepts/config-maps-secrets/#configmaps","title":"ConfigMaps","text":"<p>ConfigMaps are used within Kubernetes to store non-sensitive, configuration data that containers in your Pods may need to consume. Common uses include: </p> <ul> <li>Hostnames: Names of other services that the application may need to communicate with.</li> <li>Server Configurations: External server configurations like server names or IP addresses.</li> <li>Database Configurations: Database connection details except passwords, which should be stored in Secrets.</li> <li>Account Names: Usernames or other account identifiers.</li> <li>Environment Variables: Other miscellaneous settings as key-value pairs that your application uses to modify its behavior in different environments.</li> </ul> <p>You should not use ConfigMaps to store sensitive data such as passwords. Secrets should be used for that purpose.</p> <p>Under the covers, ConfigMaps are effectively key-value pairs. Keys are completely arbitrary and can be any name created from letters, numbers, underscores, dashes, and dots. Values can contain anything. As with many other key-value paradigms, they are separated by a colon (<code>key</code>:<code>value</code>). </p> <p>As mentioned above, a (simple) database configuration might look something like this in a ConfigMap definition:</p> <pre><code>hostname: mysql-dev-01\ndb-port: 3306\nusername: vinny\n</code></pre> <p>Data stored in ConfigMaps can be injected into a container in a number of ways:  </p> <ol> <li>As environment variable(s)</li> <li>Arguments in the container's startup command</li> <li>As files in a volume</li> </ol> <p>These methods are all transparent to the application - it has no idea the ConfigMap is even a thing, it just knows it's data is where it's supposed to be. How it got there is an irrelevant mystery.</p>"},{"location":"core-concepts/config-maps-secrets/#environment-variables","title":"Environment Variables","text":"<p>To inject data as environment variables, you created a ConfigMap and map its entries to environment variables inside of the Pod spec template. Once the Pod and underlying container start, the environment variables will appear as standard environment variables for the OS relevant to that container.  </p> <p>Here's how that might look when injecting some database information into a container using a ConfigMap called <code>myCM</code>:  </p> <p><code>myCM</code>: <pre><code>...\ndata:\n  database: mysql-01\n  loc: STL\n  user: vinny\n</code></pre></p> <p>Pod definition: <pre><code>spec:\n  containers:\n  ...\n    env:\n      - name: host\n      valueFrom:\n        configMapKeyRef:\n          name: myCM\n          key: database\n      - name: location\n      valueFrom:\n        configMapKeyRef:\n          name: myCM\n          key: loc\n      - name: user\n      valueFrom:\n        configMapKeyRef:\n          name: myCM\n          key: user\n</code></pre></p> <p>With these configs, this is what the mapping from ConfigMap to container variables would look like:  </p> <ul> <li>database host</li> <li>loc location</li> <li>user user </li> </ul> <p>If you were to login interactively to the container, you would be able to seamlessly view these environment variables:  </p> <pre><code>$ echo $host\nmysql-01\n\n$ echo $location\nSTL\n\n$ echo $user\nvinny\n</code></pre>"},{"location":"core-concepts/config-maps-secrets/#container-startup-commands","title":"Container Startup Commands","text":"<p>This method of injecting data into containers from ConfigMaps is pretty straightforward. In your Pod template YAML, you specify a startup command and insert variables defined from your ConfigMap. Below is an example of inserting the database hostname from above into a startup command for the container. Here is how the Pod YAML might look:</p> <pre><code>...\nspec:\n  containers:\n  - name: my-container-1\n    image: busybox\n    command: [\"/bin/sh\", \"-c\", \"echo Database to use is $(host)\"]\n    env:\n      - name: host\n        valueFrom:\n          configMapKeyRef:\n            name: myCM\n            key: database\n</code></pre> <p>From this Pod definition, when the container starts it will run the following shell command that we defined:</p> <pre><code>echo Database to use is $(host)\n</code></pre> <p>In our config, we mapped <code>host</code> to the <code>database</code> key in <code>myCM</code>, which we defined as having a value of <code>mysql-01</code>. Thus, when the container runs that command, it will produce the following output:</p> <pre><code>Database to use is mysql-01\n</code></pre>"},{"location":"core-concepts/config-maps-secrets/#volumes","title":"Volumes","text":"<p>The most flexible way to leverage ConfigMaps is with volumes. By using them with volumes you can reference entire configuration files and make live updates to them which will be reflected in running containers. The entire process can be summed up in the following steps:</p> <ol> <li>Create a ConfigMap</li> <li>Create a ConfigMap volume in your Pod spec</li> <li>Mount the ConfigMap volume into the container </li> </ol> <p>Here's an example YAML file that would create a Pod called <code>configMapVol</code>, a volume called <code>volMap</code>, and mounts the <code>volMap</code> volume to <code>/etc/regions</code>:  </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: configMapVol\nspec:\n  volumes:\n    - name: volMap\n      configMap:\n        name: my-cm\n  containers:\n    - name: container1\n      image: busybox\n      volumeMounts:\n        - name: volMap\n          mountPath: /etc/regions\n</code></pre> <p>If you were to deploy this Pod and exec into it, you could run an <code>ls</code> command to view the files we defined in the ConfigMap diagram above mounted at <code>/etc/regions</code>:  </p> <pre><code>$ kubectl exec configMapVol -- ls /etc/regions\ncentral\nwest\n</code></pre>"},{"location":"core-concepts/config-maps-secrets/#secrets","title":"Secrets","text":"<p>Secrets are extremely similar in shape and function to ConfigMaps in that they hold configuration data that can be injected into containers at run-time. However, Secrets differ in the fact that they base-64 encode values. </p> <p>Secrets are crucial for managing sensitive data such as passwords, tokens, and keys within Kubernetes. Unlike ConfigMaps, Secrets are intended to hold confidential information and offer a mechanism to reduce the risk of exposure:</p> <ul> <li>Encoding Data: Secrets store data in Base64 encoded format, which does not encrypt data but merely encodes it to obfuscate clear text.</li> <li>Usage in Pods: Secrets can be mounted as data volumes or exposed as environment variables to be used by a Pod without exposing the information in the Pod's definition or source code.</li> <li>Security Practices: It's essential to secure access to Secrets using Kubernetes RBAC policies to ensure that only authorized Pods and users can retrieve them.</li> </ul> <p>Here's how you might define a Secret and use it within a Pod:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\ntype: Opaque\ndata:\n  password: UGFzc3dvcmQxMjM=  # Base64 encoded \"Password123\"\n  user: dmlubnk=              # Base64 encoded \"vinny\"\n</code></pre> <p>These values are not encrypted by default and can easily be decoded.</p> <p>A standard flow for implementing secrets looks like this:  </p> <pre><code>flowchart TD\n    A[Create Secret and persist to cluster store - unencrypted] --&gt; B[Pod is configured to use Secret]\n    B --&gt; C[Secret data is transferred - unencrypted - to the node]\n    C --&gt; D[Node kubelet starts the Pod and its containers]\n    D --&gt; E[Secret is mounted into the container's temp filesystem and decoded into plain text]\n    E --&gt; F[Application consumes Secret]\n    F --&gt; G[Secret is deleted from the node once the Pod is deleted]</code></pre> <p>And here's an example of how to define a Pod and use the Secret as a volume:  </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  volumes:\n  - name: secret-vol\n    secret: \n      secretName: my-secret\n  containers:\n  - name: my-container\n    image: busybox\n    volumeMounts:\n    - name: secret-vol\n      mountPath: /etc/secrets/\n</code></pre> <p>Secrets are mounted as read-only objects in the containers to prevent accidental manipulation.</p>"},{"location":"core-concepts/container-runtime/","title":"Container Runtime","text":""},{"location":"core-concepts/container-runtime/#history","title":"History","text":"<p>Initially, Docker was the predominant container runtime, integral to Kubernetes' orchestration capabilities. To accommodate a broader range of container technologies, Kubernetes later introduced the Container Runtime Interface (CRI), facilitating integration with any runtime conforming to the Open Container Initiative (OCI) standards, notably the <code>imagespec</code> for building images and the <code>runtimespec</code> for developing runtimes. </p> <p>Despite its popularity, Docker did not initially support CRI, as it predated the standard. To bridge this gap, Docker developed the Docker Shim\u2014a provisional solution for integrating Docker with Kubernetes without full CRI compliance. Docker's ecosystem also includes <code>containerd</code>, an OCI-compliant runtime capable of operating independently of Docker. </p> <p>Starting with Kubernetes version 1.24, support for Docker Shim was phased out in favor of <code>containerd</code>, which became the default runtime. Nonetheless, Docker images remain compatible with Kubernetes due to their adherence to the <code>imagespec</code> OCI standard, ensuring seamless operation with <code>containerd</code>. </p> <p>As Kubernetes grew in popularity, Kubernetes introduced the Container Runtime Interface (CRI) which allowed any runtime to be used as long as they adhered to the Open Container Initiative (OCI). There are two main specs to be aware of:  - <code>imagespec</code> - how an image should be built - <code>runtimespec</code> - how a container runtime should be developed </p>"},{"location":"core-concepts/container-runtime/#containerd","title":"containerd","text":"<p><code>containerd</code> has achieved graduated status within the CNCF, highlighting its maturity and stability as a container runtime. It can be installed independently of Docker, providing a streamlined, Docker-free deployment option. While <code>containerd</code> includes a command-line tool called <code>ctr</code>, this tool is primarily intended for debugging purposes and may not be as user-friendly for general usage.  </p> <p><code>containerd</code> also comes with a CLI tool called <code>ctr</code> which is not very user-friendly and mainly used for debugging containerd.</p>"},{"location":"core-concepts/controllers/","title":"Controllers","text":""},{"location":"core-concepts/controllers/#understanding-kubernetes-controllers","title":"Understanding Kubernetes Controllers","text":"<p>In Kubernetes, a controller is a background process that constantly monitors the state of various components within the cluster. Its primary function is to reconcile the current state of the cluster with the desired state specified by the user.</p> Key Controllers in Kubernetes <ul> <li> <p>Node Controller: Monitors the health of nodes by checking 'heartbeats' received through the Kubernetes API server every 5 seconds. If a heartbeat is missed, the node is given a 40-second grace period before being marked as 'Unreachable'. If the node remains unreachable for 5 minutes, it is considered down, and its Pods are evicted and rescheduled to available nodes, ensuring continued availability and resilience of applications.</p> </li> <li> <p>Replication Controller: Ensures that the number of replicas for a Pod matches the desired state defined in a ReplicaSet. If there are too few Pods, it creates more; if there are too many, it removes the excess.</p> </li> </ul> Other Essential Controllers <ul> <li>Deployment Controller: Manages the life cycle of deployments by updating Pods and ReplicaSets.</li> <li>Namespace Controller: Handles namespace creation, updating, and deletion.</li> <li>Endpoint Controller: Populates the Endpoints object (that is, joins Services &amp; Pods).</li> <li>CronJob Controller: Manages time-based jobs, ensuring they run at specified times.</li> <li>ServiceAccount Controller: Manages Service Accounts, automating token creations.</li> <li>Job Controller: Oversees tasks that run to completion.</li> <li>StatefulSet Controller: Manages applications that require persistent state or unique identities.</li> </ul> Kubernetes Controller Manager <p>All these controllers are consolidated into a single binary \u2014 the Kubernetes Controller Manager. This component optimizes cluster management by centralizing the control mechanisms required to manage various cluster states effectively.</p> <p>By understanding and leveraging these controllers, operators can ensure their Kubernetes clusters operate smoothly and resiliently, automatically handling failures and changes in the environment.</p>"},{"location":"core-concepts/controllers/#extending-controller-functionality","title":"Extending Controller Functionality","text":"<p>Kubernetes is also extensible, allowing developers to create custom controllers that introduce new behaviors or manage third-party resources, further enhancing the ecosystem's capabilities.</p>"},{"location":"core-concepts/deployments/","title":"Deployments","text":""},{"location":"core-concepts/deployments/#overview","title":"Overview","text":"<p>The main idea behind Deployments is that you tell Kubernetes the desired state of your application while a looping controller watches your app and continuously attempts to reconcile the actual state of your app with the desired state you previously defined.</p>"},{"location":"core-concepts/deployments/#deployment-spec","title":"Deployment Spec","text":"<p>The way you tell Kubernetes how you want your application to look is through the use of a YAML file (Deployment spec). When you POST the Deployment spec (via <code>kubectl</code>) to the API server, Kubernetes goes through the process of deploying your application to match the desired state and leverages a Deployment controller to continuously watch your application state.  </p> <p>It should be noted that while each Deployment object is configured primarily around a single Pod template, it can manage multiple replicas of that Pod. This means one Deployment can handle scaling of identical Pods horizontally to meet demand.</p>"},{"location":"core-concepts/deployments/#replicasets","title":"ReplicaSets","text":"<p>Under the covers, Deployments actually leverage a different Kubernetes object to handle Pod scaling and reboots - the ReplicaSet. You should never be managing ReplicaSets directly, but it's good to know they exist and understand the hierarchy of control here. Containers will be wrapped in Pods, which have their scaling and self-healing managed by ReplicaSets, which in turn are managed by Deployments.</p> <p>deployrspodpodpod</p>"},{"location":"core-concepts/deployments/#scaling-and-self-healing","title":"Scaling and Self-Healing","text":"<p>If you deploy a pod by itself (i.e. not via a Deployment), if it dies or fails, the Pod is lost forever.</p> <p>We never \"revive\" Pods; the appropriate way to \"revive\" a failed Pod is to create a new one to replace it.</p> <p>However, with the magic of Deployments, if a Pod that was created via Deployment fails, it will be replaced. Remember that Deployment controllers continuously watch for deviations from your desired state; so if you specified that your application should run 3 Pods and one of the Pods fails, the controller will recognize that actual state (2 Pods) no longer matches desired state (3 Pods), and it will kick off a series of actions to deploy another Pod.</p>"},{"location":"core-concepts/deployments/#rolling-updates","title":"Rolling Updates","text":"<p>This same logic enables seamless, zero-downtime updates for your applications. For example, if your application is currently running with five Pods at version v1.2, and you want to update to v1.3 after some improvements or fixes, you would update your Deployment spec to reflect the new version. The Deployment controller recognizes that the actual state (v1.2) does not match the desired state (v1.3) and initiates a rolling update. This process involves gradually terminating older-version Pods and replacing them with new-version Pods to ensure no downtime.</p> <p>Some things to keep in mind for this to work: your application(s) need(s) to maintain loose coupling and maintain backwards and forwards compatability (cloud native application design pattern). For upgrades or rollbacks to truly have zero-downtime, forward and backword compatibility is a must, as is having clearly defined API specs.</p> <p>There are different rolling update strategies you can employ that specify how to handle rollouts/rollbacks, how many can be spun up or down at once, etc. For more in-depth information on these strategies, refer to the official Kubernetes documentation.</p>"},{"location":"core-concepts/deployments/#rollbacks","title":"Rollbacks","text":"<p>Rollbacks work in the same manner as rolling updates from above but in reverse. Imagine you had an issue with <code>v1.3</code> and need to roll back to <code>v1.2</code>. It's as simple as updating your Deployment spec and letting the Deployment controller notice this change and begin that reconciliation process. Kubernetes let you specify how many revisions (old versions) of your Deployments should be maintained for the purposes of rollbacks. In your Deployment spec, this is defined by the <code>revisionHistoryLimit</code> block.  </p> <p>You can view the update history of a Deployment by running the following command: <pre><code>kubectl rollout history deployment/&lt;deployment-name&gt;\n</code></pre></p>"},{"location":"core-concepts/deployments/#scaling","title":"Scaling","text":"<p>Performing manual scaling operations with Deployments is also super straightforward and can be done in a similar manner to the one above. If you decide you actually want 10 Pods instead of 5, it's as simple as updating your Deployment spec and updating the <code>Replicas</code> block to the desired amount of Pods. Once again, the Deployment controller will notice the variation in states and begin reconciliation.</p>"},{"location":"core-concepts/deployments/#monitoring-and-debugging-deployments","title":"Monitoring and Debugging Deployments","text":"<p>Effective management of deployments includes monitoring their status and performance. You can use <code>kubectl</code> to track the progress of a deployment and check for any issues. For example, <code>kubectl rollout status deployment/&lt;deployment-name&gt;</code> provides real-time updates about the deployment process. Additionally, examining the logs of Pods managed by a Deployment can offer insights into runtime operations and help identify errors or misconfigurations.</p>"},{"location":"core-concepts/kube-proxy/","title":"Kube Proxy","text":""},{"location":"core-concepts/kube-proxy/#overview","title":"Overview","text":"<p>Kube Proxy is an essential component of Kubernetes networking, ensuring that the communication between Pods across different nodes is possible and efficient. It acts as a network proxy and load balancer for a service on a single Kubernetes node.</p> Role of Kube Proxy <p>Kube Proxy maintains network rules on nodes. These rules allow network communication to Pods from network sessions inside or outside of the cluster. Kube Proxy ensures that each Pod can communicate with others seamlessly, facilitating the distributed nature of a Kubernetes cluster.</p> How Kube Proxy Works <ul> <li>Service Discovery: Kube Proxy watches the Kubernetes API server for the addition of new services. When it discovers a new service, it sets up routes on the node to handle traffic to that service.</li> <li>Traffic Routing: Kube Proxy routes the traffic directed at services to the appropriate Pods, regardless of which node the Pod is on. This is crucial for the functionality of Kubernetes services, which provide a stable interface to Pods that might change or be replaced over time.</li> </ul> Pod Network Implementation <ul> <li>Virtual Networking: Pods in a Kubernetes cluster communicate via a virtual network that spans all the nodes in the cluster. Kube Proxy configures and maintains this network, ensuring all Pods can reach each other.</li> <li>Handling Non-Pod Traffic: Services, which act as stable endpoints for Pods, do not join Pod networks because they are not processes that can have network interfaces. Instead, services forward requests to Pods using rules set up by Kube Proxy.</li> </ul> Forwarding Rules and Load Balancing <p>Kube Proxy can operate in different modes, each affecting how network traffic is handled: - User space mode: Traffic is forwarded using space switching. - iptables mode: Uses native Linux iptables to route traffic, which is less resource-intensive and faster. - IPVS mode: Supports more advanced load balancing features.</p> <p>The choice of mode can affect the performance and capabilities of the network, with IPVS mode providing the best performance for large-scale, high-throughput systems.</p>"},{"location":"core-concepts/kube-proxy/#practical-example-monitoring-kube-proxy-activity","title":"Practical Example: Monitoring Kube Proxy Activity","text":"<p>Monitoring Kube Proxy is important for diagnosing network issues and ensuring that the routing of traffic to services is efficient. Here are some metrics you might consider monitoring:</p> <ul> <li>Traffic volume: Measures the amount of traffic being routed by Kube Proxy.</li> <li>Rule changes: Tracks when rules are added, updated, or removed, which can indicate changes in services or Pod configurations.</li> <li>Latency metrics: Helps identify delays in traffic routing, which can impact application performance.</li> </ul> <p>Tools like Prometheus can be used to collect and visualize these metrics, providing insights into the health and performance of Kube Proxy.</p> <p>By enhancing your understanding of Kube Proxy and its operational dynamics, you can better manage and troubleshoot Kubernetes network issues, ensuring robust and efficient communications across your cluster.</p>"},{"location":"core-concepts/kubelet/","title":"Kubelet","text":""},{"location":"core-concepts/kubelet/#overview","title":"Overview","text":"<p>Kubelet is a vital agent running on each node in a Kubernetes cluster. Its primary role is to ensure that the containers are running in a Pod as specified in the Pod's manifest.</p> Core Responsibilities of Kubelet <ol> <li>Node Registration: When a node is added to the cluster, Kubelet is responsible for registering it with the API Server, making it available for scheduling Pods.</li> <li>Pod Lifecycle Management: After the Scheduler decides to place a Pod on a particular node, it communicates this decision to the Kubelet through the API Server. Kubelet then takes steps to ensure that the Pod's containers start running:</li> <li>Container Runtime Communication: Kubelet instructs the container runtime to pull the required image(s) and run the instance(s).</li> <li>Health Monitoring: Continuously monitors the health of running containers and restarts containers that have failed.</li> <li>Resource Management: Manages the node's resources through features like CPU management, Memory management, and Device Scheduling.</li> </ol> Kubelet's Operation Cycle <p>Kubelet operates in a loop, checking the status of containers on its node and adjusting them to match the desired state described in the Pod specifications. This process includes:</p> <ul> <li>Listening for new assignments from the API Server.</li> <li>Executing Pod and container operations such as start, stop, or restart.</li> <li>Reporting back the status of the node and its Pods to the API Server.</li> </ul> Kubelet and Pod Lifecycle <p>Here's a breakdown of how Kubelet manages a Pod's lifecycle:</p> <ul> <li>Scheduling and Running: Once Kubelet is notified of a new Pod to be scheduled on its node, it interacts with the container runtime to create the environment the Pod requires.</li> <li>Lifecycle Hooks: Kubelet also handles lifecycle hooks that developers can use to perform specific actions at stages in the Pod lifecycle, such as <code>PostStart</code> or <code>PreStop</code>.</li> <li>Logging and Monitoring: Responsible for collecting and forwarding logs to a central log database. Also, it monitors the state of the Pod and containers, sending regular updates to the master node.</li> </ul>"},{"location":"core-concepts/kubelet/#practical-example-monitoring-kubelet-activity","title":"Practical Example: Monitoring Kubelet Activity","text":"<p>Monitoring the health and performance of Kubelet is crucial for maintaining cluster stability. Here are some metrics you might consider monitoring:</p> <ul> <li>Container start latency: Measures the time it takes for a container to start after being scheduled.</li> <li>Number of running Pods: Tracks the number of Pods that are successfully running on the node.</li> <li>Resource usage: Monitors CPU, memory, and disk utilization to ensure they remain within expected limits.</li> </ul> <p>You can use tools like Prometheus or Kubernetes' built-in metrics pipeline to gather and visualize these metrics.</p>"},{"location":"core-concepts/kubelet/#kubelet-configuration","title":"Kubelet Configuration","text":"<p>Configuring Kubelet properly is essential for the smooth operation of a Kubernetes node. Some of the key configuration options include:</p> <ul> <li>PodCIDR: Specifies the CIDR range that the Pods in this node are part of.</li> <li>MaxPods: Limits the number of Pods that Kubelet can run.</li> <li>NodeStatusUpdateFrequency: Determines how often Kubelet posts the node status to the API Server.</li> </ul> <p>By understanding and managing Kubelet effectively, administrators can ensure robust and efficient operation of their Kubernetes clusters.</p>"},{"location":"core-concepts/kubernetes-api/","title":"API Server","text":""},{"location":"core-concepts/kubernetes-api/#overview","title":"Overview","text":"<p>The Kubernetes API serves as the central nervous system of the platform, orchestrating all interactions within the system. Every operation within Kubernetes, from creating and reading to updating and deleting resources such as Pods and Services, is conducted through requests to the API, which are then processed by the API server. </p> <p>While <code>kubectl</code> is the go-to command-line tool for sending these requests, they can also be composed programmatically or via specialized API development tools. Regardless of how they're formulated, all requests are funneled to the API server, where they undergo authentication and authorization checks. Once verified, these requests are actioned within the cluster. For instance, a request to create a resource results in its deployment to the cluster, and the object's configuration is then stored in the cluster's datastore. This API-centric design ensures a consistent and secure method for managing the cluster's state and operations.  </p> <p>Communication within Kubernetes relies on serialization, the process of converting objects like Pods and Services into JSON strings for HTTP transmission. This conversion is bidirectional: clients, including kubectl, serialize objects into JSON to make requests, and the API server does likewise when responding. Moreover, Kubernetes stores these serialized states in persistent storage, typically etcd, ensuring the cluster\u2019s state is both maintained and recoverable.  </p> <p>Serialization in Kubernetes isn't limited to JSON; it also embraces Protobuf, a schema known for its speed and efficiency, outpacing JSON in performance and scalability. However, Protobuf's complexity makes it less accessible for direct inspection and debugging, which is why it's primarily utilized for internal communications within the cluster, while JSON remains the go-to format for external client interactions.  </p> <p>To smooth out the serialization process, clients specify their supported formats using the Content-Type header in their HTTP requests. For instance, a client that only understands JSON will declare <code>Content-Type: application/json</code>, prompting Kubernetes to respond with data serialized in JSON, adhering to the client's capabilities and preferences.  </p> <p>Kubernetes is a world of API-defined objects, ranging from the familiar Pods and Services to the traffic-managing Ingresses. All these elements are accessible via the API server, which acts as the gateway for interaction with the cluster. You typically use <code>kubectl</code>, the command-line interface, to make requests for these objects. The beauty of Kubernetes extends to its extensibility, allowing third parties to define custom resources that are just as accessible through <code>kubectl</code> and the API server.  </p> <p>When you make a request for an object, the API server springs into action, creating that object within your cluster. But it doesn't just stop there; the API server provides a watch functionality, letting you observe the object as it comes to life. Once the object is up and running, Kubernetes maintains a vigilant watch over it, with the API server offering real-time insight into its current state. Whether you're scaling up with more objects or pruning with deletions, these interactions are all routed through the central hub of the API server.  </p> <pre><code>sequenceDiagram\n    participant user\n    participant kubectl\n    participant api-server\n    participant etcd\n\n    user-&gt;&gt;kubectl: Request Object Creation\n    kubectl-&gt;&gt;api-server: Create Object\n    api-server-&gt;&gt;etcd: Serialize and Persist Object\n    etcd--&gt;&gt;api-server: Confirm Object Stored\n    api-server--&gt;&gt;kubectl: Object Creation Watch\n    kubectl--&gt;&gt;user: Object Status Updates\n    Note over user,etcd: Object is now ready for use\n\n    user-&gt;&gt;kubectl: Query Object State\n    kubectl-&gt;&gt;api-server: Get Object State\n    api-server-&gt;&gt;etcd: Retrieve Object Data\n    etcd--&gt;&gt;api-server: Object Data\n    api-server--&gt;&gt;kubectl: Object State\n    kubectl--&gt;&gt;user: Object State Response</code></pre>"},{"location":"core-concepts/kubernetes-api/#api-server","title":"API Server","text":"<p>The Kubernetes API server is the central hub through which all interactions in the cluster are routed, functioning as the front-end interface for Kubernetes' API. Picture it as the Grand Central Station of Kubernetes \u2014 every command, status update, and inter-service communication passes through the API server via RESTful calls over HTTPS. Here's a snapshot of how it operates:  </p> <ul> <li><code>kubectl</code> commands are directed to the API server, whether it's for creating, retrieving, updating, or deleting Kubernetes objects.</li> <li>Node Kubelets keep an eye on the API server, picking up new tasks and sending back their statuses.</li> <li>The control plane services don't chat amongst themselves directly; they communicate through the API server.  </li> </ul> <p>Zooming in on the API server itself, it's part of the Kubernetes control plane services, often running as a Pod set within the kube-system Namespace on the control plane nodes. For those managing their own Kubernetes clusters, ensuring the high availability and robust performance of the control plane is crucial to keep the API server operational. In contrast, for hosted Kubernetes services, these details are abstracted away from the user.  </p> <p>At its core, the API server's role is to make the Kubernetes API accessible, both to clients within the cluster and to those outside. It secures client connections with TLS encryption and applies various authentication and authorization protocols to vet and process only legitimate requests. All requests, no matter their origin, are subject to the same stringent auth checks.  </p> <p>The \"RESTful\" part of the API means it adheres to a modern web API structure that deals with CRUD-style (Create, Read, Update, Delete) requests via standard HTTP methods like <code>POST</code>, <code>GET</code>, <code>PUT</code>, <code>PATCH</code>, and <code>DELETE</code>.  </p> <p>Typically, the API server is available on ports 443 or 6443, although these can be configured to suit specific needs. The flexibility of the API server ensures that it can cater to different environments while maintaining strict security and reliable service.  </p> <p>The following command will show you the address and port your Kubernetes cluster is exposed on: $ kubectl cluster-info<pre><code>    Kubernetes control plane is running at https://192.168.1.105:6443\n    CoreDNS is running at https://192.168.1.105:6443/api/v1/namespaces/...\n    Metrics-server is running at https://192.168.1.105:6443/api/v1/namespaces/...\n</code></pre></p> <p>In essence, the Kubernetes API server serves as the gateway to the cluster, offering a secure, RESTful interface for interacting with the cluster's state. Operating from the control plane, it necessitates robust availability and performance to ensure swift and reliable handling of requests, embodying the critical link between the user's commands and the cluster's operational response.</p> <p>Note</p> <p>The API Server is the only component in Kubernetes that interacts directly with etcd.</p> <p>If you're unfamiliar with REST, AWS has a great one-pager to get you up to speed.  </p>"},{"location":"core-concepts/kubernetes-api/#api","title":"API","text":"<p>The Kubernetes API is expansive and RESTful, structured to define all Kubernetes resources. Initially, the API was a single, monolithic entity, but as Kubernetes evolved, it transitioned into a more modular form for better manageability, distinguishing between the core group and named groups of API resources.  </p> <p>Core API Group: This group houses the original, fundamental resources such as Pods, Nodes, and Services, accessible under <code>/api/v1</code>. These objects, crucial from the early Kubernetes days, have paths that may vary based on whether they are namespaced (e.g., Pods within a specific namespace) or cluster-wide (e.g., Nodes).  </p> <p>Named Groups: Representing the evolution and expansion of the Kubernetes API, named groups contain newer resources organized by functionality. For instance, the \"apps\" group includes workload-related resources like Deployments and StatefulSets, while \"networking.k8s.io\" focuses on network aspects such as Ingresses. Unlike the core group, resources in named groups are found under <code>/apis/{group-name}/{version}/</code>, reflecting their categorization and versioning.  </p> <p>This division enhances the API's scalability and navigability, facilitating the introduction of new resources. To explore available resources and their groupings, <code>kubectl api-resources</code> provides a comprehensive overview, indicating whether resources are namespaced or cluster-scoped, alongside their shortnames and API group affiliations. This command is instrumental in understanding the API's layout and the scope of resources within a Kubernetes cluster.  </p>"},{"location":"core-concepts/kubernetes-api/#core-group","title":"Core Group","text":"Resource REST Path Pods <code>/api/v1/namespaces{namespace}/pods/</code> Services <code>/api/v1/namespaces/{namespace}services</code> Nodes <code>/api/v1/nodes/</code> Namespaces <code>/api/v1/namespaces</code>"},{"location":"core-concepts/kubernetes-api/#named-groups","title":"Named Groups","text":"Resource REST Path Ingress <code>/apis/networking.k8s.io/v1/namespaces/{namespace}/ingresses/</code> RoleBinding <code>/apis/rbac.authorization.k8s.io/v1/namespaces/{namespace}/rolebindings/</code> ClusterRole <code>/apis/rbac.authorization.k8s.io/v1/clusterroles/</code> StorageClass <code>/apis/storage.k8s.io/v1/storageclasses/</code> <p>Note: This is not all-inclusive, just a few examples.</p> <p>The <code>kubectl api-resources</code> command is a great way to see which API resources are available on your cluster, as well as useful information about them:  </p> $ kubectl api-resources<pre><code>NAME                              SHORTNAMES   APIVERSION     NAMESPACED\nbindings                                       v1             true\ncomponentstatuses                 cs           v1             false\nComponentStatus\nconfigmaps                        cm           v1             true\nConfigMap\nendpoints                         ep           v1             true\nEndpoints\nevents                            ev           v1             true\nlimitranges                       limits       v1             true\nLimitRange\nnamespaces                        ns           v1             false\nNamespace\nnodes                             no           v1             false\npersistentvolumeclaims            pvc          v1             true\nPersistentVolumeClaim\npersistentvolumes                 pv           v1             false\nPersistentVolume\n</code></pre> <p>Truncated for brevity</p> <p>In Kubernetes discussions, you might hear \"resources,\" \"objects,\" and \"primitives\" used as if they're the same. While common usage often blends these terms together, there's a technical distinction worth noting: Kubernetes fundamentally operates on a resource-based API model.  </p> <p>What this means: At its core, the Kubernetes API deals with \"resources.\" These resources are predominantly \"objects\" like Pods, Services, and Ingresses. Yet, the API isn't limited to objects alone; it also encompasses lists and a select few operations. Given that the bulk of resources are indeed objects, the terms \"resource\" and \"object\" are frequently used interchangeably without causing confusion.  </p> <p>Scope of Resources: Kubernetes differentiates between namespaced and cluster-scoped resources. Namespaced resources must reside within a specific Namespace, tailoring their scope and impact to that Namespace. For instance, Pods and Services require a Namespace to exist. Conversely, cluster-scoped resources either span multiple Namespaces or operate outside the Namespace system altogether. Nodes, for example, are cluster-scoped resources existing beyond Namespace boundaries, while ClusterRoles can be tied to specific Namespaces through RoleBindings to apply permissions across the cluster.  </p> <p>To get a grip on the resources available in your cluster and their scope, <code>kubectl api-resources</code> is an invaluable command. It provides a snapshot of all resources, highlighting whether they are namespaced or cluster-scoped, thereby offering insight into how Kubernetes structures and manages its diverse set of resources.  </p>"},{"location":"core-concepts/kubernetes-api/#extending-the-api","title":"Extending the API","text":"<p>Kubernetes offers a powerful framework for managing and automating containerized applications, largely through its predefined set of resources and controllers that observe and manage the state of objects within the cluster. Yet, one of Kubernetes' most compelling features is its extensibility, allowing you to tailor the system to your specific needs by introducing custom resources and controllers.  </p> <p>Extending Kubernetes with Custom Resources and Controllers A vivid example of such extensibility can be observed in the storage domain, where third-party vendors integrate advanced functionalities\u2014like snapshot scheduling\u2014directly into Kubernetes through custom resources. While Kubernetes natively supports storage operations through StorageClasses and PersistentVolumeClaims, these custom resources enable the exposure of vendor-specific features within the same Kubernetes ecosystem.</p> <p>The Extension Blueprint Extending the Kubernetes API usually involves:</p> <ol> <li> <p>Creating a Custom Controller: Develop a controller that uses your custom logic to monitor changes to your resources, ensuring the cluster achieves and maintains the desired state.</p> </li> <li> <p>Defining a Custom Resource: Utilize Kubernetes' CustomResourceDefinition (CRD) API object to create new resource types. CRDs integrate seamlessly with the Kubernetes API and include their own RESTful paths. Once established, manage these resources with kubectl as you would with standard resources, thus maintaining a consistent Kubernetes experience.</p> </li> </ol> <p>This approach not only enriches the Kubernetes ecosystem with new functionalities but also maintains the uniformity and coherence of the Kubernetes API, ensuring that custom resources are as accessible and manageable as the built-in ones. Through CRDs, Kubernetes embraces an extendable architecture, empowering developers to innovate and expand the platform's capabilities to meet their unique operational requirements.  </p> <p>Info</p> <p>Creating a custom resource doesn't do a whole lot unless you create a custom controller to accompany it. If you're interested in digging into those details, I recommend reading the official Kubernetes documentation on custom controllers.  </p>"},{"location":"core-concepts/namespaces/","title":"Namespaces","text":""},{"location":"core-concepts/namespaces/#overview","title":"Overview","text":"<p>Namespaces in Kubernetes are like virtual clusters within a physical cluster. They help partition resources among multiple users and teams, enabling a more organized deployment environment. Namespaces provide a mechanism to scope resource names, simplify resource management, and facilitate the application of specific access controls, resource quotas, and policies at a granular level. </p> <p>Namespaces are not intended to be used for secure isolation</p> <p>While namespaces help in organizing and managing access to resources, they do not provide a strong security boundary capable of preventing all forms of resource access or interference between namespaces. For scenarios requiring stringent security isolation, such as multi-tenant environments with strict compliance requirements, the best practice is to use multiple Kubernetes clusters. </p>"},{"location":"core-concepts/namespaces/#common-uses-for-namespaces","title":"Common Uses for Namespaces","text":"<p>Namespaces are commonly used to separate environments within a single Kubernetes cluster. This separation helps in managing different deployment stages such as development, testing, staging, and production within the same cluster but without interference. Each environment can have its own set of permissions, limits, and policies, ensuring that resources are not accidentally shared or overwritten across different team functions. </p>"},{"location":"core-concepts/namespaces/#network-and-namespace-policies","title":"Network and Namespace Policies","text":"<p>Network policies in Kubernetes provide an additional layer of security when using namespaces. By default, pods within a namespace can communicate freely. Network policies allow you to define rules for inbound and outbound traffic between pods across different namespaces, helping to enforce a stricter communication policy and prevent unauthorized access.</p> <p>For example, you can create a network policy that only allows traffic from the 'development' namespace to the 'testing' namespace, enhancing the security and isolation between different deployment stages.</p>"},{"location":"core-concepts/namespaces/#built-in-namespaces","title":"Built-in Namespaces","text":"<p>Kubernetes starts with several built-in Namespaces:</p> <ul> <li><code>default</code>: The space where objects are placed if no other Namespace is specified.</li> <li><code>kube-system</code>: For objects created by the Kubernetes system.</li> <li><code>kube-public</code>: Usually reserved for resources that should be visible and readable publicly throughout the whole cluster.</li> <li><code>kube-node-lease</code>: For lease objects associated with nodes which help the Kubelet in determining node health.  </li> </ul> <p>To view all the namespaces in your cluster and verify their status, you can use the <code>kubectl get namespaces</code> command. This command lists all namespaces, providing a quick overview of the operational scope within your cluster: </p> $ kubectl get namespaces<pre><code>    NAME              STATUS   AGE\n    default           Active   22h\n    gmp-public        Active   22h\n    gmp-system        Active   22h\n    kube-node-lease   Active   22h\n    kube-public       Active   22h\n    kube-system       Active   22h\n</code></pre> <p>Your output will vary based on your environment.  </p>"},{"location":"core-concepts/namespaces/#deploying-objects-to-namespaces","title":"Deploying Objects to Namespaces","text":"<p>When deploying objects on Kubernetes you can specify the target Namespace imperatively by adding the <code>-n &lt;Namespace&gt;</code> flag to your command, or declaratively by specifying the Namespace in your YAML file:  </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: my-namespace\n...\n</code></pre>"},{"location":"core-concepts/namespaces/#namespace-creation","title":"Namespace Creation","text":"<p>Creating a new Namespace is as simple as applying a new YAML file:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-new-namespace\n</code></pre> <p>You can also create a Namespace with the kubectl command:</p> <pre><code>kubectl create namespace my-new-namespace\n</code></pre>"},{"location":"core-concepts/overview/","title":"Overview","text":"<p>We'll dive into many of these topics in greater detail later on, but for now here is a primer on Kubernetes to get you started.  </p>"},{"location":"core-concepts/overview/#history","title":"History","text":"<p>Kubernetes itself was born out of Google's experience running billions of containers at scale and managing them with proprietary systems called Borg and Omega. In 2014 Google donated Kubernetes as an open-source project to the Cloud Native Computing Foundation (CNCF).</p>"},{"location":"core-concepts/overview/#20k-foot-view","title":"20K-foot View","text":"<p>At a high-level, Kubernetes is responsible for deploying your applications and dynamically responding to changes to keep your applications running how you intended. Kubernetes runs on any cloud or on-premise datacenter, abstracting away all of the underlying infrastructure and letting you focus on application development. All applications running on Kubernetes must be containerized, and those containers must be running inside of a Pod.  </p> <p>Fundamentally, Kubernetes is a cluster - a group of machines, so to speak. These machines are called nodes in the Kubernetes world and can be cloud instances, virtual machines, physical servers, your laptop, etc.  Cluster nodes can be lumped into one of two categories: Control Plane or worker nodes.</p> Control Plane and Worker Nodes <p>The Control Plane is the brain of the Kubernetes cluster. It makes global decisions about the cluster (e.g., scheduling), detects and responds to cluster events. The main components of the Control Plane include:</p> <ul> <li>API Server: The hub for all communication.</li> <li>Scheduler: Assigns your apps to run on various nodes.</li> <li>Controller Manager: Oversees a number of smaller controllers that perform actions like replicating pods and handling node operations.</li> </ul> <p>Worker nodes are the \"muscles\" of the cluster where your applications actually run. Each worker node has the following components installed:</p> <ul> <li>Kubelet: Ensures that containers are running in a Pod.</li> <li>Kube-proxy: Maintains network rules.</li> </ul> <p>Control PlaneControl PlaneWorker NodeWorker NodeAPI ServerAPI ServeretcdetcdSchedulerSchedulerController ManagerController ManagerKubeletKubeletKube ProxyKube ProxyContainer RuntimeContainer Runtime</p> <p>Understanding these components and their interactions is crucial for effectively managing and troubleshooting a Kubernetes cluster.</p>"},{"location":"core-concepts/overview/#api-server","title":"API Server","text":"<p>Speaking of, the API Server is the central component for all communication for all components in Kubernetes.  </p> <p>Any communication inbound or outbound to/from the Kubernetes cluster must be routed through the API Server.</p>"},{"location":"core-concepts/overview/#etcd","title":"etcd","text":"<p>The Control Plane, like many aspects of Kubernetes, exists in a stateless manner. However, etcd does not - it persistently stores the state of the cluster and other configuration data. </p> <p>etcd plays a critical role in Kubernetes by persistently storing the state of the cluster and other configuration data. For high-availability setups  </p> <p>etcd is not merely installed on every control plane node; it is commonly configured externally or runs in a clustered mode across several nodes. This configuration helps avoid single points of failure and ensures that the cluster remains operational even if one or more etcd instances fail. It's important to note that while etcd can handle network partitions or \"split-brain\" scenarios by preventing state updates, it will still allow the running applications to operate, thus ensuring service continuity. </p> <p>Every result you see when you a run <code>kubectl get</code> command is actually data returned from etcd (via the API Server).</p>"},{"location":"core-concepts/overview/#controllers","title":"Controllers","text":"<p>In Kubernetes, controllers are fundamental components that act as watchful guardians of the cluster's desired state. They are essentially background loops monitoring for deviations in the cluster and invoking corrective measures. These controllers include:</p> <ul> <li>ReplicationController: Ensures the specified number of pod replicas are running at any given time.</li> <li>DeploymentController: Manages the deployment process by updating applications and rolling out new versions seamlessly and safely.</li> </ul> <p>All these controllers are orchestrated by a higher-level component known as the Controller Manager. This manager oversees various controllers' activities, ensuring that if the current state doesn't match the desired state, appropriate actions are taken to reconcile the two.</p> <p>The following logic is at the core of what Kubernetes is and how it works:  </p> <p>apietcdstore desiredstatestore desired...get currentstateget current...NONOcurrent state =desired state?current state...take actiontake actionYESYESkeepcheckingkeep...</p>"},{"location":"core-concepts/overview/#declarative-model","title":"Declarative Model","text":"<p>Key to truly mastering Kubernetes is the concept of the declarative model. You tell Kubernetes how you want your application to look and run (how many replicas, which image to use, network settings, commands to run, how to perform updates, etc.), and it's Kubernetes job to ensure that happens. You \"tell\" Kubernetes through the use of manifest files written in YAML.  </p> <p>You take those manifest files and <code>POST</code> them to the Kubernetes API Server (typically through the use of <code>kubectl</code> commands). The API Server will then authenticate the request, inspect the manifest for formatting, route the request to the appropriate controller (i.e. if you've defined a manifest file for a Deployment, it will send the request to the Deployments controller), and then it will record your desired state in the cluster store (remember, etcd). After this, the relevant controller will get started on performing any tasks necessary to get your application into its desired state.  </p> <p>After your application is up and running, controllers begin monitoring its state in the background and ensuring it matches the desired state in etcd (see simple logic diagram above).</p>"},{"location":"core-concepts/pods/","title":"Pods","text":"<p>Pods are the atomic unit of scheduling in Kubernetes. As virtual machines were in the VMware world, so are Pods in the world of Kubernetes. Every container running on Kubernetes must be wrapped up in a Pod. Think of a Pod as a wrapper for your application\u2019s container(s), similar to how a virtual machine encapsulates an entire operating system and its applications.  </p> <p>The most simple implementation of this are single-container Pods - one container inside one Pod. However there are certain instances where multi-container Pods make sense.</p> <p>It's important to note that when you scale up/down applications in Kubernetes, you're not doing so by adding/removing containers directly - you do so by adding/removing Pods.</p>"},{"location":"core-concepts/pods/#atomic","title":"Atomic","text":"<p>Pods are atomic units in the sense that they are deployed, scaled, and terminated together. This makes them a fundamental component for deploying applications in Kubernetes:</p> <ul> <li>Single-container Pods: The simplest form, containing only one container.</li> <li>Multi-container Pods: These contain multiple containers that need to work closely together.</li> </ul>"},{"location":"core-concepts/pods/#lifecycle","title":"Lifecycle","text":"<p>Pods are designed to be ephemeral in nature. Once a Pod dies, it's not meant to be restarted or revived. Instead, the intent is to spin up a brand new Pod in the failed ones place (based off of your defined manifest). Further, Pods are immutable and should not be changed once running. If you need to change your application, you update the configuration via the manifest and deploy a new Pod.  </p> <p>Pods also follow a defined restart policy in order to handle container failures:  </p> <ul> <li><code>Always</code>: The container is restarted even if it exits successfully.</li> <li><code>OnFailure</code>: The container is only restarted if it exits with an error.</li> <li><code>Never</code>: The container is never restarted, regardless of the exit status.</li> </ul>"},{"location":"core-concepts/pods/#shared-resources-and-communication","title":"Shared Resources and Communication","text":"<p>Containers within a Pod share networking and volumes:</p> <ul> <li>Networking: Containers in a Pod share the same IP address and port space.</li> <li>Volumes: Allows sharing of storage between containers within a Pod.</li> </ul>"},{"location":"core-concepts/pods/#multi-container-pods","title":"Multi-container Pods","text":"<p>As mentioned above, the simplest way to run an app on Kubernetes is to run a single container inside of a single Pod. However, in situations where you need to tightly couple two or more functions you can co-locate multiple containers inside of the same pod. One such example would be leveraging the sidecar pattern for logging wherein the main container dumps logs to a supporting container that can sanitize and format the logs for consumption. This frees up the main container from having to worry about formatting logs.  </p>"},{"location":"core-concepts/pods/#affinity-and-anti-affinity","title":"Affinity and Anti-affinity","text":"<p>Kubernetes offers ways to control where Pods are placed relative to other Pods or to specific nodes. Pod affinity rules attract Pods to nodes with specific labels, while anti-affinity rules repel them, ensuring high availability and optimal resource utilization.</p>"},{"location":"core-concepts/pods/#resource-requests-and-limits","title":"Resource Requests and Limits","text":"<p>Pods can specify the amount of CPU and memory required (requests) and the maximum that can be consumed (limits). This helps Kubernetes make better scheduling decisions and manage system resources efficiently.</p>"},{"location":"core-concepts/pods/#probes-readiness-and-liveness","title":"Probes: Readiness and Liveness","text":"<p>Kubernetes uses probes to manage the lifecycle of containers within Pods:</p> <ul> <li>Readiness Probes: Ensure traffic is not sent to a Pod until it is ready to handle it.</li> <li>Liveness Probes: Help to keep applications healthy by restarting containers that fail the defined checks.</li> </ul>"},{"location":"core-concepts/pods/#init-containers","title":"Init Containers","text":"<p>Init containers run before the application containers and are used to perform setup tasks or wait for some condition before the app starts. They run to completion and must exit before the main application containers start.</p>"},{"location":"core-concepts/pods/#quality-of-service-qos-classes","title":"Quality of Service (QoS) Classes","text":"<p>Pods are assigned QoS classes based on their resource requests and limits:</p> <ul> <li><code>Guaranteed</code>: Pods with defined and equal requests and limits, ensuring the highest priority.</li> <li><code>Burstable</code>: Pods with defined requests lower than limits, giving some flexibility.</li> <li><code>BestEffort</code>: Pods with no requests or limits, receiving the lowest priority.</li> </ul>"},{"location":"core-concepts/pods/#pod-disruption-budgets","title":"Pod Disruption Budgets","text":"<p>Pod Disruption Budgets (PDBs) allow you to ensure that a minimum number of Pods are always available during voluntary disruptions, such as node maintenance, safeguarding against outages.</p>"},{"location":"core-concepts/pods/#annotations-and-labels","title":"Annotations and Labels","text":"<p>Labels are key/value pairs for organizing and selecting groups of Pods, while annotations provide a way to store additional metadata to help manage applications.</p>"},{"location":"core-concepts/pods/#service-accounts","title":"Service Accounts","text":"<p>Pods use service accounts to authenticate to the Kubernetes API, which is crucial for Pods that need to interact with the API for automation and orchestration.</p>"},{"location":"core-concepts/pods/#summary","title":"Summary","text":"<p>Pods are the building blocks of a Kubernetes application. They ensure that containers run in a controlled, isolated, and secure environment with all the necessary configurations and resources. While Pods are inherently transient, their patterns and behaviors are foundational to how applications are designed and managed in Kubernetes. For more granular control and advanced features, refer to the official Kubernetes documentation.</p>"},{"location":"core-concepts/scheduler/","title":"Scheduler","text":""},{"location":"core-concepts/scheduler/#kubernetes-scheduler-overview","title":"Kubernetes Scheduler Overview","text":"<p>The Scheduler in Kubernetes is a critical component responsible for assigning Pods to Nodes in the cluster. While the Scheduler decides the placement of Pods, it is the Kubelet on each node that actually places the Pod onto the Node.</p> Responsibilities of the Scheduler <p>The primary function of the Scheduler is to: - Evaluate the resource requirements of each Pod. - Assess the current availability and conditions of the Nodes. - Assign the Pod to a Node that best meets the Pod\u2019s needs.</p> Decision-Making Criteria <p>The Scheduler uses a variety of criteria to determine the most suitable Node for a Pod, including:</p> <ul> <li>Resource Requirements: Checks if nodes have the CPU, memory, and storage to meet the Pod's requirements.</li> <li>Application Requirements: Considers specific demands such as GPU needs or data locality.</li> <li>Quality of Service (QoS): Prioritizes Pods based on their QoS class (Guaranteed, Burstable, BestEffort).</li> </ul> Scheduling Algorithm <p>The Scheduler employs a two-phase process consisting of filtering and scoring:</p> <ol> <li>Filtering Phase: The Scheduler filters out nodes that do not satisfy the Pod\u2019s requirements. Factors considered include:</li> <li>Node capacity.</li> <li>Resource availability.</li> <li>Taints and tolerations.</li> <li> <p>Affinity and anti-affinity specifications.</p> </li> <li> <p>Scoring Phase: Nodes that pass the filtering phase are scored on a scale from 0 to 10 based on how well they meet the requirements. The Scheduler considers:</p> </li> <li>Resource utilization and availability.</li> <li>Affinity rules specified in the Pod configuration.</li> <li>Other custom policies that may be configured.</li> </ol> <p>The node with the highest score is selected for the Pod placement. If multiple nodes have the same score, the Scheduler selects one at random.</p> Advanced Scheduling <p>Advanced scheduling features allow for more complex decision-making:</p> <ul> <li>Node Affinity/Anti-Affinity: Allows specifying rules that a node must or must not meet to host a Pod.</li> <li>Pod Affinity/Anti-Affinity: Enables placing Pods in proximity to other Pods to meet various operational requirements.</li> <li>Taints and Tolerations: Nodes can be \"tainted\" to repel Pods unless they \"tolerate\" the taint.</li> </ul>"},{"location":"core-concepts/scheduler/#practical-example-using-node-affinity","title":"Practical Example: Using Node Affinity","text":"<p>To leverage node affinity, define it in your Pod spec. For instance, to schedule a Pod on a node with a specific feature (like SSDs), your Pod spec might include:</p> <pre><code>...\naffinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n        - matchExpressions:\n            - key: kubernetes.io/ebs-volume\n              operator: In\n              values:\n                - ssd\n</code></pre>"},{"location":"core-concepts/scheduling/","title":"Scheduling","text":"<ul> <li>Every Pod specification includes an optional <code>nodeName</code> field, which is typically left unset. When a Pod is created without a specific <code>nodeName</code>, Kubernetes' scheduler automatically assigns the Pod to an appropriate node, populating the <code>nodeName</code> field with the selected node's name. </li> <li>When the Scheduler reviews Pods, it finds Pods that do not have that field set and those are the candidates for scheduling</li> <li>It then runs the scheduling algorithm to determine which Node to place the Pod on and populates the <code>nodeName</code> field</li> </ul>"},{"location":"core-concepts/scheduling/#manual-scheduling","title":"Manual Scheduling","text":"<ul> <li>You can manually set the specific Node in your Pod definition:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: nginx\n  nodeName: k8s02 # you can specify the Node you want here\n</code></pre> <p>The <code>nodeName</code> field cannot be updated once the Pod is running. If you want to update the Node a running Pod resides on, you have to create a <code>Binding</code> object and send a POST request to the Binding API of the Pod,</p>"},{"location":"core-concepts/scheduling/#taints-and-tolerations","title":"Taints and Tolerations","text":"<p>Taints and tolerations work together to ensure that Pods are not scheduled onto inappropriate nodes. Taints are applied to nodes and can repel Pods that do not have a corresponding toleration. </p> <ul> <li>Taints are markers applied to nodes that denote special behavior. For example, a node might have a taint that indicates it should not host any Pods that do not explicitly tolerate the taint. </li> <li>Tolerations are applied to Pods and allow them to \"ignore\" taints on nodes, thus enabling them to be scheduled onto nodes with those taints.</li> </ul> <p>Together, taints and tolerations provide a powerful way to ensure that Pods are only scheduled onto suitable nodes. For instance, you might taint a node to only allow database Pods that require high I/O performance, ensuring that other workloads do not disrupt database operations. </p> <ul> <li>Taints are specific to Nodes</li> <li>Tolerations are specific to Pods</li> <li>Taints on a Node say \u201conly these certain Pods can be scheduled here\u201d</li> <li>Tolerations on a Pod say \u201cyou can tolerate a Node that has this given taint\u201d</li> </ul>"},{"location":"core-concepts/scheduling/#node-selectors","title":"Node Selectors","text":"<ul> <li>Node selectors specify which Node you want a Pod to run on based on labels you place on the Node itself<ul> <li>i.e. if you label a Pod as <code>disktype=ssd</code>, implying it can only be scheduled on Nodes with that label, you can specify in the Pod definition YAML:</li> </ul> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  nodeSelector:\n    disktype: ssd # label here\n</code></pre> <p>Note there are limitations here as you cannot use complex operators like <code>IS NOT</code> , <code>OR</code> , <code>EXISTS</code> , etc.</p>"},{"location":"core-concepts/scheduling/#node-affinity","title":"Node Affinity","text":"<p>Node affinity is a set of rules that the scheduler uses to determine where a Pod can be placed based on labels on nodes. Unlike simple node selectors, node affinity allows for more complex expressions:</p> <ul> <li>Required rules must be met for a Pod to be scheduled on a node.</li> <li>Preferred rules suggest preferences but do not strictly enforce them.</li> </ul> <p>This feature allows you to specify that certain Pods should or should not be placed in specific ways relative to other Pods or based on node attributes, enhancing the scheduler's ability to distribute workloads optimally. </p> <ul> <li>Node affinity does let you use complex operators</li> <li>Node affinity is\u00a0a set of rules used by the Scheduler to determine where a Pod can be placed.</li> <li>The rules are defined using custom labels on Nodes and label selectors specified in Pods.</li> <li>Node affinity allows a Pod to specify an affinity (or anti-affinity) towards a group of Nodes it can be placed on.</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - ssd\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"core-concepts/scheduling/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Among other things, the Scheduler takes into account the resources required by the Pod and the resources available on the Node(s) when attempting to schedule a Pod</li> <li>You can specify how much CPU and memory to request for your Pod when defining it<ul> <li>This is called a Resource Request</li> </ul> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp-color\n  labels:\n    name: simple-webapp-color\nspec:\n  containers:\n  - name: simple-webapp-color\n    image: nginx\n    ports:\n      - containerPort: 8080\n    resources:\n      requests:\n        memory: \"4Gi\"\n        cpu: 2\n</code></pre> <ul> <li>By default, a Pod has no limits on the amount of resources it can consume from a Node</li> <li>You can also specify a limit how many resources your Pod can consume when defining it</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp-color\n  labels:\n    name: simple-webapp-color\nspec:\n  containers:\n  - name: simple-webapp-color\n    image: nginx\n    ports:\n      - containerPort: 8080\n    resources:\n      requests:\n        memory: \"2Gi\"\n        cpu: 4\n      limits:\n        memory: \"8Gi\"\n        cpu: 10\n</code></pre> <ul> <li>When a Pod tries to go beyond the CPU limit, the Node will throttle the CPU usage of the Pod</li> <li> <p>A Pod can use more memory than it\u2019s limit however and will throw an OOM (Out of Memory) error if it happens frequently</p> </li> <li> <p>To ensure all Pods have some sort of limit set, you can introduce a LimitRange, which ensures all created Pods within a Namespace have certain default values without having to set them at the Pod definition-level</p> </li> </ul> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-resource-constraint\nspec:\n  limits:\n  - default: # limit\n      cpu: 500m\n    defaultRequest: # request\n      cpu: 500m\n    max: # limit\n      cpu: \"1\"\n    min: # request\n      cpu: 100m\n    type: Container\n</code></pre> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\nname: memory-resource-constraint\nspec:\nlimits:\n- default: # limit\n  memory: 1Gi\ndefaultRequest: # request\n  memory: 1Gi\nmax: # limit\n  memory: 1Gi\nmin: # request\n  memory: 500Mi\ntype: Container\n</code></pre> <p>Changing a LimitRange will not affect running Pods - only newly created ones</p> <ul> <li>To set limits at a Namespace-level, you use Resource Quotas</li> </ul> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: my-resource-quota\nspec:\n  hard:\n    requests.cpu: \"4\"\n    requests.memory: 4Gi\n    limits.cpu: \"10\"\n    limits.memory: 10Gi\n</code></pre>"},{"location":"core-concepts/scheduling/#daemonsets","title":"DaemonSets","text":"<ul> <li> <p>DaemonSets run a single copy of each Pod on every Node of the cluster</p> <ul> <li>If a new Node is added to the cluster, a new Pod is put onto the new Node</li> </ul> </li> <li> <p>Use cases might be:</p> <ul> <li>Monitoring agent</li> <li>Logging agent</li> <li>Networking solutions which require an agent on every Node</li> </ul> </li> <li> <p>The <code>kube-proxy</code> is actually a DaemonSet as well</p> </li> <li> <p>DaemonSets ensures a Pod runs on every single Node in the cluster by using Node Affinity rules</p> </li> </ul> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetdata:\n  name: monitoring-daemon\nspec:\n  selector:\n    matchLabels:\n      app: monitoring-agent\n  template:\n    metadata:\n      labels:\n        app: monitoring-agent\n     spec:\n       containers:\n       - name: monitoring-agent-container\n         image: monitoring-agent-software\n</code></pre>"},{"location":"core-concepts/scheduling/#multiple-schedulers","title":"Multiple Schedulers","text":"<p>Kubernetes allows the deployment of multiple scheduler instances to handle Pod placement according to various customized scheduling policies. You can specify which scheduler to use for each Pod if you have special scheduling needs that the default scheduler does not support.</p> <p>For example, you might deploy a custom scheduler that optimizes for GPU-intensive applications and use it alongside the default Kubernetes scheduler. </p> <ul> <li>You can write your own Scheduler program and deploy it as the default scheduler or supplemental schedulers</li> <li>When defining and deploying a Pod, you can instruct it to leverage a specific Scheduler</li> </ul> <pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- schedulerName: my-scheduler\nleaderElection: # if running multiple masters\n  leaderElect: true\n  resourceNamespace: kube-system\n  resourceName: lock-object-my-scheduler\n</code></pre> <ul> <li>You can deploy an additional Scheduler as a Pod</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-custom-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-scheduler\n    - --address=127.0.0.1\n    - --kubeconfig=/etc/kubernetes/scheduler.conf\n    - --config=/etc/kubernetes/my-scheduler-config.yaml\n\n    image: k8s.grc.io/kube-scheduler-amd64:v1.11.3\n    name: kube-scheduler\n</code></pre> <ul> <li> <p>You can also deploy it as a Deployment</p> </li> <li> <p>To configure a Pod to use a custom Scheduler, we define it in the Pod definition:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - image: nginx\n    name: nginx-container\n  schedulerName: my-custom-scheduler\n</code></pre> <ul> <li>To view which Scheduler was used for Pods you can look at events by running <code>kubectl get events -o wide</code></li> </ul>"},{"location":"core-concepts/statefulsets/","title":"StatefulSets","text":""},{"location":"core-concepts/statefulsets/#overview","title":"Overview","text":"<p>StatefulSets are Kubernetes constructs designed to manage stateful applications that require persistent data and identity across pod restarts and deployments. Each pod in a StatefulSet is given a stable and unique network identifier and persistent storage, which remains associated with the pod, even when it is rescheduled to a different node within the cluster. </p> <p>It's easy to compare StatefulSets with Deployments given they are v1 API objects and follow the controller architecture - but there are notable differences. StatefulSets are Kubernetes tools for running and managing applications that need to remember who they are and what they know\u2014think of them like memory keepers for your apps, such as databases that need to recall data after a reboot. Unlike Deployments that are more about stateless apps (think of them as forgetful but easily replaceable), StatefulSets make sure each of their Pods has a consistent name, network identity, and storage, even if they move around in the cluster. This makes StatefulSets perfect for when your app's individual identity and history are crucial for running smoothly.  </p> <p>StatefulSets can guarantee Pod names, volume bindings, and DNS hostnames across reboots - whereas Deployments cannot. Below are two diagrams that illustrate this point:  </p>"},{"location":"core-concepts/statefulsets/#nodepod-failure-with-deployments","title":"Node/Pod Failure with Deployments","text":"<p>deploypodvol10.0.0.510.0.0.5deploypod10.0.0.910.0.0.9Pod/NodefailurePod/Node... </p>"},{"location":"core-concepts/statefulsets/#nodepod-failure-with-statefulsets","title":"Node/Pod Failure with StatefulSets","text":"<p>podvol10.0.0.510.0.0.5pod10.0.0.510.0.0.5Pod/NodefailurePod/Node...stsstsvol</p> <p>Notice how with a Deployment, when a Pod is replaced it comes up with a new name, IP address, and its volume is no longer bound to it. With StatefulSets, the new Pod comes up looking exactly the same as the previous failed one.  </p> <p>Below is a typical YAML file for defining a StatefulSet:  </p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: my-sts\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n    serviceName: \"my-sts\"\n    replicas: 5\n    template: \n      metadata:\n        labels:\n          app: nginx\n      spec:\n        containers:\n        - name: nginx-container\n          image: nginx:latest\n\n...\n</code></pre> <p>In this example, the name of the StatefulSet is <code>my-sts</code> and it defines 5 Pod replicas that will run the latest version of the NGINX image. Once you post this to the API server (via <code>kubectl</code>), the definition will be persisted to the cluster store (<code>etcd</code>), replicas will be assigned to nodes, and the StatefulSet controller will begin monitoring the state of the cluster to ensure observed = desired.  </p>"},{"location":"core-concepts/statefulsets/#naming","title":"Naming","text":"<p>One nice feature of StatefulSets is that all Pods managed by them get a predictable name. The format of the names given to Pods managed by StatefulSets is <code>&lt;StatefulSet name&gt;-&lt;integer&gt;</code>. The integer begins with 0 and increases each time a new Pod from this StatefulSet is deployed. So in the example of <code>my-sts</code> above, the Pods would have the names of <code>my-sts-0</code>, <code>my-sts-1</code>, <code>my-sts-2</code>, <code>my-sts-3</code>, etc.</p> <p>StatefulSet names need to be valid DNS names</p>"},{"location":"core-concepts/statefulsets/#order-of-creationdeletion","title":"Order of Creation/Deletion","text":"<p>StatefulSets in Kubernetes are all about order and precision. They initiate and terminate Pods one at a time, following a strict sequence. This approach guarantees that each Pod is fully operational and ready to handle requests before the next Pod in the sequence is brought to life. Unlike the more free-form Deployments, which may initiate a bunch of Pods at once through a ReplicaSet\u2014potentially tripping over themselves with race conditions\u2014StatefulSets are the thoughtful orchestrators ensuring each Pod gets the attention it needs to start or stop without a rush.  </p>"},{"location":"core-concepts/statefulsets/#deploying-a-statefulset","title":"Deploying a StatefulSet","text":"<p>my-stsmy-stsstspodmy-sts-2my-sts-2podmy-sts-1my-sts-1podmy-sts-0my-sts-0running + ready?running + ready?running + ready?running + ready?</p> <p>StatefulSets in Kubernetes not only ensure a methodical boot-up but also adhere to a careful scaling strategy, both up and down. For instance, when scaling from five to seven replicas, the StatefulSet will sequentially initiate each new Pod and ensure it's fully operational before moving on to the next. Conversely, during scale-down, the StatefulSet will remove Pods starting from the highest index, allowing each to decommission completely before proceeding. This step-by-step approach is critical for applications like databases, where simultaneous termination could lead to data loss.  </p> <p>StatefulSets also offer mechanisms like the <code>terminationGracePeriodSeconds</code> to fine-tune this process, ensuring no data is compromised. Moreover, unlike Deployments which rely on a ReplicaSet for managing replicas, StatefulSet controllers handle scaling and self-healing autonomously, ensuring stateful applications maintain their integrity and data throughout their lifecycle.  </p> <p>Deleting a StatefulSet does not terminate Pods in order</p> <p>If you want to terminate StatefulSet Pods in order, consider scaling to 0 replicas before deleting the StatefulSet.</p>"},{"location":"core-concepts/statefulsets/#update-management-in-statefulsets","title":"Update Management in StatefulSets","text":"<p>StatefulSets provide controlled update mechanisms that allow for fine-tuned management of application deployments. Updates to StatefulSets, unlike Deployments, are handled on a pod-by-pod basis in a sequential order. This ordered update mechanism is crucial for applications like databases where the sequence of pod updates affects the consistency and availability of data.</p> <p>For example, when a StatefulSet is updated, it updates one pod at a time, starting from the last pod to the first, ensuring each pod is correctly running before proceeding to the next. This sequential deployment strategy minimizes the risk of simultaneous outages and maintains data integrity during updates.</p>"},{"location":"core-concepts/statefulsets/#statefulsets-and-volumes","title":"StatefulSets and Volumes","text":"<p>StatefulSets in Kubernetes are intrinsically tied to their volumes, which form an integral part of the Pods' state. Each Pod in a StatefulSet is bound to its distinct volumes, which are created simultaneously with the Pod and bear unique identifiers linking them directly to their respective Pods. Thanks to the Persistent Volume Claim (PVC) system, these volumes enjoy a separate lifecycle from the Pods, ensuring their preservation across Pod failures and deletions. When a StatefulSet Pod is terminated or fails, its volumes remain intact, ready to be reattached to any new Pod that takes its place, even if that Pod spins up on a different node within the cluster.  </p> <p>When scaling down a StatefulSet, the persistent volumes associated with the Pods are not deleted. These volumes remain within the cluster, retaining the data until they are either explicitly deleted or reattached to new Pods during a scale-up operation. This retention ensures that data is not inadvertently lost during scaling operations, providing a reliable method for managing stateful data across Pod lifecycle events. </p>"},{"location":"core-concepts/statefulsets/#handling-failures","title":"Handling Failures","text":"<p>The StatefulSet controller in Kubernetes meticulously monitors the cluster's status, making sure the current state aligns with the intended setup. Consider a StatefulSet with five replicas; if one, say <code>my-sts-4</code>, goes down, the controller promptly replaces it, ensuring it retains the same name and rebinds it to the original volumes. </p> <p>But, complications arise if the failed Pod makes a comeback post-replacement and suddenly you've got twin Pods vying for the same volume. To avoid this, the StatefulSet controller handles failures with extra caution.</p> <p>When it comes to node failures, the controller faces a tricky challenge. If a node goes silent, it's hard to tell if it's down for good or just temporarily unreachable due to network issues, a kubelet crash, or a reboot. Since the controller can't guarantee that a termination command will reach a Pod on an unresponsive node, it hesitates to substitute Pods until it's certain of the node's fate. This precaution means that when a node appears to be down, manual intervention is usually necessary before Kubernetes will venture to replace any Pods that were running on it. This cautious approach ensures data integrity at the cost of requiring human oversight in ambiguous failure scenarios.</p>"},{"location":"core-concepts/statefulsets/#dns","title":"DNS","text":"<p>StatefulSets cater to applications that demand reliability and persistence in their Pods. This predictability extends to other applications and services that might need to establish direct connections with specific Pods. To enable these direct connections, StatefulSets employ a headless Service, which provides a stable DNS entry for each Pod based on its unique, predictable hostname. As a result, other components within the ecosystem can retrieve the entire roster of Pod hostnames via DNS queries, allowing for precise and direct Pod communications.  </p> <p>Below is a snippet of YAML that shows a headless Service being defined:  </p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  clusterIP: None  # this is the headless Service\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sts-mysql\nspec:\n  serviceName: mysql  # this is the governing Service\n</code></pre> <p>Headless Services in Kubernetes are essentially standard Service objects that lack a dedicated IP address, achieved by setting <code>spec.clusterIP</code> to <code>None</code>. This Service transforms into what is known as a governing Service for a StatefulSet when it is referenced in the StatefulSet's configuration under <code>spec.serviceName</code>.</p> <p>Linking a headless Service to a StatefulSet in this way prompts the Service to create DNS SRV records for each Pod that fits the headless Service's label selector criteria. This setup allows other Pods and applications within the cluster to discover and connect to the StatefulSet's Pods by querying the DNS for the headless Service's name. To leverage this feature, applications will need to be specifically coded to perform such DNS lookups and handle connections to the StatefulSet members dynamically.</p>"},{"location":"core-concepts/statefulsets/#summary","title":"Summary","text":"<p>This section contained a lot of \"in the weeds\" information and probably warrants a quick summary. StatefulSets within Kubernetes serve as a robust solution for deploying and managing state-persistent applications. StatefulSets are equipped with the ability to self-repair, scale both upwards and downwards, and conduct orderly rollouts - although rollbacks typically require a manual process.</p> <p>StatefulSets provide each of their Pod replicas with consistent and enduring identities. This includes predictable names, DNS hostnames, and a unique set of volumes that remain associated through the Pod's lifecycle, encompassing failures, restarts, and rescheduling events. These persistent identities are not just superficial labels; they are fundamental to the StatefulSet's scaling mechanics and their interaction with persistent storage.</p> <p>To conclude, it's important to recognize that StatefulSets offer a structural blueprint rather than a complete solution. They set the stage for resilience and consistency, but it's up to the applications themselves to be architecturally compatible with the StatefulSet paradigm to fully harness its benefits.</p>"},{"location":"core-concepts/storage/","title":"Storage","text":""},{"location":"core-concepts/storage/#overview","title":"Overview","text":"<p>Arguably the most important aspect of any application is the ability to persist and retrieve data. TKubernetes supports a diverse array of storage back-ends, ranging from local storage on nodes to network-attached storage (NAS) and cloud-based storage solutions. It integrates seamlessly with many third-party systems to offer features like replication, snapshots, and backups, enhancing data durability and availability.   </p> <p>Kubernetes can also support different types of storage - anything from objects to files or blocks. However, regardless of the type of storage or where it's located (on-premise, cloud, etc.), Kubernetes will treat it as a volume.  </p> <p>Kubernetes is able to support so many different storage types and services by leveraging the Container Storage Interface (CSI). The CSI is an established standard that provides a straightforward interface for Kubernetes.</p> <p>NetAppNetAppAzureAzuremisc.misc.PV subsystemPV subsystemCSICSI</p> <p>The only thing required for an external storage provider to be surfaced as a volume in Kubernetes is for it to have a CSI plugin. On the right side of the diagram you'll also notice three Kubernetes API objects:  </p> <ul> <li>Persistent Volumes (PV): map to external storage objects</li> <li>Persistent Volume Claims (PVC): akin to \"tickets\" that authorize Pods to be able to use the relevant PV</li> <li>Storage Classes (SC): wrap the previous two in some automation</li> </ul> <p>Take an example below where our cluster is running on GKE and we have a 2TB block of storage called <code>gce-pd</code>. We then create a PV called <code>k8s-vol</code> that will map to the <code>gce-pd</code> with the <code>pd.csi.storage.gke.io</code> CSI plugin.</p> <p>Multiple Pods cannot access the same volume.</p> <p>You cannot map an external storage volume to multiple PVs.</p>"},{"location":"core-concepts/storage/#container-storage-interface-csi","title":"Container Storage Interface (CSI)","text":"<p>The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage systems to containerized workloads on Container Orchestration Systems (COS) like Kubernetes. CSI allows for the consistent configuration and management of storage solutions across various container orchestration systems. </p> <p>CSI enables storage providers to develop a standardized plugin once and have it work across a multitude of container orchestration systems without requiring changes. This simplifies the process of adding new storage capabilities to Kubernetes clusters and ensures compatibility and extendibility.</p> <p>While CSI is a critical piece of getting storage working in Kubernetes, unless you explicitly work on writing storage plugins you'll likely never interact with it directly. Most of your interaction with CSI will simply be referencing your relevant CSI plugin in YAML files.  </p>"},{"location":"core-concepts/storage/#persistent-volumes","title":"Persistent Volumes","text":"<p>At a high level, PVs are the way external storage objects are represented in Kubernetes.</p>"},{"location":"core-concepts/storage/#storage-classes","title":"Storage Classes","text":"<p>StorageClasses (SCs) allow you to define different types of storage. How they are defined depends on the type of storage you're using. For example, if you're using Google Cloud Storage you have classes such as Standard, Nearline, Coldline, and Archive. You may also have simpler/more straightforward classes at your disposal such as SSD and HDD. When you create a SC you map both of those definitions so Pods in your cluster can use either or.</p> External Type K8s StorageClass SSD sc-fast HDD sc-slow <p>Below is an example of how a StorageClass YAML definition may look for leveraging Google Cloud Storage:  </p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ssd\nprovisioner: pd.csi.storage.gke.io  # Google Cloud CSI plugin\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nparameters:\n  type: pd-ssd  # Google Cloud SSD drives\n  provisioned-iops-on-create: '10000'\n</code></pre> <p>StorageClass objects are immutable. You cannot modify them after they are deployed.</p> <p>Below is the high-level flow for creating and using StorageClasses:</p> <ol> <li>Ensure you have a storage back-end (cloud, on-prem, etc.)</li> <li>Have a running Kubernetes cluster</li> <li>Install and setup the CSI storage plugin to connect to Kubernetes</li> <li>Create at least one StorageClass on Kubernetes</li> <li>Deploy Pods with PVCs that reference those Storage classes</li> </ol> <p>Below is an example YAML file that ties SC, PVC, and Pods together so you can see how they all interact:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ssd  # this will be referenced by the PVC below\nprovisioner: pd.csi.storage.gke.io\nparameters:\n  type: pd-ssd\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: mypvc  # this will be referenced by the Pod below\nspec:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n        requests:\n            storage: 256Gi\n    storageClassName: ssd  # matches the name of the SC above\n---\napiVersion: v1\nkind: Pod\nmetadata:\n    name: mypod\nspec:\n    volumes:\n    - name: data\n    persistentVolumeClaim:\n        claimName: mypvc  # matches the name of the PVC above\n...\n</code></pre> <p>This YAML is only partially complete - it's mainly for showing the relationships between these objects via metadata.</p>"},{"location":"core-concepts/storage/#access-mode","title":"Access Mode","text":"<p>Kubernetes StorageClasses support three different types of volume access modes:</p> <ol> <li>ReadWriteOnce(RWO): PV can only be bound as read/write by a single PVC (or Pod)</li> <li>ReadWriteMany: PV can be bound as read/write by multiple PVCs (or Pods)</li> <li>ReadOnlyMany: PV can be bound as read-only by multiple PVCs (or Pods)</li> </ol>"},{"location":"core-concepts/storage/#reclaim-policy","title":"Reclaim Policy","text":"<p>When you define a reclaim policy on a volume, you tell Kubernetes how it should deal with a PV after the relevant PVC is released. There are two options that can be selected:  </p> <ol> <li>Delete: Default option that will delete the PV and any underlying storage resources on the external system itself once the PVC is released.</li> <li>Retain: This will keep the PV object as well as any underlying data on the external system. However, no PVCs can use it going forward. </li> </ol>"},{"location":"core-concepts/storage/#dynamic-provisioning","title":"Dynamic Provisioning","text":"<p>Storage Classes in Kubernetes abstract the details of how storage is provided from how it is consumed. They enable dynamic provisioning of storage resources as needed, which is particularly useful in cloud environments where storage can be requested and scaled programmatically.</p> <p>For example, a StorageClass can define the type of storage (e.g., standard, high-speed SSD, etc.), the replication factor, and the region. When a PVC that references this StorageClass is created, Kubernetes automatically provisions the required storage according to the specifications and binds it to the PVC.</p> <p>Here's an example YAML definition of a StorageClass that uses a Google Cloud persistent disk with high-performance SSD: <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-storage\nprovisioner: kubernetes.io/gce-pd\nparameters:\n  type: pd-ssd\n  replication-type: none\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre></p>"},{"location":"misc/backup-restore/","title":"Backup & Restore","text":"<ul> <li> <p>You can save many objects on the cluster by querying the API Server and exporting it to YAML by running:</p> <p><pre><code>kubectl get all --all-namespaces -o yaml &gt; all-deploy-services.yaml\n</code></pre> </p> </li> <li> <p>Instead of backing up resources, you can back up the etcd server itself - etcd comes with a built-in snapshot solution:</p> <p><pre><code>etcdctl snapshot save /opt/snapshot-pre-boot.db\n--endpoints=https://127.0.0.1:2379 \\\n--cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n--cert=/etc/kubernetes/pki/etcd/server.crt \\\n--key=/etc/kubernetes/pki/etcd/server.key\n</code></pre> </p> </li> <li> <p>You can view the status of a snapshot by running:</p> <p><pre><code>etcdctl snapshot status snapshot.db\n</code></pre> </p> </li> <li> <p>Here are the generalized steps to restore from etcd from a backup:</p> <p><pre><code># first stop the API Server\nservice kube-apiserver stop\n\n# then restore\n# note the new directory being used by etcd\netcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup\n\n# edit etcd service to use new directory\nvi /etc/kubernetes/manifests/etcd.yaml\n\n# restart the etcd service\nsystemctl daemon-reload\n\n# wait 1-2 minutes for pods to come back up\nservice etcd restart\n\n# start API Server\nservice kube-apiserver start\n</code></pre> </p> </li> <li> <p>An easy way to view etcd servers for a cluster is by inspecting the API Server pod running in the <code>kube-system</code> Namespace</p> <ul> <li>It will have a field called <code>--etcd-servers</code> under the <code>command</code> field </li> </ul> </li> <li> <p>To view details about the etcd server you can also run:</p> <p><pre><code>ps -ef | grep etcd\n</code></pre> </p> </li> <li> <p>You can edit the etcd service at:     <pre><code>vi /etc/systemd/system/etcd.service\n</code></pre></p> </li> </ul>"},{"location":"misc/ha/","title":"Configuring HA","text":""},{"location":"misc/ha/#configuring-ha","title":"Configuring HA","text":"<ul> <li> <p>If you lose a Master node, your applications will remain running on worker nodes and users can still access those applications</p> <ul> <li>However, Pods won\u2019t restart or update as part of Deployment because there will be no controllers </li> </ul> </li> <li> <p>The API Server can be run in an <code>Active</code>-<code>Active</code> mode on multiple masters because they process one request at a time and simply pass information to other services which take action</p> <ul> <li>With multiple masters you can point your Kubeconfig to a loadbalancer on port 6443 in front of the masters to distribute traffic between the underlying API Servers </li> </ul> </li> <li> <p>The Controller Manager and Scheduler watch the cluster for changes and take necessary actions based on those changes</p> <ul> <li>These must be run in an <code>Active</code>-<code>Standby</code> mode to ensure that actions are not duplicated</li> <li>Leader election process picks the <code>Active</code> one</li> </ul> </li> </ul>"},{"location":"misc/ha/#etcd-in-ha","title":"ETCD in HA","text":"<ul> <li>ETCD can be configured in two topologies:<ul> <li>Stacked Topology: running on Kubernetes master nodes</li> <li>External Topology: running on dedicated servers external to the cluster</li> <li>Remember the API Server is the only component that talks to ETCD<ul> <li>And must be configured to point to the servers, if hosted externally </li> </ul> </li> </ul> </li> </ul> <p>How does ETCD stay consistent when it allows you to read or write from any instance?</p> <ul> <li>With reads, the same data is available across all nodes so it\u2019s straight forward</li> <li>With writes, etcd ensures that only one instance is responsible for PROCESSING the writes via leader election<ul> <li>The leader then ensures the followers are sent a copy of the data</li> </ul> </li> <li>Writes that come in to an instance other than the leader, they are forwarded to the leader</li> <li>A write is only considered complete once the data has been copied to a majority of the instances<ul> <li>If an instance goes offline during a write, but the majority copy it - the data will be copied over to the node if/when it comes back online</li> </ul> </li> </ul>"},{"location":"misc/upgrades/","title":"Upgrades","text":""},{"location":"misc/upgrades/#os-upgrades","title":"OS Upgrades","text":"<ul> <li>By default, Nodes has 5 minutes to come back up before their Pods are killed</li> <li>When performing an OS upgrade, it\u2019s best to <code>drain</code> the Node before upgrading, because it may take longer than 5 minutes<ul> <li>Use the <code>--ignore-daemonsets</code> flag</li> </ul> </li> <li>Once the Node comes back up, you\u2019ll have to <code>uncordon</code> the Node to make it available to schedule Pods again</li> <li>The <code>cordon</code> command simply marks the Node as un-schedulable (but does not drain the Pods from the Node)</li> </ul>"},{"location":"misc/upgrades/#cluster-upgrade-process","title":"Cluster Upgrade Process","text":"<ul> <li> <p>It is not mandatory for all components to have the same version numbers</p> <ul> <li>However, no components should have a higher version than the API Server</li> <li>Controller Manager can be one version lower</li> <li>Scheduler can be one version lower</li> <li>Kubelet and Kube Proxy can each be two versions lower</li> <li>Kubectl can be one version higher OR lower</li> </ul> </li> <li> <p>Kubernetes supports up to the latest 3 minor versions</p> </li> <li> <p>The recommended approach to upgrade is one minor version at a time</p> </li> <li> <p>Upgrade master nodes first</p> </li> <li> <p>Kubeadm does not manage Kubelets, which must be upgraded manually</p> </li> <li> <p>The output of <code>kubectl get nodes</code> shows the versions of the Kubelets on each node</p> </li> </ul> <p>Generalized steps for upgrading:</p> <pre><code># show recommended version to ugprade to\nkubeadm upgrade plan\n\n# upgrade the kudeadm tool\napt-get upgrade -y kubeadm=&lt;version&gt;\n\n# on the master node, upgrade control plane services\nkubeadm upgrade apply v&lt;version&gt;\n\n# upgrade the kubelet\napt-get upgrade -y kubelet=&lt;version&gt;\n\n# upgrade the node\nkubeadm upgrade node\n\n# restart the kubelet\nsystemctl restart kubelet\n</code></pre>"},{"location":"networking/dns/","title":"DNS","text":""},{"location":"networking/dns/#service-discovery","title":"Service Discovery","text":"<p>As we saw in the previous sections, Kubernetes can be a very busy platform with Pods constantly coming and going. Services help calm some of the storm by providing a stable endpoint for clients to connect to. But how do apps find other apps on a cluster? Through Service discovery! There are two main concepts that make up Service discovery as a whole: Registration and Discovery.</p>"},{"location":"networking/dns/#service-registration","title":"Service registration","text":"<p>This is the process of an app on Kubernetes providing its connection details to a registry in order for other apps on the cluster to be able to find it. This happens automatically when Services are created.  </p> <p>As briefly mentioned in the previous section, Kubernetes provides its own DNS service (typically referred to as the cluster DNS). It's deployed as a series of Pods managed by a Deployment called <code>coredns</code>. These Pods are behind a Service called <code>kube-dns</code>. All of these reside within the <code>kube-system</code> Namespace.  </p> <p>Every Service created on a Kubernetes cluster will automatically register itself with the cluster DNS to ensure that all Pods across the cluster can \"find\" it.</p> <p>1. register newservice1. register new...foo-svc10.0.0.8foo-svc...3. consumeservice3. consume...appapp2. discoverservice2. discover...Service Registryfoo-svc: 10.0.0.8bar-svc: 10.0.0.9ham-svc: 10.0.0.3Service Registry...svc The high-level flow of Service registration is as follows: </p> <ol> <li>Post a Service manifest to the API server (via <code>kubectl</code>)</li> <li>The Service is given a stable IP address called a ClusterIP</li> <li>EndpointSlices are created to maintain the list of healthy Pods which match the Service's label selector</li> <li>The Service's name and IP are registered with the cluster DNS.  </li> </ol> <p>It's worth noting that cluster DNS implements its own controller which constantly watches the API server for new Services being created. When a new one is observed, it automatically creates the DNS records mappings - meaning neither applications nor Services need to perform their own Service registration.  </p> <p>Every node's <code>kube-proxy</code> also watches the API server for new EndpointSlices and creates local networking rules when one is observed. This helps with redirecting ClusterIP traffic to Pod IPs.</p>"},{"location":"networking/dns/#service-discovery_1","title":"Service Discovery","text":"<p>The best way to explain discovery is likely through an example. So let's assume we have two applications on the same cluster - <code>ham</code> and <code>eggs</code>. Each application has their Pods fronted by a Service, which in turn each has their own ClusterIP.</p> $ kubectl get svc<pre><code>NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nham-svc   ClusterIP     192.168.1.200               443/TCP   5d19h\neggs-svc  ClusterIP     192.168.1.208               443/TCP   5d19h\n</code></pre> <p>In order for the <code>ham</code> application to communicate with the <code>eggs</code> application, it needs to know two things:  </p> <ol> <li>The name of the <code>eggs</code> application's Service (<code>eggs-svc</code>)</li> <li>How to convert that name to an IP address</li> </ol> <p>In the case of #1, it's the responsibility of the application developers to know which applications they need to communicate with. Kubernetes internal DNS handles point #2.  </p> <p>As mentioned above, Kubernetes automatically configures each container in the cluster to be able to resolve the IP address of the cluster DNS Service. It also appends any relevant search domains to unqualified names. It performs these actions by populating the <code>/etc/resolv.conf</code> on every container.  </p> <p>ClusterIPs exist on their own special Service network, so it takes a bit of work for traffic to get there. One thing to note is that every node in a cluster has a <code>kube-proxy</code> controller that creates IPVS rules any time a new Service is created. The steps that occur after an application attempts to communicate with another application on the cluster is a series of routing steps that can be summarized as follows:</p> <ol> <li>The application container's default gateway routes the traffic to the node it is running on.</li> <li>The node itself does not have a route to the Service network so it routes the traffic to the node kernel.</li> <li>The node kernel recognizes traffic intended for the service network (recall the IPVS rules) and routes the traffic to a healthy Pod that matches the label selector of the Service.  </li> </ol>"},{"location":"networking/dns/#namespaces","title":"Namespaces","text":"<p>A key point in understanding cluster DNS is knowing that Namespaces are able to partition a cluster's address space. Cluster address spaces are typically denoted as <code>cluster.local</code> and then have object names prepended to it. For instance, the <code>ham-svc</code> Service from above exists in the default Namespace and would have an FQDN of <code>ham-svc.default.svc.cluster.local</code>.  </p> <p>Now imagine you wanted to partition the cluster domain further with <code>perf</code> and <code>qa</code> Namespaces. For a <code>ham-svc</code> Service in each of those Namespaces, the address would look as follows:</p> <ul> <li>Perf: <code>ham-svc.perf.svc.cluster.local</code></li> <li>QA: <code>ham-svc.qa.svc.cluster.local</code></li> </ul> <p>Objects within the same Namespace can connect to each other using short names. However, cross-Namespace communication must use the FQDN. To visualize this, take the following setup where we have a Service in each Namespace fronting a few Pods:</p> <p>perf Namespaceperf Namespacensham-svcham-svcsvcsaltsaltpodbaconbaconpodham-svcham-svcqa Namespaceqa Namespacenseggs-svceggs-svcsvcpoachedpoachedpodfriedfriedpodeggs-svc.qa.svc.cluster.localeggs-svc.qa.svc.cluster.local</p> <p>For the <code>salt</code> Pod to communicate with the <code>ham-svc</code> Service, it can simply reference it by it's short name (<code>ham-svc</code>) since they are within the same <code>perf</code> Namespace.  </p> <p>However, for <code>salt</code> to communicate with the <code>eggs-svc</code> Service, which resides in the <code>qa</code> Namespace, it would have to leverage it's FQDN: <code>eggs-svc.qa.svc.cluster.local</code>.</p>"},{"location":"networking/ingress/","title":"Ingress","text":"<p>Ingress aims to bridge the gap that exists with NodePort and LoadBalancer Services. NodePorts are great, but must use a high port number and require you to know the FQDN or IP address of your nodes. LoadBalancer Services don't require this, but they are limited to one internal Service per load-balancer. So, if you have 50 applications you need exposed to the internet, you'd need 50 of your cloud provider's load-balancers instantiated - which would probably be cost prohibitive in most cases.  </p> <p>Ingresses come into play here by allowing multiple Services to be \"fronted\" by a single cloud load-balancer. To accomplish this, Ingress will use a single LoadBalancer Service and use host-based or path-based routing to send traffic to the appropriate underlying Service.  </p> <p>svc asvc aIngress Controller- routing rules- reading host &amp; path namesIngress Controller...LoadBalancer ServiceLoadBalancer Serviceingsvcpublic cloudpublic cloudsvc bsvc bsvcsvc csvc csvc</p>"},{"location":"networking/ingress/#routing-examples","title":"Routing Examples","text":""},{"location":"networking/ingress/#host-based-routing","title":"Host-based Routing","text":"<p>ham-svcham-svcIngressControllerIngress...LB ServiceLB Serviceingsvceggs-svceggs-svcsvcclientclientham.foo.barham.foo.bareggs.foo.bareggs.foo.barham.foo.barham.foo.bareggs.foo.bareggs.foo.bar </p>"},{"location":"networking/ingress/#path-based-routing","title":"Path-based Routing","text":"<p>ham-svcham-svcIngressControllerIngress...LB ServiceLB Serviceingsvceggs-svceggs-svcsvcclientclientfoo.bar/hamfoo.bar/hamfoo.bar/eggsfoo.bar/eggsfoo.bar/hamfoo.bar/hamfoo.bar/eggsfoo.bar/eggs </p> <p>Kubernetes does not come with an Ingress controller by default</p> <p></p>"},{"location":"networking/ingress/#ingress-controllers","title":"Ingress Controllers","text":"<p>An Ingress controller is deployed as a resource on Kubernetes like any other: <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress\n  template:\n        metdata:\n          labels:\n            name: nginx-ingress\n        spec:\n          containers:\n            - name: nginx-ingress-controller\n              image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0\n          args:\n            - /nginx-ingress-controller\n            - --configmap=$(POD_NAMESPACE)/ngnix-configuration\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          ports:\n            - name: http\n              containerPort: 80\n            - name: https\n              containerPort: 443\n</code></pre> </p> <p>You\u2019ll also need to create a configuration (ConfigMap) for the Ingress controller: <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n    name: nginx-configuration\n</code></pre> </p> <p>You\u2019ll also need a Service to expose the Ingress controller: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n    name: nginx-ingress\nspec:\n    type: NodePort\n    ports:\n    - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: HTTP\n    - port: 443\n    targetPort: 443\n    protocol: TCP\n    name: https\n    selector:\n    name: nginx-ingress\n</code></pre> </p> <p>Ingress controllers have intelligence built-in to monitor the Kubernetes cluster for Ingress changes and update the underlying Nginx server when something changes</p> <ul> <li>To do this, it needs a ServiceAccount with the right set of permissions:     <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: ngninx-ingress-serviceaccount\n</code></pre></li> </ul>"},{"location":"networking/ingress/#ingress-resources","title":"Ingress Resources","text":"<p>An Ingress Resource is a set of rules and configurations applied on the Ingress Controller - i.e. forward all traffic to a single application, route to different applications by URL path, domain, etc. - It is created as a Kubernetes definition file like all others:   <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n    name: ingress-eggs\nspec:\n    backend:\n    serviceName: eggs-service\n    servicePort: 80\n</code></pre>  You can set up multiple rules for handling different traffic scenarios</p> <ul> <li>Within each rule you can handle different paths </li> </ul> <p>You can define the rules in a manifest file definition for the Ingress as well:  <pre><code>apiVersion: v1\nkind: Ingress\nmetadata:\n    name: ingress-eggs-ham\nspec:\n    rules:\n    - http:\n        paths:\n        - path: /eggs\n        backend:\n            serviceName: eggs-service\n            servicePort: 80\n        - path: /ham\n        backend:\n            serviceName: ham-service\n            servicePort: 80\n</code></pre> </p> <p>Here\u2019s an example of how you would write rules based on domain names: <pre><code>apiVersion: v1\nkind: Ingress\nmetadata:\n  name: ingress-eggs-ham\nspec:\n  rules:\n  - host: eggs.my-online-store.com\n      http:\n          paths:\n          - backend:\n              serviceName: eggs-service\n              servicePort: 80\n  - host: ham.my-online-store.com\n      http:\n          paths:\n          - backend:\n              serviceName: ham-service\n              servicePort: 80\n</code></pre></p>"},{"location":"networking/ingress/#more-information","title":"More Information","text":"<p>For a deeper dive into Ingress, refer to the official Kubernetes documentation.</p>"},{"location":"networking/overview/","title":"Networking Overview","text":""},{"location":"networking/overview/#pod-networking","title":"Pod Networking","text":"<ul> <li> <p>Kubernetes does not come with a built-in solution for Pod networking, but it does have clear expectations:</p> <ul> <li>Every Pod should have it\u2019s own unique IP address</li> <li>Every Pod should be able to communicate with every other Pod on the same Node</li> <li>Every Pod should be able to communicate with every other Pod on other nodes without NAT </li> </ul> </li> <li> <p>There mare any networking solutions that solve this for you:</p> <ul> <li>weave, calico, flannel, etc.</li> </ul> </li> </ul>"},{"location":"networking/overview/#cni-in-kubernetes","title":"CNI in Kubernetes","text":"<ul> <li> <p>The plugin is configured in the <code>kubelet.service</code></p> <ul> <li>So all of the networking magic can happen when the Kubelet is creating the containers </li> </ul> </li> <li> <p>You can view this by running <code>px -aux | grep kubelet</code> </p> </li> <li> <p>The <code>/opt/cni/bin</code> directory contains all of the CNI plugins as executables </p> </li> <li>The CNI config directory has a set of configuration files and the Kubelet looks here to find out what plugin to use: <code>/etc/cni/net.d</code><ul> <li>If there are multiple listed, it will chose the first one in alphabetical order</li> </ul> </li> </ul>"},{"location":"networking/overview/#network-policy","title":"Network Policy","text":"<ul> <li> <p>By default, all Pods and Services can talk to all other Pods and Services within a Kubernetes cluster regardless of which Node(s) they are on - <code>Allow All</code> by default </p> </li> <li> <p>To disable communication between certain Pods or Services, you would implement a NetworkPolicy</p> <ul> <li>You link a NetworkPolicy to one or more Pods </li> </ul> </li> <li> <p>To link a NetworkPolicy to a Pod, you leverage labels and selectors </p> </li> </ul> <p>This policy can be configured as part of a NetworkPolicy definition:</p> <p><pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n    name: db-policy\nspec:\n    podSelector:\n    matchLabels:\n        role: db\n    policyTypes:\n    - Ingress\n    ingress:\n    - from:\n    - podSelector:\n        matchLabels:\n            name: api-pod\n    - namespaceSelector:\n        matchLabels:\n            name: prod # must have this label on the Namespace for it to work\n    ports:\n    - protocol: TCP\n        port: 3306\n</code></pre> </p> <ul> <li>Kubernetes networking solutions that support NetworkPolicies:<ul> <li>Kube-router</li> <li>Calico</li> <li>Romana</li> <li>Weave-net </li> </ul> </li> </ul> <p>You do not need to allow egress for a response to ingress. For example, imagine an API pod hitting a DB pod. The DB can allow ingress from the API server and not have to specify to allow egress for the API server to get results - the response is allowed back by default</p> <ul> <li> <p>i.e. when you\u2019re determining rules, you only need to be concerned with where the traffic originates, not responses </p> </li> <li> <p>You can omit the <code>podSelector</code> and just use <code>namespaceSelector</code> to allow all traffic within the Namespace to connect </p> </li> </ul> <p>You can specify resources outside of the Kubernetes by IP addresses as well with the <code>ipBlock.cidr</code> section:</p> <p><pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n    name: db-policy\nspec:\n    podSelector:\n    matchLabels:\n        role: db\n    policyTypes:\n    - Ingress\n    ingress:\n    - from:\n    **- ipBlock:\n        cidr: 192.168.5.10/32**\n    ports:\n    - protocol: TCP\n        port: 3306\n</code></pre> </p> <p>You specify the AND/OR criteria of the selectors by dashes. For example, this rule will allow traffic from Pods that match the given label AND match the given Namespace:   <pre><code>...\ningress:\n- from:\n    **-** podSelector:\n        matchLabels:\n        name: api-pod\n    namespaceSelector:\n        matchLabels:\n        name: Prod\n...\n</code></pre> </p> <p>This will allow traffic from Pods that match either OR criteria:</p> <p><pre><code>...\ningress:\n- from:\n    **-** podSelector:\n        matchLabels:\n        name: api-pod\n    **-** namespaceSelector:\n        matchLabels:\n        name: Prod\n...\n</code></pre> </p> <p>For egress, we need an egress rule: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: db-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 192.168.5.10/32\n    ports:\n    - protocol: TCP\n      port: 3306\n  egress:\n  - to: \n    - ipBlock:\n        cidr: 192.168.5.10/32\n    ports:\n       - protocol: TCP\n         port: 80\n</code></pre></p>"},{"location":"networking/services/","title":"Services","text":""},{"location":"networking/services/#overview","title":"Overview","text":"<p>As mentioned in the Deployments section, Pods will likely be spinning up and down a lot in your environment throughout the course of updates, rollbacks, failures, etc. As such, it's never a good idea for any client to connect directly to a Pod. Pods are there one minute, gone the next - awfully unreliable in and of themselves.  </p> <p>This is where Services come in. Services provide stable, long-lived connection points for clients to connect to. They also maintain a list of Pods to route to and provide basic load-balancing capabilities. With Services, the underlying Pods can come and go, but any client should be able to maintain open communication with the application as the Service provides the logic to know which Pods are healthy and where to route traffic.  </p> <p>my-deploymy-deploydeploypodpodpodpodsvcuser</p>"},{"location":"networking/services/#labels-and-selectors","title":"Labels and Selectors","text":"<p>So how does that work? How do Services know which Pods they should be sending traffic to? The short answer is labels and selectors. In essence, when you define a Service, you specify labels and selectors that - when matched with the same ones on Pods - will route traffic to them.  </p> <p>As an example, image you want to put a stable Service in front of series of Pods that make up your shopping application. When you defined the Deployment of the application you listed the following labels and selectors for the Pods: <code>env=prod</code> and <code>app=shop</code>. Now, when you set up this new Service, you used those same labels in it's YAML definition. The new Service will find all Pods on the cluster with those same labels and is now in charge of routing traffic to them.  </p> <p>Similar to other Kubernetes objects, the Services controller will continually monitor new Pods labels and continually update it's \"list\" (more on that list later) of Pods to route to.  </p> <p>my-deploymy-deploydeploypodpodpodpodsvcenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopenv=devenv=devapp=shopapp=shop</p> <p>One thing to note is that Pods can have extra labels and still be managed by the Service if it's other labels still match. As a concrete example, both of the Pods below will still have traffic routed to them, even though one of them has a label that the Service does not.</p> <p>my-deploymy-deploydeploypodpodsvcenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopenv=prodenv=prodapp=shopapp=shopcur=usdcur=usd</p>"},{"location":"networking/services/#endpointslices","title":"EndpointSlices","text":"<p>As mentioned above, as Pods are spinning up and down, the Service will keep an updated list of Pods with the given labels and selectors. How it does this is through the use of EndpointSlices, which are effectively just dynamic lists of healthy Pods that match a given label selector.  </p> <p>Any new Pods that are created on the cluster that match a Service's label selector will automatically be added to the given Service's EndpointSlice object. When a Pod disappears (fails, node goes down, etc.) it will be removed from the EndpointSlice. The net result is that the Service's EndpointSlice should always be up to date with a list of healthy Pods that the Service can route to.  </p>"},{"location":"networking/services/#service-types","title":"Service Types","text":""},{"location":"networking/services/#clusterip","title":"ClusterIP","text":"<p>Kubernetes supports different types of Services, but the default type is ClusterIP, which is only accessible from inside the cluster. Any time you create a Service in Kubernetes it will automatically get a ClusterIP that's registered in the cluster's internal DNS service (more on the DNS service in a different section). Every single Pod on a cluster leverages the cluster's DNS service - which results in all Pods being able to resolve Service names to ClusterIPs.  </p>"},{"location":"networking/services/#nodeport","title":"NodePort","text":"<p>Another type of Service that Kubernetes supports is called NodePort. This is very similar to ClusterIP but adds the ability for external access on a dedicated port on every node in the cluster. NodePort intentionally uses high-numbered ports (30000 - 32767) to avoid clashing with common ports. To access a NodePort Service from an external client, you simply direct traffic to the IP address of any node in the cluster on the given port. The Service will then route the request to the appropriate Pod based on it's list of healthy ones in it's EndpointSlice object.  </p> <p>Here's a sample definition file of a NodePort that's exposing an application on port 80 via NodePort:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\nspec:\n  type: NodePort\n  ports:\n  - targetPort: 80 # port exposed on the Pod/container\n    port: 80 # port exposed on the Service\n    nodePort: 30008 # port exposed on the Node\n  selector:\n    app: myapp\n    type: front-end\n</code></pre> <p>Only <code>port</code> is required in the configuration for <code>spec.ports</code>. <code>targetPort</code> is assumed to be the same as port if not specified. <code>nodePort</code> is auto-assigned if not specified.</p> <p></p>"},{"location":"networking/services/#loadbalancer","title":"LoadBalancer","text":"<p>If you're running your Kubernetes cluster on a public cloud environment you can leverage a LoadBalancer Service. This will provision an internet-facing load-balancer that you can leverage to send traffic to your Service. For more specifics on this type of Service, refer to the official Kubernetes documentation.</p>"},{"location":"security/auth-rbac/","title":"Auth/RBAC","text":""},{"location":"security/auth-rbac/#overview","title":"Overview","text":"<p>In Kubernetes, everything from creating new resources to updating or deleting them involves making requests to the API server. This is true for everyone and everything in the Kubernetes ecosystem: from developers using <code>kubectl</code>, to the Pods running in your cluster, to the kubelets on each node, and the control plane services that oversee cluster operations.  </p> <p>Let's take an example: imagine a user named \"vinny\" wants to deploy a new application using a Deployment named \"treats\" in the \"petropolis\" Namespace. vinny runs a <code>kubectl apply</code> command, which sends a request to the API server. This request is securely sent over TLS, carrying vinny's credentials. The API server first authenticates vinny, making sure they are who they claim to be. Next, it checks if vinny has the permissions (via RBAC) to create Deployments in the petropolis Namespace. If vinny passes these checks, the request goes through admission control for any additional policy checks before being executed on the cluster.</p>"},{"location":"security/auth-rbac/#authentication-authn","title":"Authentication (AuthN)","text":"<p>Authentication is all about proving who you are. It's often referred to as \"authN.\" At its core are credentials\u2014every request to the API server must include them. The authentication layer checks these credentials; if they don\u2019t match, you get a \"401 Unauthorized\" response. If they check out, you move on to authorization.  </p> <p>Kubernetes doesn\u2019t keep its own user database; instead, it connects to external systems like Active Directory or cloud IAM services for identity management. This setup prevents the creation of redundant identity systems. While Kubernetes supports client certificates out of the box, for practical use, you'll likely integrate it with your existing identity management system. Hosted Kubernetes services usually offer easy integration with their native IAM solutions.  </p>"},{"location":"security/auth-rbac/#checking-your-authentication-setup","title":"Checking Your Authentication Setup","text":"<p>Your connection details to Kubernetes are stored in a <code>kubeconfig</code> file. This file tells tools like <code>kubectl</code> which cluster to talk to and which credentials to use. It includes sections for defining clusters, users, contexts (which pair a user with a cluster), and the current context (the default cluster-user pair for commands).  </p> <p>The <code>clusters</code> section outlines details like the cluster's API server endpoint and its CA's public key. The <code>users</code> section lists user names and their tokens, which are often X.509 certificates signed by a trusted CA. The <code>contexts</code> section pairs users with clusters, and the <code>current-context</code> sets the default for commands.  </p> <p>Given a specific <code>kubeconfig</code>, <code>kubectl</code> commands are directed to the specified cluster and authenticated as the defined user. If your cluster uses an external IAM, it handles the authentication. Once authenticated, the request can proceed to authorization, where Kubernetes decides if you have the necessary permissions to carry out your request.  </p>"},{"location":"security/auth-rbac/#authorization","title":"Authorization","text":"<p>After you've proven your identity to Kubernetes (that's authentication), you're faced with authorization, often abbreviated as authZ. This is where Kubernetes decides if you're allowed to do what you're asking to do, like creating or deleting resources.</p> <p>Kubernetes uses a modular system for authorization, meaning you can have different methods in play. But once any method says \"yes\" to a request, it's off to the next step: admission control. The most common method for making these decisions is Role-Based Access Control (RBAC).</p>"},{"location":"security/auth-rbac/#key-concepts-in-rbac","title":"Key Concepts in RBAC","text":"<p>RBAC boils down to three main ideas:</p> <ul> <li>Users: Who is making the request?</li> <li>Actions: What are they trying to do?</li> <li>Resources: What are they trying to do it to?</li> </ul> <p>Essentially, RBAC controls which users can perform which actions on which resources.</p>"},{"location":"security/auth-rbac/#rbac-in-action","title":"RBAC in Action","text":"<p>In RBAC, you'll deal with Roles and RoleBindings:</p> <ul> <li>Roles specify permissions (what actions can be performed on what resources).</li> <li>RoleBindings link those permissions to users.</li> </ul> <p>For example, you might have a Role that allows reading Deployments in a specific Namespace and a RoleBinding that grants a user those read permissions.</p> <pre><code>kind: Role\nmetadata:\n  namespace: petropolis\n  name: read-deployments\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <p>This Role, by itself, doesn't do much. It needs to be connected to a user through a RoleBinding:</p> <pre><code>kind: RoleBinding\nmetadata:\n  name: read-deployments\n  namespace: petropolis\nsubjects:\n- kind: User\n  name: vinny\nroleRef:\n  kind: Role\n  name: read-deployments\n</code></pre> <p>With this setup, a user named \"vinny\" can list, watch, and get deployments in the \"petropolis\" Namespace.</p>"},{"location":"security/auth-rbac/#the-bigger-picture","title":"The Bigger Picture","text":"<p>Kubernetes doesn't just have Roles and RoleBindings; there are also ClusterRoles and ClusterRoleBindings for cluster-wide permissions. This system allows you to define permissions once at the cluster level and then apply them to specific Namespaces as needed.</p> <p>Most Kubernetes setups come with a set of pre-created roles to get you started, including powerful roles like <code>cluster-admin</code> that should be used cautiously.</p>"},{"location":"security/auth-rbac/#authorization-takeaways","title":"Authorization Takeaways","text":"<p>Authorization in Kubernetes, especially through RBAC, is about specifying what authenticated users are allowed to do within the cluster. It's a system built on allowing certain actions while denying everything else by default, making it crucial to carefully manage permissions to maintain security and functionality in your cluster.</p> <p>Once a request clears the authentication and authorization stages, it's evaluated by admission control to apply any further policies before being executed on the cluster.</p>"},{"location":"security/auth-rbac/#understanding-admission-control-in-kubernetes","title":"Understanding Admission Control in Kubernetes","text":"<p>After a request passes through authentication and authorization, it encounters the final gatekeeper before being executed: admission control. This stage is where Kubernetes applies various policies to ensure the request aligns with cluster rules and standards.</p>"},{"location":"security/auth-rbac/#types-of-admission-controllers","title":"Types of Admission Controllers","text":"<p>Kubernetes employs two main types of admission controllers:</p> <ul> <li>Mutating Admission Controllers: These can alter requests to ensure they comply with policies. For example, they might add a missing label to an object to meet a labeling policy.</li> <li>Validating Admission Controllers: These verify requests against policies but don't modify the requests. If a request violates a policy, it's rejected.</li> </ul> <p>Mutating controllers operate before validating ones, ensuring that any modifications are in place before final checks are made. Only requests that would change the cluster's state are subject to admission control; read-only requests bypass this process.</p>"},{"location":"security/auth-rbac/#example-in-action","title":"Example in Action","text":"<p>Imagine you have a policy requiring all objects to include an <code>app=shop</code> label. A mutating controller could automatically add this label if it's missing from a request, whereas a validating controller would reject any request lacking the label.</p>"},{"location":"security/auth-rbac/#admission-control-on-a-cluster","title":"Admission Control on a Cluster","text":"<p>On Docker Desktop, for instance, the <code>NodeRestriction</code> admission controller is enabled by default, limiting what nodes can modify within their scope. Real-world clusters typically enable a broader set of controllers for comprehensive policy enforcement.</p> <p>A notable example is the <code>AlwaysPullImages</code> controller, a mutating type that ensures Pods always pull their container images from a registry, preventing the use of potentially unsafe local images and ensuring only nodes with proper registry credentials can pull and run containers.</p>"},{"location":"security/auth-rbac/#admission-controls-role","title":"Admission Control's Role","text":"<p>If any admission controller rejects a request, it stops there\u2014no further processing occurs. But if a request gets the green light from all controllers, it's saved to the cluster store and deployed.</p> <p>Admission controllers are increasingly crucial for maintaining the security and integrity of production clusters, given their power to enforce policies directly on incoming requests.</p>"},{"location":"security/auth-rbac/#recap-of-authn-authz-and-rbac","title":"Recap of AuthN, AuthZ, and RBAC","text":"<ul> <li> <p>Authentication (AuthN) validates who you are, using credentials included in every API server request. While Kubernetes doesn't manage user identities internally, it integrates with external systems for robust identity checks.</p> </li> <li> <p>Authorization (AuthZ), particularly through RBAC, dictates what authenticated users can do. It's a system of allowing specific actions via Roles and RoleBindings, ensuring users have only the permissions they need.</p> </li> <li> <p>Admission Control is the last hurdle, enforcing policies on requests post-authorization. It plays a key role in keeping the cluster secure by either modifying requests to align with policies (mutating) or rejecting those that don't comply (validating).</p> </li> </ul> <p>Throughout these stages, TLS secures communication, ensuring that sensitive information remains protected as it travels to the Kubernetes API server.</p>"},{"location":"security/certs/","title":"Certificates","text":""},{"location":"security/certs/#tls","title":"TLS","text":"<ul> <li>A certificate is used to guarantee trust between two parties during a transaction</li> <li>Asymmetrical encryption is where generate private and public keys<ul> <li>You keep the private key with you</li> <li>The public key can, in theory, be shared out anywhere</li> <li>Even if an attacker has the public key (i.e. lock), they can\u2019t actually decrypt the data, access the server, etc. unless they have the private key to do so</li> </ul> </li> </ul>"},{"location":"security/certs/#certificates","title":"Certificates","text":"<ul> <li>Every component in Kubernetes communicates via secure communications</li> <li>Kubernetes requires at least one Certificate Authority (CA) per cluster</li> </ul>"},{"location":"security/certs/#creating-certificates","title":"Creating Certificates","text":"<p>For creating the certificates for the CA: </p> <p>Generate keys:</p> <pre><code>openssl genrsa -out ca.key 2048\n</code></pre> <p></p> <p>Create certificate signing request: <pre><code>openssl req -new -key ca.key -subj \"/CN=KUBERNETES=CA\" -out ca.csr\n</code></pre> </p> <p>Sign the certificate. Note this will be self-signed using it\u2019s own private key we generated in step 1 above, as we are just now initially creating the CA. Subsequent tickets will all be signed by this CA:   <pre><code>openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt\n</code></pre> </p> <p>Generating client certificates (i.e. admin user here) </p> <p>Generate keys:   <pre><code>openssl genrsa -out admin.key 2048\n</code></pre></p> <p></p> <p>Certificate signing request. Note you must add group details (<code>O=system:masters</code>) when creating user certs:  <pre><code>openssl req -new -key admin.key -sub \"/CN=kube-admin/O=system:masters\" -out admin.csr\n</code></pre> </p> <p>Sign certificates by specifying CA cert (<code>-CA ca.crt</code>) and key (<code>-CAkey ca.key</code>):   <pre><code>openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt\n</code></pre></p> <p></p>"},{"location":"security/certs/#what-to-do-with-certs","title":"What to do with certs?","text":"<p>One simple way to use them (i.e. admin user cert here) is to include them in REST calls to the API server. You must include the client key (<code>admin.key</code>), the client cert (<code>admin.crt</code>) and the CA cert (<code>ca.crt</code>) in your call:</p> <p><pre><code>curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt\n</code></pre>  Another easy thing to do is to include them in a kubeconfig file:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority: ca.crt\n    server: https://kube-apiserver:6443\n  name: kubernetes-cluster-1\nkind: Config\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate: admin.crt\n    client-key: admin.key\n</code></pre> <p>All certificate operations are carried out by the ControllerManager on the control plane</p> <p></p>"},{"location":"security/certs/#kubeconfig","title":"KubeConfig","text":"<p>A kubeconfig file lets you specify certificate information without having to type it in every time you run a <code>kubectl</code> command</p> <p>By default, <code>kubectl</code> will look for a kubeconfig file at <code>$HOME/.kube/config</code></p> <p></p> <p>A kubeconfig file consists of three specific parts: 1. clusters: specification of the cluster you want to connect to (i.e. dev, production, etc.) 2. users: the user account you will use to run commands(i.e. admin, dev, etc.) 3. contexts the \"marrying\" of a cluster and a user (i.e. dev user on production cluster)</p> <p>Under each cluster and user specs, you can list out the necessary certificates required for access</p> <p> <pre><code>apiVersion: v1\nkind: Config\n\nclusters:\n- name: my-kube-playground\n  cluster:\n    certificate-authority: ca.crt\n    server: https://my-kube-playground:6443\n\ncontexts:\n- name: my-kube-admin@my-kube-playground\n  context:\n    cluster: my-kube-playground\n    user: my-kube-admin\n    namespace: finance # this field is optional\nusers:\n- name: my-kube-admin\n  user:\n    client-certificate: admin.crt\n    client-key: admin.key\n</code></pre></p> <p>To change contexts, run:</p> <p><pre><code>kubectl config use-context &lt;context-name&gt;\n</code></pre> </p> <p>You can also specify the certificate info via base64 as well: <pre><code>apiVersion: v1\nkind: Config\n\nclusters:\n- name: my-kube-playground\n  cluster:\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNZekNDQWN5Z0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRVUZBREF1TVFzd0NRWURWUVFHRXdKVlV6RU0gCk1Bb0dBMVVFQ2hNRFNVSk5NUkV3RHdZRFZRUUxFd2hNYjJOaGJDQkRRVEFlRncwNU9URXlNakl3TlRBd01EQmEgCkZ3MHdNREV5TWpNd05EVTVOVGxhTUM0eEN6QUpCZ05WQkFZVEFsVlRNUXd3Q2dZRFZRUUtFd05KUWsweEVUQVAgCkJnTlZCQXNUQ0V4dlkyRnNJRU5CTUlHZk1BMEdDU3FHU0liM0RRRUJBUVVBQTRHTkFEQ0JpUUtCZ1FEMmJaRW8gCjd4R2FYMi8wR0hrck5GWnZseEJvdTl2MUptdC9QRGlUTVB2ZThyOUZlSkFRMFFkdkZTVC8wSlBRWUQyMHJIMGIgCmltZERMZ05kTnlubXlSb1MyUy9JSW5mcG1mNjlpeWMyRzBUUHlSdm1ISWlPWmJkQ2QrWUJIUWkxYWRrajE3TkQgCmNXajZTMTR0VnVyRlg3M3p4MHNOb01TNzlxM3R1WEtyRHN4ZXV3SURBUUFCbzRHUU1JR05NRXNHQ1ZVZER3R0cgCitFSUJEUVErRXp4SFpXNWxjbUYwWldRZ1lua2dkR2hsSUZObFkzVnlaVmRoZVNCVFpXTjFjbWwwZVNCVFpYSjIgClpYSWdabTl5SUU5VEx6TTVNQ0FvVWtGRFJpa3dEZ1lEVlIwUEFRSC9CQVFEQWdBR01BOEdBMVVkRXdFQi93UUYgCk1BTUJBZjh3SFFZRFZSME9CQllFRkozK29jUnlDVEp3MDY3ZExTd3IvbmFseDZZTU1BMEdDU3FHU0liM0RRRUIgCkJRVUFBNEdCQU1hUXp0K3phajFHVTc3eXpscjhpaU1CWGdkUXJ3c1paV0pvNWV4bkF1Y0pBRVlRWm1PZnlMaU0gCkQ2b1lxK1puZnZNMG44Ry9ZNzlxOG5od3Z1eHBZT25SU0FYRnA2eFNrcklPZVp0Sk1ZMWgwMExLcC9KWDNOZzEgCnN2WjJhZ0UxMjZKSHNRMGJoek41VEtzWWZid2ZUd2ZqZFdBR3k2VmYxbllpL3JPK3J5TU8KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLSA=\n\n...\n</code></pre></p>"},{"location":"security/container-security/","title":"Container Security","text":""},{"location":"security/container-security/#image-security","title":"Image Security","text":"<ul> <li> <p>Generally, it's best-practice to only use container images from repositories that you trust - either one's internal to your company, your own private repo, or Docker's verified registry (although you should still be careful with these). </p> </li> <li> <p>If you don\u2019t specify a registry in the <code>image</code> name in your manifest, it\u2019s assumed to be Docker\u2019s default registry - <code>docker.io</code></p> <ul> <li>i.e. putting <code>image: nginx</code> will assume it\u2019s actually <code>image: docker.io/library/nginx</code> </li> </ul> </li> <li> <p>Another popular container repo is <code>gcr.io</code> - Google\u2019s container repository where a lot of Kubernetes core images reside</p> </li> </ul>"},{"location":"security/container-security/#security-in-docker","title":"Security in Docker","text":"<ul> <li> <p>By default, Docker runs processes in containers as <code>root</code></p> <ul> <li>You can change this though </li> </ul> </li> <li> <p>Processes running in a container are also visible as running processes on the host itself </p> </li> <li> <p>The <code>root</code> user in the container is not the same as the <code>root</code> user on the host</p> <ul> <li>It\u2019s limited in it\u2019s ability to impact the host or other processes on the host</li> </ul> </li> </ul>"},{"location":"security/container-security/#security-contexts","title":"Security Contexts","text":"<p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-pod\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu\n      securityContext:\n        runAsUser: 1000\n        capabilities:\n          add: [\"MAC_ADMIN\"]\n</code></pre> In the snippet above, we configure security context for a Pod, including:</p> <ul> <li>what user to run as</li> <li> <p>and Linux capabilities to give the Pod </p> </li> <li> <p>You can specify the <code>runAsUser</code> at the container or Pod level - but <code>capabilities</code> is not supported at the Pod level </p> </li> <li>You can find out the user that is used to execute in a container by running:     <pre><code>kubectl exec &lt;pod-name&gt; -- whoami\n</code></pre></li> </ul>"},{"location":"security/service-account/","title":"Service Accounts","text":"<p>There are two types of accounts in Kubernetes</p> <ol> <li>Service - bots</li> <li>User - humans </li> </ol> <p>To create a service account, run <code>kubectl create serviceaccount &lt;name&gt;</code></p> <ul> <li>You must separately created a token (<code>kubectl create token &lt;name&gt;</code>), which the ServiceAccount can use as an authentication bearer token when interacting with the Kubernetes API </li> </ul> <p>For every Namespace in Kubernetes, there is a default ServiceAccount</p> <ul> <li>Whenever a Pod is created, the default ServiceAccount and it\u2019s token are automatically mounted to that Pod as a volume</li> </ul> <p>The default ServiceAccount only has basic permissions to run Kubernetes operations</p> <ul> <li> <p>You can modify your Pod definition to leverage a different ServiceAccount, if desired under <code>spec.serviceAccountName</code></p> <ul> <li>You cannot edit the existing ServiceAccount of a Pod (immutable), but you can for Deployments</li> </ul> </li> </ul>"}]}