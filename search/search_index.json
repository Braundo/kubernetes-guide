{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to K8s Guide","text":"<p>I'm still working on migrating and consolidating content from the original site.</p> <p>Content current as of Kubernetes verison 1.28</p> <p>Welcome to k8s.guide, your concise companion through the world of Kubernetes. My aim was to provide a simplified, digestible, and easy-to-navigate version of the official Kubernetes documentation.</p> <p></p> <p>Legal discalimer:  </p> <ul> <li> <p>\"Kubernetes\", \"K8s\" and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  </p> </li> <li> <p>Neither myself nor this site are officially associated with the Linux Foundation.  </p> </li> </ul>"},{"location":"architecture/cgroupv2/","title":"cgroup v2","text":"<p>On Linux, control groups constrain resources that are allocated to processes. The kubelet and the underlying container runtime need to interface with cgroups to enforce resource management for pods and containers which includes cpu/memory requests and limits for containerized workloads. There are two versions of cgroups in Linux: <code>cgroup v1</code> and <code>cgroup v2</code>. <code>cgroup v2</code> is the new generation of the cgroup API.</p>"},{"location":"architecture/cgroupv2/#what-is-cgroup-v2","title":"What is <code>cgroup v2</code>?","text":"<ul> <li><code>cgroup v2</code> is the next version of the Linux cgroup API.</li> <li>Provides a unified control system with enhanced resource management capabilities.</li> <li>Offers improvements like a single unified hierarchy design in API, safer sub-tree delegation to containers, and enhanced resource allocation management.</li> </ul>"},{"location":"architecture/cgroupv2/#using-cgroup-v2","title":"Using <code>cgroup v2</code>","text":"<ul> <li>Recommended to use a Linux distribution that enables and uses <code>cgroup v2</code> by default.</li> </ul>"},{"location":"architecture/cgroupv2/#requirements","title":"Requirements","text":"<ul> <li>OS distribution should enable <code>cgroup v2</code>.</li> <li>Linux Kernel version should be 5.8 or later.</li> <li>Container runtime should support <code>cgroup v2</code>, e.g., <code>containerd v1.4</code> and later, <code>cri-o v1.20</code> and later.</li> </ul>"},{"location":"architecture/cgroupv2/#linux-distribution-cgroup-v2-support","title":"Linux Distribution <code>cgroup v2</code> support","text":"<ul> <li>Linux distributions that support <code>cgroup v2</code>, such as Container Optimized OS, Ubuntu, Debian GNU/Linux, Fedora, Arch Linux, and RHEL.</li> </ul>"},{"location":"architecture/cgroupv2/#migrating-to-cgroup-v2","title":"Migrating to <code>cgroup v2</code>","text":"<ul> <li>Ensure you meet the requirements and then upgrade to a kernel version that enables <code>cgroup v2</code> by default.</li> <li>The kubelet automatically detects <code>cgroup v2</code> and performs accordingly.</li> </ul>"},{"location":"architecture/cgroupv2/#identify-the-cgroup-version-on-linux-nodes","title":"Identify the cgroup version on Linux Nodes","text":"<ul> <li>To check which cgroup version your distribution uses, you can run specific commands like: <pre><code>stat -fc %T /sys/fs/cgroup/.\n</code></pre></li> </ul>"},{"location":"architecture/cloud-controller-manager/","title":"Cloud Controller Manager","text":"<pre><code>graph TB\n    subgraph Kubernetes Control Plane\n        cm[Cloud Controller Manager]\n        api[API Server]\n        etcd[(ETCD)]\n        cm --&gt;|interacts with| api\n        api --&gt; etcd\n    end\n\n    subgraph Cloud Infrastructure\n        nodeController[Node Controller]\n        routeController[Route Controller]\n        serviceController[Service Controller]\n        cloudResources[Cloud Resources]\n        nodeController --&gt; cloudResources\n        routeController --&gt; cloudResources\n        serviceController --&gt; cloudResources\n    end\n\n    cm --&gt;|manages| nodeController\n    cm --&gt;|manages| routeController\n    cm --&gt;|manages| serviceController\n\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:2px;\n    class cm,nodeController,routeController,serviceController k8s;\n</code></pre>"},{"location":"architecture/cloud-controller-manager/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<ul> <li>A Kubernetes control plane component that embeds cloud-specific control logic.</li> <li>Decouples the interoperability logic between Kubernetes and underlying cloud infrastructure.</li> <li>Allows cloud providers to release features at a different pace compared to the main Kubernetes project.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#design","title":"Design","text":"<ul> <li>Runs in the control plane as a replicated set of processes, usually as containers in Pods.</li> <li>Implements multiple controllers in a single process.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#node-controller","title":"Node Controller","text":"<ul> <li>Updates Node objects when new servers are created in the cloud infrastructure.</li> <li>Annotates and labels the Node object with cloud-specific information.</li> <li>Verifies the node's health and deletes the Node object if the server has been deleted from the cloud.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#route-controller","title":"Route Controller","text":"<ul> <li>Configures routes in the cloud so that containers on different nodes can communicate.</li> <li>May also allocate blocks of IP addresses for the Pod network.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#service-controller","title":"Service Controller","text":"<ul> <li>Integrates with cloud infrastructure components like managed load balancers and IP addresses.</li> <li>Sets up load balancers and other infrastructure when a Service resource requires them.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#api-object-access","title":"API Object Access","text":"<ul> <li>Requires specific access levels to various API objects like Node, Service, and Endpoints.</li> <li>For example, full access to read and modify Node objects, and list and watch access to Service objects.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#rbac-clusterrole","title":"RBAC ClusterRole","text":"<ul> <li>Defines the permissions required for the cloud controller manager, such as creating events and service accounts.</li> </ul>"},{"location":"architecture/controllers/","title":"Controllers","text":""},{"location":"architecture/controllers/#control-loop","title":"Control Loop","text":"<ul> <li>A non-terminating loop that regulates the state of a system.</li> <li>Example: A thermostat in a room that adjusts the temperature to reach the desired state.</li> </ul>"},{"location":"architecture/controllers/#controller-pattern","title":"Controller Pattern","text":"<ul> <li>Tracks at least one Kubernetes resource type.</li> <li>Responsible for making the current state align with the desired state specified in the resource's spec field.</li> </ul>"},{"location":"architecture/controllers/#control-via-api-server","title":"Control via API Server","text":"<ul> <li>Controllers interact with the cluster API server to manage state.</li> <li>Example: The Job controller creates or removes Pods via the API server to complete a task.</li> </ul>"},{"location":"architecture/controllers/#direct-control","title":"Direct Control","text":"<ul> <li>Some controllers interact with external systems to achieve the desired state.</li> <li>Example: A controller that scales the number of nodes in a cluster by interacting with cloud provider APIs.</li> </ul>"},{"location":"architecture/controllers/#desired-vs-current-state","title":"Desired vs. Current State","text":"<ul> <li>Kubernetes aims for a cloud-native approach, handling constant change.</li> <li>Controllers work to bring the current state closer to the desired state, even if the cluster is never in a stable state.</li> </ul>"},{"location":"architecture/controllers/#design-principles","title":"Design Principles","text":"<ul> <li>Kubernetes uses multiple controllers for different aspects of cluster state.</li> <li>Allows for resilience, as one controller can take over if another fails.</li> </ul>"},{"location":"architecture/controllers/#ways-of-running-controllers","title":"Ways of Running Controllers","text":"<ul> <li>Built-in controllers run inside the kube-controller-manager.</li> <li>Custom controllers can run either inside or outside the Kubernetes cluster.</li> </ul> <pre><code>graph LR\n    subgraph Kubernetes Cluster\n        apiServer[API Server]\n        controller[Controller]\n        resource[Resource Spec]\n        actualState[Actual State]\n        desiredState[Desired State]\n\n        resource --&gt;|defines| desiredState\n        apiServer --&gt;|observes| actualState\n        actualState --&gt;|reported via kubelet| apiServer\n        desiredState --&gt; controller\n    end\n\n    apiServer --&gt;|notifies of state changes| controller\n    controller --&gt;|attempts to match| desiredState\n\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:2px;\n    class apiServer,controller k8s;\n</code></pre>"},{"location":"architecture/cri/","title":"Container Runtime Interface","text":""},{"location":"architecture/cri/#what-is-cri","title":"What is CRI?","text":"<ul> <li>CRI is a plugin interface that allows the kubelet to use various container runtimes.</li> <li>It eliminates the need to recompile cluster components for different runtimes.</li> <li>A working container runtime is required on each Node for the kubelet to launch Pods and their containers.</li> </ul>"},{"location":"architecture/cri/#protocol","title":"Protocol","text":"<ul> <li>CRI defines the main gRPC protocol for communication between the kubelet and the container runtime.</li> <li>The kubelet acts as a client when connecting to the container runtime via gRPC.</li> </ul>"},{"location":"architecture/cri/#api-feature-state","title":"API Feature State","text":"<ul> <li>As of Kubernetes v1.23, CRI is considered stable.</li> <li>The kubelet uses command-line flags like <code>--image-service-endpoint</code> to configure runtime and image service endpoints.</li> </ul>"},{"location":"architecture/cri/#cri-version-support","title":"CRI Version Support","text":"<ul> <li>For Kubernetes v1.28, the kubelet prefers to use <code>CRI v1</code>.</li> <li>If a runtime doesn't support v1, the kubelet negotiates an older supported version.</li> <li><code>CRI v1alpha2</code> is considered deprecated.</li> </ul>"},{"location":"architecture/cri/#upgrading","title":"Upgrading","text":"<ul> <li>During a Kubernetes upgrade, the kubelet tries to automatically select the latest CRI version.</li> <li>If that fails, fallback mechanisms are in place.</li> <li>If a gRPC re-dial is required due to a container runtime upgrade, the runtime must support the initially selected version, or the re-dial will fail.</li> </ul>"},{"location":"architecture/garbage-collection/","title":"Garbage Collection","text":""},{"location":"architecture/garbage-collection/#what-is-garbage-collection","title":"What is Garbage Collection?","text":"<ul> <li>Collective term for mechanisms that clean up cluster resources.</li> <li>Targets terminated pods, completed jobs, objects without owner references, unused containers and images, and more.</li> </ul>"},{"location":"architecture/garbage-collection/#owners-and-dependents","title":"Owners and Dependents","text":"<ul> <li>Objects in Kubernetes link to each other through owner references.</li> <li>Owner references help the control plane and other API clients clean up related resources before deleting an object.</li> </ul>"},{"location":"architecture/garbage-collection/#cascading-deletion","title":"Cascading Deletion","text":"<p>Two types: Foreground and Background. - Foreground: Owner object first enters a \"deletion in progress\" state, and dependents are deleted before the owner. - Background: Owner object is deleted immediately, and dependents are cleaned up in the background.</p>"},{"location":"architecture/garbage-collection/#orphaned-dependents","title":"Orphaned Dependents","text":"<ul> <li>Dependents left behind when an owner object is deleted are called orphan objects.</li> <li>By default, Kubernetes deletes dependent objects, but this behavior can be overridden.</li> </ul>"},{"location":"architecture/garbage-collection/#garbage-collection-of-unused-containers-and-images","title":"Garbage Collection of Unused Containers and Images","text":"<ul> <li>Kubelet performs garbage collection on unused images every five minutes and on unused containers every minute.</li> <li>Configurable options include <code>HighThresholdPercent</code> and <code>LowThresholdPercent</code> for disk usage.</li> </ul>"},{"location":"architecture/garbage-collection/#container-garbage-collection","title":"Container Garbage Collection","text":"<ul> <li>Variables like <code>MinAge</code>, <code>MaxPerPodContainer</code>, and <code>MaxContainers</code> can be defined to control container garbage collection.</li> <li>Kubelet adjusts <code>MaxPerPodContainer</code> if it conflicts with <code>MaxContainers</code>.</li> </ul>"},{"location":"architecture/garbage-collection/#configuring-garbage-collection","title":"Configuring Garbage Collection","text":"<ul> <li>Options specific to controllers managing resources can be configured for garbage collection.</li> </ul>"},{"location":"architecture/leases/","title":"Leases","text":""},{"location":"architecture/leases/#leases-in-distributed-systems","title":"Leases in Distributed Systems","text":"<ul> <li>Leases provide a mechanism to lock shared resources and coordinate activities.</li> <li>In Kubernetes, represented by Lease objects in the <code>coordination.k8s.io</code> API Group.</li> </ul>"},{"location":"architecture/leases/#node-heartbeats","title":"Node Heartbeats","text":"<ul> <li>The Lease API is used to communicate kubelet node heartbeats to the Kubernetes API server.</li> <li>Each Node has a corresponding Lease object in the <code>kube-node-lease</code> namespace.</li> <li>The <code>spec.renewTime</code> field is updated with each heartbeat, and the control plane uses this timestamp to determine Node availability.</li> </ul>"},{"location":"architecture/leases/#leader-election","title":"Leader Election","text":"<ul> <li>Leases ensure only one instance of a component runs at a given time.</li> <li>Used by control plane components like <code>kube-controller-manager</code> and <code>kube-scheduler</code> in HA configurations.</li> </ul>"},{"location":"architecture/leases/#api-server-identity","title":"API Server Identity","text":"<ul> <li>Starting in Kubernetes v1.26, each <code>kube-apiserver</code> uses the Lease API to publish its identity.</li> <li>Enables future capabilities that may require coordination between each <code>kube-apiserver</code>.</li> </ul>"},{"location":"architecture/leases/#workloads","title":"Workloads","text":"<ul> <li>Custom workloads can define their own use of Leases for leader election or coordination.</li> <li>Good practice to name the Lease in a way that is linked to the component or product.</li> </ul>"},{"location":"architecture/leases/#garbage-collection","title":"Garbage Collection","text":"<ul> <li>Expired leases from <code>kube-apiservers</code> that no longer exist are garbage-collected by new <code>kube-apiservers</code> after 1 hour.</li> </ul>"},{"location":"architecture/leases/#feature-gate","title":"Feature Gate","text":"<ul> <li>API server identity leases can be disabled by disabling the <code>APIServerIdentity</code> feature gate.</li> </ul>"},{"location":"architecture/mixed-version-proxy/","title":"Mixed Version Proxy","text":""},{"location":"architecture/mixed-version-proxy/#feature-state","title":"Feature State","text":"<ul> <li>As of Kubernetes v1.28, the Mixed Version Proxy is an alpha feature.</li> <li>Allows an API Server to proxy resource requests to other peer API servers running different Kubernetes versions.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#use-case","title":"Use Case","text":"<ul> <li>Useful for clusters with multiple API servers running different versions, especially during long-lived rollouts to new Kubernetes releases.</li> <li>Helps in directing resource requests to the correct <code>kube-apiserver</code>, preventing unexpected 404 errors during upgrades.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#enabling-mixed-version-proxy","title":"Enabling Mixed Version Proxy","text":"<ul> <li>Enable the UnknownVersionInteroperabilityProxy feature gate when starting the API Server.</li> <li>Requires specific command-line arguments like <code>--peer-ca-file</code>, <code>--proxy-client-cert-file</code>, <code>--proxy-client-key-file</code>, and <code>--requestheader-client-ca-file</code>.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#proxy-transport-and-authentication","title":"Proxy Transport and Authentication","text":"<ul> <li>Source <code>kube-apiserver</code> uses existing flags -proxy-client-cert-file and -proxy-client-key-file to present its identity.</li> <li>Destination <code>kube-apiserver</code> verifies the peer connection based on the -requestheader-client-ca-file argument.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#configuration-for-peer-api-server-connectivity","title":"Configuration for Peer API Server Connectivity","text":"<ul> <li>Use <code>--peer-advertise-ip</code> and <code>--peer-advertise-port</code> to set the network location for proxying requests.</li> <li>If unspecified, it defaults to the values from <code>--advertise-address</code> or <code>--bind-address</code>.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#mixed-version-proxying-mechanism","title":"Mixed Version Proxying Mechanism","text":"<ul> <li>Special filter in the aggregation layer identifies API groups/versions/resources that the local server doesn't recognize.</li> <li>Attempts to proxy those requests to a peer API server capable of handling them.</li> <li>If the peer API server fails to respond, a <code>503 (\"Service Unavailable\")</code> error is returned.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#how-it-works-under-the-hood","title":"How it Works Under the Hood","text":"<ul> <li>Uses the internal <code>StorageVersion</code> API to check which API servers can serve the requested resource.</li> <li>If no peer is known for that API group/version/resource, a <code>404 (\"Not Found\")</code> response is returned.</li> <li>If the selected peer fails to respond, a <code>503 (\"Service Unavailable\")</code> error is returned.</li> </ul>"},{"location":"architecture/node-communication/","title":"Node Communication","text":""},{"location":"architecture/node-communication/#node-to-control-plane","title":"Node to Control Plane","text":"<ul> <li>Follows a \"hub-and-spoke\" API pattern.</li> <li>All API usage from nodes or their pods terminates at the API server.</li> <li>API server listens on a secure HTTPS port, typically 443.</li> <li>Nodes should have the public root certificate and valid client credentials for secure connection.</li> </ul>"},{"location":"architecture/node-communication/#api-server-to-kubelet","title":"API Server to Kubelet","text":"<ul> <li>Used for fetching logs, attaching to running pods, and port-forwarding.</li> <li>Connections terminate at the kubelet's HTTPS endpoint.</li> <li>To secure the connection, use the <code>--kubelet-certificate-authority</code> flag for the API server.</li> <li>Kubelet authentication and/or authorization should be enabled.</li> </ul>"},{"location":"architecture/node-communication/#api-server-to-nodes-pods-and-services","title":"API Server to Nodes, Pods, and Services","text":"<ul> <li>Connections default to plain HTTP and are neither authenticated nor encrypted.</li> <li>Can be run over HTTPS but will not validate the certificate or provide client credentials.</li> <li>Not safe to run over untrusted or public networks.</li> </ul>"},{"location":"architecture/node-communication/#ssh-tunnels","title":"SSH Tunnels","text":"<ul> <li>Supports SSH tunnels to protect control plane to nodes communication.</li> <li>API server initiates an SSH tunnel to each node and passes all traffic through the tunnel.</li> <li>Ensures traffic is not exposed outside the nodes' network.</li> </ul>"},{"location":"architecture/node-communication/#konnectivity-service","title":"Konnectivity Service","text":"<ul> <li>Provides TCP level proxy for control plane to cluster communication.</li> <li>Consists of the Konnectivity server in the control plane network and agents in the nodes network.</li> <li>After enabling, all control plane to nodes traffic goes through these connections.</li> </ul>"},{"location":"architecture/nodes/","title":"Nodes","text":""},{"location":"architecture/nodes/#what-are-nodes","title":"What are nodes?","text":"<ul> <li>Worker machines in a Kubernetes cluster.</li> <li>Run containerized applications managed by the Control Plane.</li> <li>Equipped with a Kubelet agent that communicates with the master components.</li> </ul>"},{"location":"architecture/nodes/#control-plane-vs-worker-nodes","title":"Control Plane vs Worker Nodes","text":"<ul> <li>Nodes are managed by master components, primarily the Control Plane.</li> <li>Operations include adding nodes, updating node software, and decommissioning nodes. <pre><code>stateDiagram-v2\n    state control_plane {\n        kube_apiserver\n        kube_apiserver --&gt; etcd: stores data\n        kube_apiserver --&gt; kube_controller_manager: watches changes\n        kube_apiserver --&gt; scheduler: finds placement\n        scheduler --&gt; kube_apiserver: watches for pods needing scheduled\n        kube_controller_manager\n    }\n\n    state worker_nodes {\n        kubelet\n        kubelet --&gt; kube_proxy: configures networking\n        kubelet --&gt; container_runtime: runs containers\n        kube_proxy --&gt; iptables_BPF: manages networking rules\n        container_runtime\n    }\n\n    control_plane --&gt; worker_nodes: manages\n    worker_nodes --&gt; control_plane: reports\n</code></pre> <p>Don't worry if some of these components don't make sense - we'll get to them in later sections.</p> </li> </ul>"},{"location":"architecture/nodes/#node-name-uniqueness","title":"Node Name Uniqueness","text":"<ul> <li>Each node must have a unique identifier within the cluster.</li> <li>Ensures accurate scheduling and task allocation.</li> </ul>"},{"location":"architecture/nodes/#self-registration-of-nodes","title":"Self-registration of Nodes","text":"<ul> <li>Nodes can automatically register themselves upon joining the cluster.</li> <li>Facilitates dynamic scaling and resource allocation.</li> </ul>"},{"location":"architecture/nodes/#manual-node-administration","title":"Manual Node Administration","text":"<ul> <li>Admins can manually add or remove nodes using the Kubernetes API or CLI tools.</li> <li>Useful for fine-grained control over the cluster.</li> </ul>"},{"location":"architecture/nodes/#node-status","title":"Node Status","text":"<ul> <li>Provides detailed information about the node, including IP addresses, conditions (<code>Ready</code>, <code>OutOfDisk</code>, etc.), and resource capacity.</li> <li>Updated periodically by the node's Kubelet.</li> </ul>"},{"location":"architecture/nodes/#node-heartbeats","title":"Node Heartbeats","text":"<ul> <li>Regular signals sent from the Kubelet to the master to indicate the node's health.</li> <li>Failure to send a heartbeat within a certain time leads to node eviction.</li> </ul>"},{"location":"architecture/nodes/#node-controller","title":"Node Controller","text":"<ul> <li>A Control Plane component responsible for monitoring nodes.</li> <li>Handles node failures and triggers pod evictions if necessary.</li> </ul>"},{"location":"architecture/nodes/#rate-limits-on-eviction","title":"Rate Limits on Eviction","text":"<ul> <li>Configurable settings that control the speed at which pods are evicted from unhealthy nodes.</li> <li>Helps to avoid overwhelming the remaining healthy nodes.</li> </ul>"},{"location":"architecture/nodes/#resource-capacity-tracking","title":"Resource Capacity Tracking","text":"<ul> <li>Nodes report available resources like CPU, memory, and storage for better scheduling.</li> <li>Helps the scheduler in placing pods where resources are available.</li> </ul>"},{"location":"architecture/nodes/#node-topology","title":"Node Topology","text":"<ul> <li>Information about the physical or virtual layout of nodes in terms of regions, zones, and other cloud-provider specific metadata.</li> <li>Used for optimizing workload distribution and high availability.</li> </ul>"},{"location":"architecture/nodes/#graceful-node-shutdown","title":"Graceful Node Shutdown","text":"<ul> <li>A process that safely evicts pods before shutting down or rebooting a node.</li> <li>Ensures minimal impact on running applications and services.</li> </ul>"},{"location":"architecture/nodes/#pod-priority-based-graceful-node-shutdown","title":"Pod Priority based Graceful Node Shutdown","text":"<ul> <li>During a graceful shutdown, pods with higher priority are evicted last.</li> <li>Ensures that critical applications continue to run for as long as possible.</li> </ul>"},{"location":"architecture/nodes/#non-graceful-node-shutdown-handling","title":"Non-graceful Node Shutdown Handling","text":"<ul> <li>In cases of abrupt failures, all pods are immediately terminated.</li> <li>Risks include data loss and potential service disruption.</li> </ul>"},{"location":"architecture/nodes/#swap-memory-management","title":"Swap Memory Management","text":"<ul> <li>Kubernetes allows for the enabling or disabling of swap memory usage on nodes.</li> <li>Swap usage can impact application performance and pod scheduling decisions.</li> </ul>"},{"location":"containers/container-environment/","title":"Environment","text":""},{"location":"containers/container-environment/#container-environment","title":"Container Environment","text":"<ul> <li>The Kubernetes Container environment provides several important resources to Containers:<ul> <li>Filesystem: A combination of an image and one or more volumes.</li> <li>Container Information: Information about the Container itself.</li> <li>Cluster Information: Information about other objects in the cluster.</li> </ul> </li> </ul>"},{"location":"containers/container-environment/#container-information","title":"Container Information","text":"<ul> <li>The hostname of a Container is the name of the Pod in which the Container is running. This can be accessed through the hostname command or the gethostname function call in <code>libc</code>.</li> <li>The Pod name and namespace are available as environment variables through the downward API.</li> <li>User-defined environment variables from the Pod definition are also available to the Container, as are any environment variables specified statically in the container image.</li> </ul>"},{"location":"containers/container-environment/#cluster-information","title":"Cluster Information","text":"<ul> <li>A list of all services running when a Container was created is available to that Container as environment variables.</li> <li>This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.</li> <li>For a service named <code>foo</code> that maps to a Container named <code>bar</code>, variables like <code>FOO_SERVICE_HOST</code> and <code>FOO_SERVICE_PORT</code> are defined.</li> <li>Services have dedicated IP addresses and are available to the Container via DNS if the DNS addon is enabled.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/","title":"Lifecycle Hooks","text":"<p>Container lifecycle hooks allow containers to be aware of events in their management lifecycle and run specific code when these events occur.</p>"},{"location":"containers/container-lifecycle-hooks/#container-hooks","title":"Container Hooks","text":"<ul> <li>PostStart: This hook is executed immediately after a container is created. However, there's no guarantee that it will execute before the container's <code>ENTRYPOINT</code>. No parameters are passed to the handler.</li> <li>PreStop: This hook is called right before a container is terminated due to various reasons like API request, liveness/startup probe failure, etc. The hook must complete before the <code>TERM</code> signal to stop the container is sent.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/#hook-handler-implementations","title":"Hook Handler Implementations","text":"<ul> <li>Containers can implement two types of hook handlers:<ul> <li><code>Exec</code>: Executes a specific command inside the container's cgroups and namespaces.</li> <li><code>HTTP</code>: Executes an HTTP request against a specific endpoint on the container.</li> </ul> </li> </ul>"},{"location":"containers/container-lifecycle-hooks/#hook-handler-execution","title":"Hook Handler Execution","text":"<ul> <li>Hook calls are synchronous within the context of the Pod containing the container.</li> <li>For <code>PostStart</code> hooks, the Container <code>ENTRYPOINT</code> and hook fire asynchronously.</li> <li><code>PreStop</code> hooks must complete before the <code>TERM</code> signal can be sent.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/#hook-delivery-guarantees","title":"Hook Delivery Guarantees","text":"<ul> <li>Generally, hook delivery is intended to be at least once, meaning a hook may be called multiple times for any given event.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/#debugging-hook-handlers","title":"Debugging Hook Handlers","text":"<ul> <li>Logs for hook handlers are not exposed in Pod events. If a handler fails, it broadcasts an event like <code>FailedPostStartHook</code> or <code>FailedPreStopHook</code>.</li> </ul>"},{"location":"containers/images/","title":"Images","text":""},{"location":"containers/images/#image-pull-operations","title":"Image Pull Operations","text":"<ul> <li>Several methods to provide credentials, including node configuration and <code>imagePullSecrets</code>.</li> <li>Requires keys for access.</li> </ul>"},{"location":"containers/images/#private-registries","title":"Private Registries","text":"<ul> <li>Automatically set based on conditions like whether a tag or digest is specified.</li> </ul>"},{"location":"containers/images/#default-image-pull-policies","title":"Default Image Pull Policies","text":"<ul> <li><code>Never</code>: Never pulls image; uses local if available.</li> <li><code>Always</code>: Always pulls image.</li> <li><code>IfNotPresent</code>: Pulls image only if not present.</li> <li>By default, the pull policy is set to <code>IfNotPresent</code>, meaning the image is pulled only if not already present locally.</li> </ul>"},{"location":"containers/images/#updating-images","title":"Updating Images","text":"<ul> <li>Tags can be added to identify versions.</li> <li>They can include a registry hostname and port number.</li> </ul>"},{"location":"containers/images/#image-names","title":"Image Names","text":"<ul> <li>They are pushed to a registry and then referred to in a Pod.</li> <li>Container images encapsulate an application and its dependencies.</li> </ul>"},{"location":"containers/overview/","title":"Overview","text":""},{"location":"containers/overview/#what-are-containers","title":"What are Containers?","text":"<ul> <li>Technology for packaging an application along with its runtime dependencies.</li> <li>Containers are repeatable and standardized, ensuring the same behavior wherever you run them.</li> <li>They decouple applications from the underlying host infrastructure, making deployment easier across different cloud or OS environments.</li> <li>In a Kubernetes cluster, each node runs the containers that form the Pods assigned to that node.</li> </ul>"},{"location":"containers/overview/#container-images","title":"Container Images","text":"<ul> <li>A container image is a ready-to-run software package.</li> <li>It contains everything needed to run an application: the code, runtime, application and system libraries, and default settings.</li> <li>Containers are intended to be stateless and immutable. Changes should be made by building a new image and recreating the container.</li> </ul>"},{"location":"containers/overview/#container-runtimes","title":"Container Runtimes","text":"<ul> <li>A fundamental component in Kubernetes for running containers effectively.</li> <li>Manages the execution and lifecycle of containers within the Kubernetes environment.</li> <li>Kubernetes supports container runtimes like containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).</li> <li>You can allow your cluster to pick the default container runtime for a Pod or specify the RuntimeClass for different settings.</li> </ul> <p> A simple way to think about the relationship of containers and Kubernetes is that each node can run multiple pods, which in turn each run a single container (typically).</p> <pre><code>graph TD\n    Node[Node] --&gt; Pod1[Pod]\n    Node --&gt; Pod2[Pod]\n    Node --&gt; Pod3[Pod]\n    Node --&gt; Pod4[Pod]\n\n    Pod1 --&gt; Container1[Container]\n    Pod2 --&gt; Container2[Container]\n    Pod3 --&gt; Container3[Container]\n    Pod4 --&gt; Container4[Container]\n\n    style Node fill:#f9f,stroke:#333,stroke-width:4px\n    style Pod1 fill:#bbf,stroke:#333,stroke-width:2px\n    style Pod2 fill:#bbf,stroke:#333,stroke-width:2px\n    style Pod3 fill:#bbf,stroke:#333,stroke-width:2px\n    style Pod4 fill:#bbf,stroke:#333,stroke-width:2px\n    style Container1 fill:#88f,stroke:#333,stroke-width:1px\n    style Container2 fill:#88f,stroke:#333,stroke-width:1px\n    style Container3 fill:#88f,stroke:#333,stroke-width:1px\n    style Container4 fill:#88f,stroke:#333,stroke-width:1px\n</code></pre>"},{"location":"containers/runtime-class/","title":"Runtime Class","text":""},{"location":"containers/runtime-class/#motivation","title":"Motivation","text":"<ul> <li>You can set different RuntimeClasses for different Pods to balance performance and security.</li> <li>For example, you might use a runtime that employs hardware virtualization for Pods requiring high levels of information security.</li> </ul>"},{"location":"containers/runtime-class/#setup","title":"Setup","text":"<ol> <li>Configure the CRI implementation on nodes: This is runtime-dependent and involves setting up configurations that have a corresponding handler name.</li> <li>Create RuntimeClass resources: Each configuration set up in step 1 should have an associated handler name. For each handler, create a corresponding RuntimeClass object.</li> </ol>"},{"location":"containers/runtime-class/#usage","title":"Usage","text":"<ul> <li>You can specify a <code>runtimeClassName</code> in the Pod spec to use a particular RuntimeClass.</li> <li>If the specified RuntimeClass doesn't exist or the CRI can't run the corresponding handler, the Pod will enter a Failed state.</li> </ul>"},{"location":"containers/runtime-class/#scheduling","title":"Scheduling","text":"<ul> <li>You can set constraints to ensure that Pods running with this RuntimeClass are scheduled to nodes that support it.</li> <li>This is done through the scheduling field for a RuntimeClass.</li> </ul>"},{"location":"pods/disruptions/","title":"Disruptions","text":""},{"location":"pods/disruptions/#types-of-disruptions","title":"Types of Disruptions","text":"<ul> <li>Involuntary Disruptions: Unavoidable cases like hardware failure, cloud provider issues, etc.</li> <li>Voluntary Disruptions: Actions initiated by the application owner or cluster administrator, such as draining a node for repair or upgrade.</li> </ul>"},{"location":"pods/disruptions/#mitigating-disruptions","title":"Mitigating Disruptions","text":"<ul> <li>Request the resources your pod needs.</li> <li>Replicate your application for higher availability.</li> <li>Use anti-affinity to spread applications across racks or zones.</li> </ul>"},{"location":"pods/disruptions/#pod-disruption-budgets-pdb","title":"Pod Disruption Budgets (PDB)","text":"<ul> <li>Allows you to specify how many pods of a replicated application can be down simultaneously.</li> <li>Cluster managers should respect PDBs by calling the Eviction API.</li> </ul>"},{"location":"pods/disruptions/#pod-disruption-conditions","title":"Pod Disruption Conditions","text":"<ul> <li>A beta feature that adds a dedicated condition to indicate that the Pod is about to be deleted due to a disruption.</li> </ul>"},{"location":"pods/disruptions/#separation-of-roles","title":"Separation of Roles","text":"<ul> <li>Discusses the benefits of separating the roles of Cluster Manager and Application Owner.</li> </ul>"},{"location":"pods/disruptions/#options-for-cluster-administrators","title":"Options for Cluster Administrators","text":"<ul> <li>Accept downtime, failover to another cluster, or write disruption-tolerant applications and use PDBs.</li> </ul>"},{"location":"pods/downward-api/","title":"Downward API","text":""},{"location":"pods/downward-api/#understanding-the-downward-api","title":"Understanding the Downward API","text":"<p>The Downward API is a feature in Kubernetes that allows pods to retrieve information about themselves or the cluster, which can be exposed to containers within the pod. This mechanism enables containers to consume details about the pod or the cluster without direct interaction with the Kubernetes API server.</p>"},{"location":"pods/downward-api/#two-methods-of-exposure","title":"Two Methods of Exposure","text":"<ul> <li> <p>Environment Variables: Specific pod and container fields can be exposed to running containers as environment variables. This is defined in the pod's configuration file and allows a container to access information like its own name, namespace, or node details.</p> </li> <li> <p>Volume Files: Kubernetes can also expose the same information through files in a volume. This special volume type is called the \"downward API volume,\" and it presents information in a filesystem that the container can read, providing a more dynamic approach to accessing the data.</p> </li> </ul>"},{"location":"pods/downward-api/#benefits-of-low-coupling","title":"Benefits of Low Coupling","text":"<p>The downward API is particularly useful for legacy applications or third-party tools that expect certain information to be available in the environment but are not designed to interact with Kubernetes directly. It simplifies the process of adapting non-native Kubernetes applications to the platform.</p>"},{"location":"pods/downward-api/#available-fields-and-resources","title":"Available Fields and Resources","text":"<ul> <li>Containers can access a variety of information via the Downward API, including:</li> <li>Pod Metadata: Such as the pod's name, namespace, annotations, labels, and unique UID.</li> <li>Resource Requests and Limits: Information about the CPU and memory limits and requests that are set for the container.</li> </ul>"},{"location":"pods/downward-api/#fallback-for-resource-limits","title":"Fallback for Resource Limits","text":"<p>When a container's resource limits are not explicitly defined in the pod specification, the <code>kubelet</code> can expose the default limits as the maximum allocatable resources available on the node. This ensures that the container has some information about the resources it can use, which is critical for managing application performance and resource usage.</p>"},{"location":"pods/downward-api/#use-cases","title":"Use Cases","text":"<ul> <li>Configuration Files: Applications that configure themselves through external files can use the Downward API volume to generate those files.</li> <li>Self-Awareness: Containers that need to be aware of their metadata (for logging, monitoring, or other operational purposes) can use the Downward API to get that information.</li> <li>Resource Management: Containers can adjust their behavior based on the resources available to them, which is particularly useful in high-density multi-tenant environments where resource constraints are common.  The Downward API provides a powerful way to maintain the abstraction that Kubernetes offers while still giving containers the necessary information to operate correctly in a dynamic and distributed system. </li> </ul> <pre><code>graph TD\n    Pod[Pod]\n    EnvVars[Environment Variables]\n    DownwardAPIVolume[Downward API Volume]\n    Container[Container]\n\n    Pod --&gt;|Exposes info via| EnvVars\n    Pod --&gt;|Exposes info via| DownwardAPIVolume\n    EnvVars --&gt;|Accessed by| Container\n    DownwardAPIVolume --&gt;|Accessed by| Container\n\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:2px;\n    class Pod,Container,EnvVars,DownwardAPIVolume k8s;\n</code></pre>"},{"location":"pods/ephemeral-containers/","title":"Ephemeral Containers","text":""},{"location":"pods/ephemeral-containers/#purpose","title":"Purpose","text":"<p>Ephemeral containers are a special type of container designed for temporary tasks like troubleshooting. They are not meant for building applications.</p>"},{"location":"pods/ephemeral-containers/#immutability","title":"Immutability","text":"<p>Once a Pod is created, you can't add a container to it. Ephemeral containers offer a way to inspect the state of an existing Pod without altering it.</p>"},{"location":"pods/ephemeral-containers/#resource-allocation","title":"Resource Allocation","text":"<p>Ephemeral containers don't have guarantees for resources or execution. They will never be automatically restarted.</p>"},{"location":"pods/ephemeral-containers/#limitations","title":"Limitations","text":"<p>Many fields that are available for regular containers are disallowed for ephemeral containers, such as ports and resource allocations.</p>"},{"location":"pods/ephemeral-containers/#creation-method","title":"Creation Method","text":"<p>These containers are created using a special <code>ephemeralcontainers</code> handler in the API, not directly through <code>pod.spec</code>.</p>"},{"location":"pods/ephemeral-containers/#use-cases","title":"Use-Cases","text":"<p>Useful for interactive troubleshooting when <code>kubectl exec</code> is insufficient, especially with distroless images that lack debugging utilities.</p>"},{"location":"pods/ephemeral-containers/#process-namespace-sharing","title":"Process Namespace Sharing","text":"<p>Enabling this feature is helpful for viewing processes in other containers within the same Pod.</p>"},{"location":"pods/init-containers/","title":"Init Containers","text":""},{"location":"pods/init-containers/#what-are-init-containers","title":"What are Init Containers?","text":"<ul> <li>Specialized containers that run before the application containers in a Pod. They can contain utilities or setup scripts not present in the application image.</li> <li>Init containers always run to completion, and each must complete successfully before the next one starts. If an init container fails, it is restarted until it succeeds, depending on the Pod's <code>restartPolicy</code>.</li> </ul>"},{"location":"pods/init-containers/#configuration","title":"Configuration","text":"<ul> <li>Init containers are specified in the Pod specification under the <code>initContainers</code> field, similar to how application containers are defined.</li> </ul>"},{"location":"pods/init-containers/#differences-from-regular-containers","title":"Differences from Regular Containers","text":"<ul> <li>Init containers support all the features of application containers but do not support lifecycle hooks or probes like <code>livenessProbe</code>, <code>readinessProbe</code>, and <code>startupProbe</code>.</li> </ul>"},{"location":"pods/init-containers/#esource-handling","title":"esource Handling","text":"<ul> <li>Resource requests and limits for init containers are managed differently than for application containers.</li> </ul>"},{"location":"pods/init-containers/#sequential-execution","title":"Sequential Execution","text":"<ul> <li>If multiple init containers are specified, they are run sequentially, and each must succeed before the next can run.</li> </ul>"},{"location":"pods/init-containers/#use-cases","title":"Use Cases:","text":"<ul> <li>Running utilities or custom code for setup that are not present in the application image.</li> <li>Blocking or delaying application container startup until certain preconditions are met.</li> <li>Limiting the attack surface by keeping unnecessary tools separate.</li> <li>Examples: The documentation provides YAML examples to demonstrate how to define a Pod with init containers.</li> <li>Advanced features like <code>activeDeadlineSeconds</code> can be used to prevent init containers from failing forever.</li> <li>Starting from Kubernetes v1.28, a feature gate named <code>SidecarContainers</code> allows specifying a <code>restartPolicy</code> for init containers independent of the Pod and other init containers.</li> <li>Resource Sharing: The highest of any particular resource request or limit defined on all init containers is the effective init request/limit for the Pod.</li> <li>Pod Restart Reasons: A Pod can restart, causing re-execution of init containers, for various reasons like Pod infrastructure container restart or all containers in a Pod being terminated.</li> </ul>"},{"location":"pods/overview/","title":"Overview","text":"<p>Pods are the smallest deployable units of computing in Kubernetes. A Pod is a group of one or more containers with shared storage and network resources. They are always co-located and co-scheduled, running in a shared context. Pods model an application-specific \"logical host\" and can contain one or more application containers that are tightly coupled.</p>"},{"location":"pods/overview/#key-concepts","title":"Key Concepts:","text":"<ul> <li> <p>What is a Pod?: A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.</p> </li> <li> <p>Using Pods: Pods are generally not created directly but are created using workload resources like Deployment or Job.</p> </li> <li> <p>Workload Resources: These are resources that manage one or more Pods for you. Examples include Deployment, StatefulSet, and DaemonSet.</p> </li> <li> <p>Pod Templates: These are specifications for creating Pods and are included in workload resources.</p> </li> <li> <p>Pod Update and Replacement: When the Pod template for a workload resource is changed, new Pods are created based on the updated template.</p> </li> <li> <p>Resource Sharing and Communication: Pods enable data sharing and communication among their constituent containers.</p> </li> <li> <p>Storage in Pods: A Pod can specify a set of shared storage volumes that all containers in the Pod can access.</p> </li> <li> <p>Pod Networking: Each Pod is assigned a unique IP address. Containers in a Pod share the network namespace, including the IP address and network ports.</p> </li> <li> <p>Privileged Mode for Containers: Any container in a Pod can run in privileged mode to use operating system administrative capabilities.</p> </li> <li> <p>Static Pods: These are managed directly by the <code>kubelet</code> daemon on a specific node, without the API server observing them.</p> </li> <li> <p>Container Probes: These are diagnostics performed periodically by the <code>kubelet</code> on a container.</p> </li> </ul>"},{"location":"pods/pod-lifecycle/","title":"Pod Lifecycle","text":""},{"location":"pods/pod-lifecycle/#phases","title":"Phases","text":"<p>A Pod's life begins in <code>Pending</code> when it's accepted by the Kubernetes system, but the container images are not yet running. It moves to <code>Running</code> when its containers start, but may enter <code>Succeeded</code> or <code>Failed</code> if it completes its task or encounters an error, respectively. <code>Unknown</code> indicates that the cluster cannot determine the Pod's state, often due to communication problems.</p>"},{"location":"pods/pod-lifecycle/#container-states","title":"Container States","text":"<ul> <li>Containers within a Pod can be in different states:</li> <li><code>Waiting</code>: The container is not yet running its workload, typically because it's pulling its image or waiting for its command to start.</li> <li><code>Running</code>: The container is executing without issues.</li> <li><code>Terminated</code>: The container has stopped, either because it completed its task or due to an error. This state is often accompanied by exit codes and status messages that can be checked using <code>kubectl</code>.</li> </ul>"},{"location":"pods/pod-lifecycle/#container-restart-policy","title":"Container Restart Policy","text":"<ul> <li>The <code>restartPolicy</code> field within a Pod specification dictates the Kubelet's behavior when handling container terminations:</li> <li><code>Always</code>: Automatically restart the container if it stops.</li> <li><code>OnFailure</code>: Restart only if the container exits with a non-zero exit status (indicative of failure).</li> <li><code>Never</code>: Do not automatically restart the container.</li> </ul>"},{"location":"pods/pod-lifecycle/#pod-conditions","title":"Pod Conditions","text":"<ul> <li>These are flags set by the Kubelet to provide more granular status than the phase:<ul> <li><code>PodScheduled</code>: Indicates if the pod has been scheduled to a node.</li> <li><code>ContainersReady</code>: All containers in the Pod are ready.</li> <li><code>Initialized</code>: All init containers have started successfully.</li> <li><code>Ready</code>: The Pod is able to serve requests and should be added to the load balancing pools of all matching services.</li> </ul> </li> </ul>"},{"location":"pods/pod-lifecycle/#custom-readiness-checks","title":"Custom readiness checks","text":"<ul> <li>Can be configured via <code>readinessGates</code> in a Pod's specification, allowing you to define additional conditions to be evaluated before considering a Pod as ready.</li> <li><code>PodReadyToStartContainers</code> is a hypothetical condition that could be used to signify network readiness, implying the Pod's network setup is complete and it's ready to start containers.</li> </ul>"},{"location":"pods/pod-lifecycle/#container-probes","title":"Container Probes","text":"<ul> <li>These are diagnostic tools used by the Kubelet to assess the health and readiness of a container:</li> <li><code>livenessProbe</code>: Determines if a container is running. If this probe fails, the Kubelet kills the container which may be restarted depending on the pod's <code>restartPolicy</code>.</li> <li><code>readinessProbe</code>: Determines if a container is ready to respond to requests. Failing this probe means the container gets removed from service endpoints.</li> <li><code>startupProbe</code>: Used for containers that take a long time to start. If this probe fails, the Kubelet will not start the liveness or readiness probes, giving the container more time to initialize.</li> </ul>"},{"location":"pods/pod-lifecycle/#using-probes","title":"Using Probes","text":"<ul> <li>Liveness Probes: Implement if you need to handle the container's inability to recover from a deadlock or other runtime issues internally.</li> <li>Readiness Probes: Utilize when your container needs to perform certain actions such as warming a cache or migrating a database before it can serve requests.</li> <li>Startup Probes: Employ for slow-starting containers to ensure that Kubernetes doesn't kill them before they're up and running.</li> </ul>"},{"location":"pods/pod-lifecycle/#termination-of-pods","title":"Termination of Pods","text":"<ul> <li>Pods are terminated gracefully, allowing for cleanup and shutdown procedures to complete. The Kubelet sends a <code>SIGTERM</code> signal to the containers, indicating that they should shut down. You can specify the grace period during which a container should complete its shutdown before being forcibly killed.</li> </ul>"},{"location":"pods/qos-classes/","title":"QoS Classes","text":""},{"location":"pods/qos-classes/#scheduler-behavior","title":"Scheduler Behavior","text":"<p>The <code>kube-scheduler</code> does not consider QoS class when selecting which Pods to preempt.</p>"},{"location":"pods/qos-classes/#resource-management","title":"Resource Management","text":"<p>The resource request of a Pod is the sum of the resource requests of its containers, and similarly for the resource limit.</p>"},{"location":"pods/qos-classes/#behavior-independent-of-qos-class","title":"Behavior Independent of QoS Class","text":"<p>Any container exceeding a resource limit will be killed and restarted, and Pods exceeding resource requests become candidates for eviction.</p>"},{"location":"pods/qos-classes/#memory-qos-with-cgroup-v2","title":"Memory QoS with cgroup v2","text":"<p>This feature, in alpha stage as of Kubernetes v1.22, uses the memory controller of cgroup v2 to guarantee memory resources.</p>"},{"location":"pods/qos-classes/#types-of-qos-classes","title":"Types of QoS Classes","text":"<ul> <li>Guaranteed: These Pods have strict resource limits and are least likely to be evicted.</li> <li>Burstable: These Pods have some lower-bound resource guarantees but do not require a specific limit.</li> <li>BestEffort: These Pods can use any available node resources and are the first to be evicted under resource pressure.</li> </ul>"},{"location":"pods/user-namespaces/","title":"User Namespaces","text":""},{"location":"pods/user-namespaces/#feature-state","title":"Feature State","text":"<p>This is an alpha feature introduced in Kubernetes verison 1.25</p>"},{"location":"pods/user-namespaces/#purpose","title":"Purpose","text":"<p>User namespaces isolate the user running inside the container from the one in the host. This enhances security by limiting the damage a compromised container can do to the host or other pods.</p>"},{"location":"pods/user-namespaces/#linux-only-feature","title":"Linux-only Feature","text":"<p>This feature is specific to Linux and requires support for <code>idmap</code> mounts on the filesystems used.</p>"},{"location":"pods/user-namespaces/#container-runtime-support","title":"Container Runtime Support","text":"<p>CRI-O version 1.25 and later support this feature. <code>Containerd</code> v1.7 is not compatible with certain Kubernetes versions in terms of user namespace support.</p>"},{"location":"pods/user-namespaces/#uidgid-mapping","title":"UID/GID Mapping","text":"<p>The <code>kubelet</code> will assign unique host UIDs/GIDs to each pod to ensure no overlap.</p>"},{"location":"pods/user-namespaces/#capabilities","title":"Capabilities","text":"<p>Capabilities granted to a pod are limited to the pod's user namespace and are mostly invalid outside of it.</p>"},{"location":"pods/user-namespaces/#limitations","title":"Limitations","text":"<p>When using a user namespace, you cannot use other host namespaces like network, IPC, or PID.</p>"},{"location":"pods/user-namespaces/#security","title":"Security","text":"<p>The feature mitigates the impact of certain CVEs by ensuring that UIDs/GIDs used by the host's files and host's processes are in a specific range.</p>"},{"location":"services-and-networking/dns/","title":"DNS","text":""},{"location":"services-and-networking/dns/#terminology","title":"Terminology","text":"<ul> <li>DNS: Domain Name System</li> <li>FQDN: Fully Qualified Domain Name</li> <li>SRV Records: Service records in DNS</li> </ul>"},{"location":"services-and-networking/dns/#what-is-dns-for-services-and-pods","title":"What is DNS for Services and Pods?","text":"<ul> <li>Kubernetes creates DNS records for Services and Pods.</li> <li>Allows for name-based service discovery within the cluster.</li> </ul>"},{"location":"services-and-networking/dns/#dns-records","title":"DNS Records","text":"<ul> <li>Services and Pods get DNS records.</li> <li>\"Normal\" Services get A/AAAA records.</li> <li>Headless Services also get A/AAAA records but resolve to the set of IPs of all Pods selected by the Service.</li> <li>SRV Records are created for named ports.</li> </ul>"},{"location":"services-and-networking/dns/#pods","title":"Pods","text":"<ul> <li>Pods have A/AAAA records.</li> <li>The DNS resolution is <code>pod-ip-address.my-namespace.pod.cluster-domain.example</code>.</li> <li>Pods exposed by a Service have additional DNS resolution.</li> </ul>"},{"location":"services-and-networking/dns/#pods-hostname-and-subdomain-fields","title":"Pod's hostname and subdomain fields","text":"<ul> <li>The hostname is by default the Pod's <code>metadata.name</code>.</li> <li>The hostname can be overridden by <code>spec.hostname</code>.</li> <li>The fully qualified domain name (FQDN) can be set using <code>spec.subdomain</code>.</li> </ul>"},{"location":"services-and-networking/dns/#pods-dns-policy","title":"Pod's DNS Policy","text":"<ul> <li>DNS policies can be set per-Pod.</li> <li>Options include <code>Default</code>, <code>ClusterFirst</code>, <code>ClusterFirstWithHostNet</code>, and <code>None</code>.</li> </ul>"},{"location":"services-and-networking/dns/#pods-dns-config","title":"Pod's DNS Config","text":"<ul> <li>Allows more control over DNS settings for a Pod.</li> <li>Can specify nameservers, searches, and options.</li> </ul>"},{"location":"services-and-networking/dns/#dns-search-domain-list-limits","title":"DNS search domain list limits","text":"<ul> <li>Kubernetes does not limit the DNS Config until the length of the search domain list exceeds 32 or the total length of all search domains exceeds 2048.</li> </ul>"},{"location":"services-and-networking/dns/#dns-resolution-on-windows-nodes","title":"DNS resolution on Windows nodes","text":"<ul> <li><code>ClusterFirstWithHostNet</code> is not supported on Windows nodes.</li> <li>Windows treats all names with a <code>.</code> as a FQDN and skips FQDN resolution.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/","title":"Endpoint Slices","text":""},{"location":"services-and-networking/endpoint-slices/#endpointslice-api","title":"EndpointSlice API","text":"<ul> <li>Provides a scalable and extensible way to track network endpoints in a Kubernetes cluster.</li> <li>Automatically created for any Kubernetes Service with a selector.</li> <li>Groups network endpoints by protocol, port number, and Service name.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#address-types","title":"Address Types","text":"<ul> <li>Supports IPv4, IPv6, and FQDN (Fully Qualified Domain Name).</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#conditions","title":"Conditions","text":"<ul> <li>Three conditions: <code>ready</code>, <code>serving</code>, and <code>terminating</code>:</li> <li><code>ready</code> maps to a Pod's Ready condition.</li> <li><code>serving</code> is for pod readiness during termination.</li> <li><code>terminating</code> indicates whether an endpoint is terminating.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#topology-information","title":"Topology Information","text":"<ul> <li>Includes the location of the endpoint, Node name, and zone.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#management","title":"Management","text":"<ul> <li>Mostly managed by the control plane's endpoint slice controller.</li> <li>Label <code>endpointslice.kubernetes.io/managed-by</code> indicates the entity managing an EndpointSlice.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#ownership","title":"Ownership","text":"<ul> <li>Owned by the Service they track endpoints for.</li> <li>Ownership indicated by an owner reference and a <code>kubernetes.io/service-name</code> label.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#endpointslice-mirroring","title":"EndpointSlice Mirroring","text":"<ul> <li>Mirrors custom Endpoints resources to EndpointSlices unless certain conditions are met.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#distribution-of-endpointslices","title":"Distribution of EndpointSlices","text":"<ul> <li>Tries to fill EndpointSlices as full as possible but does not actively rebalance them.</li> <li>Prioritizes limiting EndpointSlice updates over a perfectly full distribution.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#duplicate-endpoints","title":"Duplicate Endpoints","text":"<ul> <li>Endpoints may be represented in more than one EndpointSlice due to the nature of EndpointSlice changes.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#comparison-with-endpoints","title":"Comparison with Endpoints","text":"<ul> <li>EndpointSlices offer better scalability and extensibility compared to the original Endpoints API.</li> </ul>"},{"location":"services-and-networking/ingress/","title":"Ingress","text":""},{"location":"services-and-networking/ingress/#terminology","title":"Terminology","text":"<ul> <li>Edge Router: A router that enforces the firewall policy for your cluster.</li> <li>Cluster Network: A set of links, logical or physical, that facilitate communication within a cluster.</li> </ul>"},{"location":"services-and-networking/ingress/#what-is-ingress","title":"What is Ingress?","text":"<ul> <li>Manages external access to services within a cluster.</li> <li>Typically provides HTTP and HTTPS routes.</li> <li>Can provide load balancing, SSL termination, and name-based virtual hosting.</li> </ul>"},{"location":"services-and-networking/ingress/#prerequisites","title":"Prerequisites","text":"<ul> <li>Must have an Ingress controller to satisfy an Ingress.</li> <li>Basic workflow: Create an Ingress object -&gt; Ingress controller configures the load balancer.</li> </ul>"},{"location":"services-and-networking/ingress/#the-ingress-resource","title":"The Ingress Resource","text":"<ul> <li>Mainly composed of a set of rules based on hostnames and paths.</li> <li>API object that manages external access to services.</li> </ul>"},{"location":"services-and-networking/ingress/#ingress-rules","title":"Ingress Rules","text":"<ul> <li>Define how to route traffic by hostnames and paths.</li> <li>Each rule has one or more HTTP paths, each forwarding to a defined backend.</li> </ul>"},{"location":"services-and-networking/ingress/#defaultbackend","title":"DefaultBackend","text":"<ul> <li>An addressable Kubernetes Service to handle all requests not matching any path in the Ingress rules.</li> <li>Serves as a catch-all for undefined routes.</li> </ul>"},{"location":"services-and-networking/ingress/#resource-backends","title":"Resource Backends","text":"<ul> <li>A feature to forward traffic to resources other than Kubernetes Services.</li> <li>Can be used to route traffic to a custom resource.</li> </ul>"},{"location":"services-and-networking/ingress/#path-types","title":"Path Types","text":"<ul> <li>Defines how to match requests based on their paths.</li> <li>Types: <code>Exact</code>, <code>Prefix</code>, and <code>ImplementationSpecific</code>.</li> </ul>"},{"location":"services-and-networking/ingress/#hostname-wildcards","title":"Hostname Wildcards","text":"<ul> <li>Allows for the routing of HTTP traffic based on wildcards in hostnames.</li> <li>E.g., <code>.foo.com</code> routes to a specific service.</li> </ul>"},{"location":"services-and-networking/ingress/#ingress-class","title":"Ingress Class","text":"<ul> <li>Allows you to configure multiple Ingress controllers.</li> <li>Each controller is identified by a unique class.</li> </ul>"},{"location":"services-and-networking/ingress/#ingressclass-scope","title":"IngressClass Scope","text":"<ul> <li>Defines the scope of a particular Ingress class.</li> <li>Can be either cluster-wide or namespaced.</li> </ul>"},{"location":"services-and-networking/ingress/#deprecated-annotation","title":"Deprecated Annotation","text":"<ul> <li>Annotations for specifying ingress class are deprecated.</li> <li>Replaced by the ingressClassName field in the Ingress spec.</li> </ul>"},{"location":"services-and-networking/ingress/#default-ingressclass","title":"Default IngressClass","text":"<ul> <li>Specifies the ingress class to use when none is defined.</li> <li>Configured through a cluster-wide setting.</li> </ul>"},{"location":"services-and-networking/ingress/#types-of-ingress","title":"Types of Ingress","text":"<ul> <li>Single Service Ingress: Simplest kind, routes everything to one Service.</li> <li>Simple fanout: Routes traffic from a single IP address to more than one Service.</li> <li>Name-based virtual hosting: Routes traffic on multiple hostnames to different services.</li> </ul>"},{"location":"services-and-networking/ingress/#ingress-controllers","title":"Ingress Controllers","text":""},{"location":"services-and-networking/ingress/#introduction","title":"Introduction","text":"<ul> <li>Ingress controllers are essential for the functioning of an Ingress resource in a Kubernetes cluster.</li> <li>Unlike other controllers, Ingress controllers are not started automatically and must be set up manually.</li> </ul>"},{"location":"services-and-networking/ingress/#supported-controllers","title":"Supported Controllers","text":"<ul> <li>Kubernetes officially supports and maintains AWS, GCE, and nginx ingress controllers.</li> </ul>"},{"location":"services-and-networking/ingress/#additional-controllers","title":"Additional Controllers","text":"<ul> <li>Various third-party ingress controllers like AKS Application Gateway, Apache APISIX, Avi Kubernetes Operator, and many others are available.</li> </ul>"},{"location":"services-and-networking/ingress/#using-multiple-ingress-controllers","title":"Using Multiple Ingress Controllers","text":"<ul> <li>You can deploy multiple ingress controllers in a cluster using ingress class.</li> <li>The <code>.metadata.name</code> of the ingress class resource is important when creating an Ingress object.</li> </ul>"},{"location":"services-and-networking/ingress/#default-ingressclass_1","title":"Default IngressClass","text":"<ul> <li>If an Ingress object doesn't specify an IngressClass and there's exactly one IngressClass marked as default, Kubernetes applies the default IngressClass.</li> <li>An IngressClass is marked as default by setting the i<code>ngressclass.kubernetes.io/is-default-class</code> annotation to <code>true</code>.</li> </ul>"},{"location":"services-and-networking/ingress/#controller-specifications","title":"Controller Specifications","text":"<ul> <li>While all ingress controllers should ideally fulfill the Kubernetes specification, they may operate differently.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/","title":"IPv4 - IPv6","text":""},{"location":"services-and-networking/ipv4-ipv6/#ipv4ipv6-dual-stack-networking","title":"IPv4/IPv6 dual-stack networking","text":"<ul> <li>Enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.</li> <li>Enabled by default for Kubernetes clusters starting in version 1.21.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#supported-features","title":"Supported Features","text":"<ul> <li>Dual-stack Pod networking</li> <li>IPv4 and IPv6 enabled Services</li> <li>Pod off-cluster egress routing via both IPv4 and IPv6 interfaces</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.20 or later</li> <li>Provider support for dual-stack networking</li> <li>A network plugin that supports dual-stack networking</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#configure-ipv4ipv6-dual-stack","title":"Configure IPv4/IPv6 dual-stack","text":"<ul> <li>Various flags for kube-apiserver, kube-controller-manager, kube-proxy, and kubelet to set dual-stack cluster network assignments.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#services","title":"Services","text":"<ul> <li>Can use IPv4, IPv6, or both.</li> <li><code>.spec.ipFamilyPolicy</code> can be set to <code>SingleStack</code>, <code>PreferDualStack</code>, or <code>RequireDualStack</code>.</li> <li><code>.spec.ipFamilies</code> can be set to define the order of IP families for dual-stack.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#dual-stack-service-configuration-scenarios","title":"Dual-stack Service configuration scenarios","text":"<ul> <li>Examples provided for various dual-stack Service configurations.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#dual-stack-defaults-on-existing-services","title":"Dual-stack defaults on existing Services","text":"<ul> <li>Existing Services are configured by the control plane to set <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code>.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#service-type-loadbalancer","title":"Service type LoadBalancer","text":"<ul> <li>To provision a dual-stack load balancer, set <code>.spec.type</code> to <code>LoadBalancer</code> and <code>.spec.ipFamilyPolicy</code> to <code>PreferDualStack</code> or <code>RequireDualSactk</code>.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#egress-traffic","title":"Egress traffic","text":"<ul> <li>Enable egress traffic for Pods using non-publicly routable IPv6 addresses through mechanisms like transparent proxying or IP masquerading.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#windows-support","title":"Windows support","text":"<ul> <li>Dual-stack IPv4/IPv6 networking is supported for pods and nodes with single-family services on Windows.</li> </ul>"},{"location":"services-and-networking/network-overview/","title":"Overview","text":""},{"location":"services-and-networking/network-overview/#the-kubernetes-network-model","title":"The Kubernetes Network Model","text":"<ul> <li>Every Pod gets a unique cluster-wide IP address.</li> <li>Pods can communicate with all other pods on any node without NAT.</li> <li>Agents on a node can communicate with all pods on that node.</li> <li>\"IP-per-pod\" model: Containers within a Pod share their network namespaces, including their IP and MAC addresses.</li> </ul>"},{"location":"services-and-networking/network-overview/#kubernetes-networking-concerns","title":"Kubernetes Networking Concerns","text":"<ul> <li>Containers within a Pod communicate via loopback.</li> <li>Cluster networking enables communication between different Pods.</li> <li>The Service API exposes applications running in Pods to be reachable from outside the cluster.</li> <li>Ingress provides extra functionality for exposing HTTP applications, websites, and APIs.</li> </ul>"},{"location":"services-and-networking/network-overview/#service","title":"Service","text":"<ul> <li>Exposes an application running in the cluster behind a single outward-facing endpoint.</li> </ul>"},{"location":"services-and-networking/network-overview/#ingress","title":"Ingress","text":"<ul> <li>Makes HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism.</li> </ul>"},{"location":"services-and-networking/network-overview/#ingress-controllers","title":"Ingress Controllers","text":"<ul> <li>Required for an Ingress to work in the cluster.</li> </ul>"},{"location":"services-and-networking/network-overview/#endpointslices","title":"EndpointSlices","text":"<ul> <li>Allows the Service to scale to handle large numbers of backends.</li> </ul>"},{"location":"services-and-networking/network-overview/#network-policies","title":"Network Policies","text":"<ul> <li>Controls traffic flow at the IP address or port level.</li> </ul>"},{"location":"services-and-networking/network-overview/#dns-for-services-and-pods","title":"DNS for Services and Pods","text":"<ul> <li>Workloads can discover Services within the cluster using DNS.</li> </ul>"},{"location":"services-and-networking/network-overview/#ipv4ipv6-dual-stack","title":"IPv4/IPv6 dual-stack","text":"<ul> <li>Supports single-stack IPv4, single-stack IPv6, or dual-stack networking.</li> </ul>"},{"location":"services-and-networking/network-overview/#topology-aware-routing","title":"Topology Aware Routing","text":"<ul> <li>Helps keep network traffic within the zone where it originated.</li> </ul>"},{"location":"services-and-networking/network-overview/#service-internal-traffic-policy","title":"Service Internal Traffic Policy","text":"<ul> <li>Keeps network traffic within the node if two Pods in the cluster are running on the same node.</li> </ul>"},{"location":"services-and-networking/network-policies/","title":"Network Policies","text":""},{"location":"services-and-networking/network-policies/#the-two-sorts-of-pod-isolation","title":"The Two Sorts of Pod Isolation","text":"<ol> <li>Namespace isolation: Isolating whole namespaces from one another.</li> <li>Pod isolation: More granular control, isolating individual pods.</li> </ol>"},{"location":"services-and-networking/network-policies/#the-networkpolicy-resource","title":"The NetworkPolicy Resource","text":"<ul> <li>Defines how pods are allowed to communicate with each other and other network endpoints.</li> <li>PodSelector targets Pods to apply the policy.</li> <li>PolicyTypes specifies what types of traffic are affected.</li> </ul>"},{"location":"services-and-networking/network-policies/#behavior-of-to-and-from-selectors","title":"Behavior of to and from Selectors","text":"<ul> <li>ingress and egress rules can be set.</li> <li>Rules can be as specific as \"allow traffic from these IPs\" or \"allow traffic from Pods with these labels.\"</li> </ul>"},{"location":"services-and-networking/network-policies/#default-policies","title":"Default Policies","text":"<ul> <li>Policies that apply when no other policies do.</li> </ul>"},{"location":"services-and-networking/network-policies/#default-deny-all-ingress-traffic","title":"Default Deny All Ingress Traffic","text":"<ul> <li>Blocks all incoming traffic to Pods unless it matches a NetworkPolicy.</li> </ul>"},{"location":"services-and-networking/network-policies/#allow-all-ingress-traffic","title":"Allow All Ingress Traffic","text":"<ul> <li>Allows all incoming traffic to Pods.</li> </ul>"},{"location":"services-and-networking/network-policies/#default-deny-all-egress-traffic","title":"Default Deny All Egress Traffic","text":"<ul> <li>Blocks all outgoing traffic from Pods unless it matches a NetworkPolicy.</li> </ul>"},{"location":"services-and-networking/network-policies/#allow-all-egress-traffic","title":"Allow All Egress Traffic","text":"<ul> <li>Allows all outgoing traffic from Pods.</li> </ul>"},{"location":"services-and-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic","title":"Default Deny All Ingress and All Egress Traffic","text":"<ul> <li>Blocks both incoming and outgoing traffic unless they match a NetworkPolicy.</li> </ul>"},{"location":"services-and-networking/network-policies/#sctp-support","title":"SCTP Support","text":"<ul> <li>SCTP (Stream Control Transmission Protocol) is supported as a protocol alongside TCP and UDP.</li> </ul>"},{"location":"services-and-networking/network-policies/#targeting-a-range-of-ports","title":"Targeting a Range of Ports","text":"<ul> <li>NetworkPolicy can target a range of ports instead of a single port.</li> </ul>"},{"location":"services-and-networking/network-policies/#targeting-multiple-namespaces-by-label","title":"Targeting Multiple Namespaces by Label","text":"<ul> <li>NetworkPolicy can target multiple namespaces using namespace labels.</li> </ul>"},{"location":"services-and-networking/network-policies/#targeting-a-namespace-by-its-name","title":"Targeting a Namespace by its Name","text":"<ul> <li>Specific namespaces can be targeted by their name.</li> </ul>"},{"location":"services-and-networking/network-policies/#what-you-cant-do-with-network-policies-at-least-not-yet","title":"What You Can't Do with Network Policies (at least, not yet)","text":"<ul> <li>Limitations like not being able to enforce egress based on DNS names, or not being able to limit access based on the protocol's fields.</li> </ul>"},{"location":"services-and-networking/service-networking/","title":"Service Networking","text":""},{"location":"services-and-networking/service-networking/#introduction","title":"Introduction","text":"<ul> <li>Services expose applications running on a set of Pods.</li> <li>Services can have a cluster-scoped virtual IP address (ClusterIP).</li> <li>Clients connect using the virtual IP, and Kubernetes load-balances the traffic.</li> </ul>"},{"location":"services-and-networking/service-networking/#how-clusterips-are-allocated","title":"How ClusterIPs are Allocated","text":"<ul> <li>Dynamically: The control plane picks a free IP address from the configured IP range.</li> <li>Statically: You can specify an IP address within the configured IP range.</li> </ul>"},{"location":"services-and-networking/service-networking/#uniqueness-of-clusterip","title":"Uniqueness of ClusterIP","text":"<ul> <li>Every Service ClusterIP must be unique across the cluster.</li> <li>Creating a Service with an already allocated ClusterIP will result in an error.</li> </ul>"},{"location":"services-and-networking/service-networking/#why-reserve-clusterips","title":"Why Reserve ClusterIPs","text":"<ul> <li>For well-known IP addresses that other components and users in the cluster can use.</li> <li>Example: DNS Service in the cluster may use a well-known IP.</li> </ul>"},{"location":"services-and-networking/service-networking/#avoiding-clusterip-conflicts","title":"Avoiding ClusterIP Conflicts","text":"<ul> <li>Kubernetes has an allocation strategy to reduce the risk of collision.</li> <li>The ClusterIP range is divided based on a formula.</li> <li>Dynamic IP assignment uses the upper band by default.  </li> </ul> <p>For the KCAD exam, understanding how ClusterIPs are allocated, both dynamically and statically, is crucial. Also, knowing how to avoid conflicts and the uniqueness constraint can be vital.</p>"},{"location":"services-and-networking/service-networking/#service-internal-traffic-policy","title":"Service Internal Traffic Policy","text":""},{"location":"services-and-networking/service-networking/#key-points","title":"Key Points:","text":"<ul> <li>Feature State: Available in Kubernetes v1.26 as stable.</li> <li>Purpose: Allows internal traffic restrictions to only route internal traffic to endpoints within the originating node. This is useful for reducing costs and improving performance.</li> <li>Configuration: You can enable this feature by setting .spec.internalTrafficPolicy to Local in the Service specification.</li> <li>This instructs kube-proxy to only use node-local endpoints for cluster-internal traffic.</li> </ul> <pre><code>    apiVersion: v1\n    kind: Service\n    metadata:\n      name: my-service\n    spec:\n      selector:\n        app.kubernetes.io/name: MyApp\n      ports:\n        protocol: TCP\n          port: 80\n          targetPort: 9376\n      internalTrafficPolicy: Local\n</code></pre>"},{"location":"services-and-networking/service-networking/#how-it-works","title":"How it Works","text":"<ul> <li>The kube-proxy filters the endpoints based on the <code>.spec.internalTrafficPolicy</code> setting. When set to <code>Local</code>, only node-local endpoints - are considered. When set to <code>Cluster</code> (the default), or not set, all endpoints are considered.</li> </ul>"},{"location":"services-and-networking/service/","title":"Services","text":"<p>What is a Service? A Kubernetes Service is an abstraction layer that defines a logical set of Pods and enables external traffic exposure, load balancing, and service discovery.</p>"},{"location":"services-and-networking/service/#types-of-services","title":"Types of Services","text":"<ol> <li> <p>ClusterIP:</p> <ul> <li>Default type.</li> <li>Exposes the service on an internal IP in the cluster.</li> <li>Accessible only within the cluster.</li> </ul> </li> <li> <p>NodePort:</p> <ul> <li>Exposes the service on a static port on each Node\u2019s IP.</li> <li>External entities can access it by <code>:</code>.</li> </ul> </li> <li> <p>LoadBalancer:</p> <ul> <li>Provisions an external load balancer and assigns a fixed, external IP to the service.</li> <li>Typically used in cloud environments.</li> </ul> </li> <li> <p>ExternalName:</p> <ul> <li>Maps the service to the contents of the externalName field (e.g., <code>foo.bar.example.com</code>).</li> </ul> </li> </ol>"},{"location":"services-and-networking/service/#service-discovery","title":"Service Discovery","text":"<ul> <li>Services are discoverable through environment variables or DNS.</li> <li>The kube-dns component handles DNS-based service discovery.</li> </ul>"},{"location":"services-and-networking/service/#selector-and-labels","title":"Selector and Labels","text":"<ul> <li>Services route traffic to Pods based on label selectors.</li> </ul>"},{"location":"services-and-networking/service/#ports","title":"Ports","text":"<ul> <li>You can specify <code>port</code> (port exposed by the service), <code>targetPort</code> (port on the Pod), and <code>nodePort</code> (port on the node).</li> </ul>"},{"location":"services-and-networking/service/#endpoints","title":"Endpoints","text":"<ul> <li>Services have associated Endpoints that contain the IP addresses of the Pods the traffic should be routed to.</li> </ul>"},{"location":"services-and-networking/service/#session-affinity","title":"Session Affinity","text":"<ul> <li>Services support <code>None</code> and <code>ClientIP</code> session affinity to maintain session state.</li> </ul>"},{"location":"services-and-networking/service/#service-account","title":"Service Account","text":"<ul> <li>You can associate a service account to control the level of access a service has.</li> </ul>"},{"location":"services-and-networking/service/#headless-services","title":"Headless Services","text":"<ul> <li>Services without a ClusterIP for direct Pod-to-Pod communication.</li> </ul>"},{"location":"services-and-networking/service/#service-topology","title":"Service Topology","text":"<ul> <li>Allows routing of traffic based on Node labels.</li> </ul>"},{"location":"services-and-networking/service/#dual-stack-services","title":"Dual-Stack Services","text":"<ul> <li>Services can be IPv4/IPv6 dual-stack enabled for hybrid communication.</li> </ul>"},{"location":"services-and-networking/service/#quality-of-service-qos","title":"Quality of Service (QoS)","text":"<ul> <li>Services don't have QoS guarantees but the Pods backing them can have QoS classes like <code>Guaranteed</code>, <code>Burstable</code>, and <code>BestEffort</code>.</li> </ul>"},{"location":"services-and-networking/service/#service-mesh","title":"Service Mesh","text":"<ul> <li>Istio or Linkerd can be used for advanced service-to-service communication features like canary deployments, circuit breakers, etc.</li> </ul>"},{"location":"services-and-networking/topolgy-aware-routing/","title":"Topolgy aware routing","text":""},{"location":"services-and-networking/topolgy-aware-routing/#motivation","title":"Motivation","text":"<ul> <li>Designed for multi-zone environments.</li> <li>Aims to keep network traffic within the originating zone for reliability, performance, and cost.</li> </ul>"},{"location":"services-and-networking/topolgy-aware-routing/#enabling-topology-aware-routing","title":"Enabling Topology Aware Routing","text":"<ul> <li>Enabled by setting the <code>service.kubernetes.io/topology-mode</code> annotation to <code>Auto</code>.</li> <li>EndpointSlices will have Topology Hints populated to allocate endpoints to specific zones.</li> </ul>"},{"location":"services-and-networking/topolgy-aware-routing/#when-it-works-best","title":"When it works best","text":"<ul> <li>Even distribution of incoming traffic.</li> <li>Service has 3 or more endpoints per zone.</li> </ul>"},{"location":"services-and-networking/topolgy-aware-routing/#how-it-works","title":"How It Works","text":"<ul> <li>\"Auto\" heuristic proportionally allocates endpoints to each zone.</li> <li>EndpointSlice controller sets hints based on allocatable CPU cores in each zone.</li> </ul>"},{"location":"services-and-networking/topolgy-aware-routing/#endpointslice-controller","title":"EndpointSlice controller","text":"<ul> <li>Responsible for setting hints on EndpointSlices.</li> <li>Allocates endpoints based on the allocatable CPU cores for nodes in each zone.</li> </ul>"},{"location":"services-and-networking/topolgy-aware-routing/#kube-proxy","title":"kube-proxy","text":"<ul> <li>Filters endpoints based on hints set by the EndpointSlice controller.</li> <li>Sometimes allocates endpoints from different zones for even distribution.</li> </ul>"},{"location":"services-and-networking/topolgy-aware-routing/#safeguards","title":"Safeguards","text":"<ul> <li>Several rules to ensure safe use of Topology Aware Hints.</li> <li>If rules aren't met, <code>kube-proxy</code> selects endpoints from anywhere in the cluster.</li> </ul>"},{"location":"services-and-networking/topolgy-aware-routing/#constraints","title":"Constraints","text":"<ul> <li>Not used when <code>internalTrafficPolicy</code> is set to <code>Local</code>.</li> <li>Does not work well for Services with traffic originating from a subset of zones.</li> <li>Does not account for unready nodes or nodes with specific labels.</li> </ul>"},{"location":"services-and-networking/topolgy-aware-routing/#custom-heuristics","title":"Custom heuristics","text":"<ul> <li>Allows for custom heuristics if the built-in ones don't meet specific use cases.</li> </ul>"},{"location":"storage/csi-volume-cloning/","title":"CSI Volume Cloning","text":""},{"location":"storage/csi-volume-cloning/#introduction","title":"Introduction","text":"<p>The document introduces the concept of cloning existing Container Storage Interface (CSI) Volumes in Kubernetes. The feature allows you to specify existing Persistent Volume Claims (PVCs) in the <code>dataSource</code> field to indicate that you want to clone a volume. A clone is essentially a duplicate of an existing Kubernetes volume that behaves like any standard volume. The key difference is that upon provisioning, the backend device creates an exact duplicate of the specified volume instead of a new empty one.</p>"},{"location":"storage/csi-volume-cloning/#implementation","title":"Implementation","text":"<p>From the Kubernetes API perspective, cloning is implemented by allowing you to specify an existing PVC as a <code>dataSource</code> during new PVC creation. The source PVC must be bound and available, meaning it should not be in use.</p>"},{"location":"storage/csi-volume-cloning/#user-considerations","title":"User Considerations","text":"<ul> <li>Cloning support is only available for CSI drivers.</li> <li>Only dynamic provisioners support cloning.</li> <li>CSI drivers may or may not have implemented volume cloning.</li> <li>Cloning can only be done within the same namespace for both source and destination PVCs.</li> <li>Cloning is supported with different Storage Classes.</li> <li>The destination volume can have the same or a different storage class as the source.</li> <li>The default storage class can be used, and storageClassName can be omitted in the spec.</li> <li>Cloning can only be done between two volumes that use the same <code>VolumeMode</code> setting.</li> </ul>"},{"location":"storage/csi-volume-cloning/#provisioning","title":"Provisioning","text":"<p>Clones are provisioned like any other PVC, except that a <code>dataSource</code> is added that references an existing PVC in the same namespace. The document provides a YAML example for creating a new PVC that is a clone of an existing one.</p>"},{"location":"storage/csi-volume-cloning/#usage","title":"Usage","text":"<p>Once the new PVC is available, it can be consumed like any other PVC. It becomes an independent object that can be consumed, cloned, snapshotted, or deleted independently. The source is not linked to the newly created clone in any way, allowing for modifications or deletions without affecting the clone.</p>"},{"location":"storage/dynamic-volume-provisioning/","title":"Dynamic Volume Provisioning","text":""},{"location":"storage/dynamic-volume-provisioning/#overview","title":"Overview","text":"<ul> <li>Dynamic volume provisioning allows for the on-demand creation of storage volumes. This eliminates the need for cluster administrators to manually create storage volumes and their corresponding PersistentVolume objects in Kubernetes. The feature is based on the API object <code>StorageClass</code> from the API group <code>storage.k8s.io</code>.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#background","title":"Background","text":"<ul> <li>A cluster administrator can define multiple <code>StorageClass</code> objects, each specifying a volume plugin (provisioner) and the parameters to pass to that provisioner. This allows for the exposure of multiple types of storage within a cluster, each with custom parameters. This design abstracts the complexity of storage provisioning from end-users, allowing them to choose from multiple storage options.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#enabling-dynamic-provisioning","title":"Enabling Dynamic Provisioning","text":"<ul> <li>To enable this feature, a cluster administrator must pre-create one or more <code>StorageClass</code> objects. These objects define which provisioner should be used and what parameters should be passed when dynamic provisioning is invoked. For example, a storage class named \"slow\" might provision standard disk-like persistent disks, while a storage class named \"fast\" might provision SSD-like persistent disks.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#using-dynamic-provisioning","title":"Using Dynamic Provisioning","text":"<ul> <li>Users can request dynamically provisioned storage by including a storage class in their <code>PersistentVolumeClaim</code>. The <code>storageClassName</code> field of the <code>PersistentVolumeClaim</code> object must match the name of a <code>StorageClass</code> configured by the administrator. For instance, to select the \"fast\" storage class, a user would specify <code>storageClassName: fast</code> in their claim.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#defaulting-behavior","title":"Defaulting Behavior","text":"<ul> <li>Dynamic provisioning can be enabled such that all claims are dynamically provisioned if no storage class is specified. This is achieved by marking one <code>StorageClass</code> object as the default and ensuring that the <code>DefaultStorageClass</code> admission controller is enabled on the API server.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#topology-awareness","title":"Topology Awareness","text":"<ul> <li>In Multi-Zone clusters, it's important that storage backends are provisioned in the Zones where Pods are scheduled. This ensures that Pods can be spread across Zones in a Region while still having access to the appropriate storage.</li> </ul>"},{"location":"storage/ephemeral-volumes/","title":"Ephemeral Volumes","text":""},{"location":"storage/ephemeral-volumes/#introduction","title":"Introduction","text":"<ul> <li>Ephemeral volumes are designed for temporary storage needs.</li> <li>They follow the Pod's lifetime and are created and deleted along with the Pod.</li> <li>Useful for caching services and read-only input data like configuration or secret keys.</li> </ul>"},{"location":"storage/ephemeral-volumes/#types-of-ephemeral-volumes","title":"Types of Ephemeral Volumes","text":"<ul> <li><code>emptyDir</code>: Empty at Pod startup, storage from kubelet base directory or RAM.</li> <li><code>configMap</code>, <code>downwardAPI</code>, <code>secret</code>: Inject Kubernetes data into a Pod.</li> <li><code>CSI ephemeral</code> volumes: Provided by special CSI drivers.</li> <li>Generic ephemeral volumes: Can be provided by any storage driver that supports dynamic provisioning.</li> </ul>"},{"location":"storage/ephemeral-volumes/#csi-ephemeral-volumes","title":"CSI Ephemeral Volumes","text":"<ul> <li>Managed locally on each node.</li> <li>Created after a Pod has been scheduled onto a node.</li> <li>No concept of rescheduling Pods.</li> <li>Not covered by storage resource usage limits of a Pod.</li> </ul>"},{"location":"storage/ephemeral-volumes/#generic-ephemeral-volumes","title":"Generic Ephemeral Volumes","text":"<ul> <li>Similar to emptyDir but may have additional features like fixed size, initial data, etc.</li> <li>Supports typical volume operations like snapshotting, cloning, resizing, and storage capacity tracking.</li> </ul>"},{"location":"storage/ephemeral-volumes/#lifecycle-and-persistentvolumeclaim","title":"Lifecycle and PersistentVolumeClaim","text":"<ul> <li>PVC parameters are allowed inside a volume source of the Pod.</li> <li>When the Pod is created, an actual PVC object is created and deleted along with the Pod.</li> <li>PVCs can be used like any other PVCs, including as data sources in volume cloning or snapshotting.</li> </ul>"},{"location":"storage/ephemeral-volumes/#security-considerations","title":"Security Considerations","text":"<ul> <li>Allows users to create PVCs indirectly.</li> <li>Normal namespace quota for PVCs still applies.</li> </ul>"},{"location":"storage/node-specific-volume-limits/","title":"Node Specific Volume Limits","text":""},{"location":"storage/node-specific-volume-limits/#kubernetes-default-limits","title":"Kubernetes Default Limits","text":"<ul> <li>Amazon Elastic Block Store (EBS): 39 volumes per Node</li> <li>Google Persistent Disk: 16 volumes per Node</li> <li>Microsoft Azure Disk Storage: 16 volumes per Node</li> </ul>"},{"location":"storage/node-specific-volume-limits/#custom-limits","title":"Custom Limits","text":"<p>You can customize these limits by setting the value of the <code>KUBE_MAX_PD_VOLS</code> environment variable and then restarting the scheduler. For CSI drivers, you may need to consult their specific documentation for customization procedures.</p>"},{"location":"storage/node-specific-volume-limits/#dynamic-volume-limits","title":"Dynamic Volume Limits","text":"<p>As of Kubernetes v1.17, dynamic volume limits are supported for Amazon EBS, Google Persistent Disk, Azure Disk, and CSI. Kubernetes automatically determines the Node type and enforces the appropriate maximum number of volumes for that node. For example: - On Google Compute Engine, up to 127 volumes can be attached, depending on the node type. - For Amazon EBS disks on M5, C5, R5, T3, and Z1D instance types, only 25 volumes can be attached. - On Azure, up to 64 disks can be attached, depending on the node type.</p>"},{"location":"storage/node-specific-volume-limits/#csi-driver-limits","title":"CSI Driver Limits","text":"<p>If a CSI storage driver advertises a maximum number of volumes for a Node, the kube-scheduler will honor that limit.</p>"},{"location":"storage/persistent-volumes/","title":"Persistent Volumes","text":""},{"location":"storage/persistent-volumes/#introduction","title":"Introduction","text":"<ul> <li>PVs are abstraction layers for storage in Kubernetes.</li> <li>PVCs are requests for PV resources by pods.</li> <li>PVs are cluster-wide and can be used by multiple pods.</li> </ul>"},{"location":"storage/persistent-volumes/#lifecycle-of-a-volume-and-claim","title":"Lifecycle of a Volume and Claim","text":""},{"location":"storage/persistent-volumes/#provisioning","title":"Provisioning","text":"<ul> <li>PVs can be provisioned statically or dynamically.</li> <li>Dynamic provisioning relies on StorageClasses.</li> <li>StorageClasses define provisioning mechanisms (e.g., AWS EBS, GCE PD).</li> </ul>"},{"location":"storage/persistent-volumes/#binding","title":"Binding","text":"<ul> <li>PVCs are bound to suitable PVs based on labels, storage capacity, and access modes.</li> </ul>"},{"location":"storage/persistent-volumes/#using","title":"Using","text":"<ul> <li>Pods specify PVCs in their volume specifications.</li> <li>Multiple pods can use the same PVC, but only one pod can mount it in <code>ReadWrite</code> mode at a time.</li> </ul>"},{"location":"storage/persistent-volumes/#reclaiming","title":"Reclaiming","text":"<ul> <li>PVs can be retained, recycled, or deleted after PVC release.</li> <li>Retain: PV data is preserved.</li> <li>Recycle: Data is deleted and PV can be reused.</li> <li>Delete: PV is deleted along with data.</li> </ul>"},{"location":"storage/persistent-volumes/#storage-object-in-use-protection","title":"Storage Object in Use Protection","text":"<ul> <li>PVs with bound PVCs have a finalizer to prevent accidental deletion.</li> <li>Ensures data safety while PVCs are in use.</li> </ul>"},{"location":"storage/persistent-volumes/#reclaiming_1","title":"Reclaiming","text":"<ul> <li>Defines PV's behavior after PVC release.</li> <li>Options include Retain, Recycle, and Delete.</li> <li>Appropriate setting depends on use case.</li> </ul>"},{"location":"storage/persistent-volumes/#persistentvolume-claims","title":"PersistentVolume Claims","text":"<ul> <li>PVCs request storage resources.</li> <li>They specify access modes (<code>ReadWriteOnce</code>, <code>ReadOnlyMany</code>, <code>ReadWriteMany</code>), resource requests, and StorageClass.</li> <li>Reference a StorageClass to dynamically provision PVs.</li> </ul>"},{"location":"storage/persistent-volumes/#access-modes","title":"Access Modes","text":"<ul> <li><code>ReadWriteOnce</code>: Can be mounted as read-write by a single node.</li> <li><code>ReadOnlyMany</code>: Can be mounted read-only by many nodes.</li> <li><code>ReadWriteMany</code>: Can be mounted as read-write by many nodes.</li> </ul>"},{"location":"storage/persistent-volumes/#volume-modes","title":"Volume Modes","text":"<ul> <li>PVCs can specify volume modes:</li> <li>Filesystem: Usual file-based volumes.</li> <li>Block: Raw block devices.</li> </ul>"},{"location":"storage/persistent-volumes/#resources","title":"Resources","text":"<ul> <li>PVCs request storage capacity (e.g., 1Gi) and StorageClass.</li> <li>Helps in selecting an appropriate PV.</li> </ul>"},{"location":"storage/persistent-volumes/#selector","title":"Selector","text":"<ul> <li>PVCs can use selectors to filter PVs based on labels and annotations.</li> <li>Useful for matching specific criteria.</li> </ul>"},{"location":"storage/persistent-volumes/#class","title":"Class","text":"<ul> <li>StorageClass defines storage type (e.g., SSD, HDD) and provisioning.</li> <li>PVCs reference a StorageClass to request storage.</li> </ul>"},{"location":"storage/persistent-volumes/#claims-as-volumes","title":"Claims As Volumes","text":"<ul> <li>Pods can consume PVCs as volumes.</li> <li>Allows dynamic provisioning based on pod requirements.</li> </ul>"},{"location":"storage/persistent-volumes/#raw-block-volume-support","title":"Raw Block Volume Support","text":"<ul> <li>Kubernetes supports raw block volumes for high-performance workloads.</li> <li>PVCs request raw block volumes.</li> <li>Useful for databases and applications needing low-level access.</li> </ul>"},{"location":"storage/persistent-volumes/#volume-snapshot-and-restore","title":"Volume Snapshot and Restore","text":"<ul> <li>Kubernetes supports volume snapshots and restoration.</li> <li>Users can create, clone, and restore volumes from snapshots.</li> </ul>"},{"location":"storage/persistent-volumes/#volume-cloning","title":"Volume Cloning","text":"<ul> <li>Enables creating PVCs from existing PV data.</li> <li>Useful for scaling applications or creating replicas.</li> </ul>"},{"location":"storage/persistent-volumes/#volume-populators-and-data-sources","title":"Volume Populators and Data Sources","text":"<ul> <li>Populators enable dynamic provisioning from data sources.</li> <li>Data sources can be external data or other PVCs.</li> </ul>"},{"location":"storage/persistent-volumes/#cross-namespace-data-sources","title":"Cross-Namespace Data Sources","text":"<ul> <li>Data sources can be referenced across namespaces.</li> <li>Enhances flexibility in PVC usage.</li> </ul>"},{"location":"storage/persistent-volumes/#data-source-references","title":"Data Source References","text":"<ul> <li>PVCs can reference data sources to create volumes.</li> <li>Supports various volume types and scenarios.</li> </ul>"},{"location":"storage/persistent-volumes/#using-volume-populators","title":"Using Volume Populators","text":"<ul> <li>Populators facilitate on-demand provisioning based on data sources.</li> <li>Ideal for dynamic storage allocation.</li> </ul>"},{"location":"storage/persistent-volumes/#using-a-cross-namespace-volume-data-source","title":"Using a Cross-Namespace Volume Data Source","text":"<ul> <li>Data sources can be referenced from different namespaces.</li> <li>Enables sharing data sources across projects.</li> </ul>"},{"location":"storage/projected-volumes/","title":"Projected Volumes","text":""},{"location":"storage/projected-volumes/#introduction","title":"Introduction","text":"<ul> <li>Projected volumes map multiple existing volume sources into a single directory.</li> <li>Supported volume sources: <code>secret</code>, <code>downwardAPI</code>, <code>configMap</code>, <code>serviceAccountToken</code>.</li> <li>All sources must be in the same namespace as the Pod.</li> </ul>"},{"location":"storage/projected-volumes/#example-configuration-secret-downwardapi-configmap","title":"Example Configuration: Secret, DownwardAPI, ConfigMap","text":"<ul> <li>Demonstrates how to combine <code>secret</code>, <code>downwardAPI</code>, and <code>configMap</code> in a single Pod.</li> <li>Uses <code>apiVersion: v1, kind: Pod</code>, and specifies volume sources under projected.sources.</li> </ul>"},{"location":"storage/projected-volumes/#example-configuration-non-default-permission-mode","title":"Example Configuration: Non-Default Permission Mode","text":"<ul> <li>Shows how to set a non-default permission mode for secrets.</li> <li>Uses mode: 511 to set specific permissions for the secret.</li> </ul>"},{"location":"storage/projected-volumes/#serviceaccounttoken-projected-volumes","title":"ServiceAccountToken Projected Volumes","text":"<ul> <li>Allows injecting the token for the current service account into a Pod.</li> <li>Fields:<ul> <li>audience: Intended audience of the token (optional).</li> <li>expirationSeconds: Token validity duration, at least 10 minutes.</li> <li>path: Relative path to the mount point.</li> </ul> </li> </ul>"},{"location":"storage/projected-volumes/#securitycontext-interactions","title":"SecurityContext Interactions","text":""},{"location":"storage/projected-volumes/#linux","title":"Linux","text":"<ul> <li>Projected files have correct ownership, including container user ownership.</li> </ul>"},{"location":"storage/projected-volumes/#windows","title":"Windows","text":"<ul> <li>Ownership is not enforced due to virtual SAM database in each container.</li> <li>Recommended to place shared files in their own volume mount outside of <code>C:\\\\</code>.</li> </ul>"},{"location":"storage/storage-capacity/","title":"Storage Capacity","text":""},{"location":"storage/storage-capacity/#feature-state","title":"Feature State","text":"<ul> <li>The feature is stable as of Kubernetes v1.24. It helps Kubernetes keep track of storage capacity and aids the scheduler in placing Pods on nodes with sufficient storage.</li> </ul>"},{"location":"storage/storage-capacity/#before-you-begin","title":"Before You Begin","text":"<ul> <li>To utilize storage capacity tracking, you must be running Kubernetes v1.28 or above and use a CSI driver that supports this feature.</li> </ul>"},{"location":"storage/storage-capacity/#api-extensions","title":"API Extensions","text":"<ul> <li><code>CSIStorageCapacity</code> objects: Created by a CSI driver in its namespace, each object contains capacity information for one storage class and specifies which nodes can access that storage.</li> <li><code>CSIDriverSpec.StorageCapacity</code> field: When set to true, the Kubernetes scheduler considers storage capacity for volumes using the CSI driver.</li> </ul>"},{"location":"storage/storage-capacity/#scheduling","title":"Scheduling","text":"<ul> <li>The scheduler uses storage capacity information if:</li> <li>A Pod uses a yet-to-be-created volume.</li> <li>The volume uses a StorageClass that references a CSI driver and uses <code>WaitForFirstConsumer</code> volume binding mode.</li> <li>The CSIDriver object for the driver has StorageCapacity set to true.</li> <li>In this case, the scheduler only considers nodes with enough storage. The check is basic and compares the volume size against the capacity listed in CSIStorageCapacity objects that include the node.</li> </ul>"},{"location":"storage/storage-capacity/#rescheduling","title":"Rescheduling","text":"<ul> <li>Node selection is tentative until the CSI driver confirms the volume creation. If the volume can't be created due to outdated capacity information, the scheduler retries.</li> </ul>"},{"location":"storage/storage-capacity/#limitations","title":"Limitations","text":"<ul> <li>Scheduling can fail permanently if a Pod uses multiple volumes and one volume consumes all the available capacity in a topology segment.</li> <li>The feature increases the chance of successful scheduling but doesn't guarantee it due to potentially outdated information.</li> </ul>"},{"location":"storage/storage-classes/","title":"Storage Classes","text":""},{"location":"storage/storage-classes/#introduction","title":"Introduction","text":"<ul> <li>StorageClass in Kubernetes allows administrators to define different \"classes\" of storage. These classes can represent various quality-of-service levels, backup policies, or any arbitrary policies set by the administrators. Kubernetes doesn't enforce what these classes should represent.</li> </ul>"},{"location":"storage/storage-classes/#the-storageclass-resource","title":"The StorageClass Resource","text":"<ul> <li>A StorageClass contains fields like provisioner, parameters, and reclaimPolicy. These are used when dynamically provisioning a</li> <li>PersistentVolume (PV) that belongs to the class. The name of the StorageClass is significant and is used by users to request a specific class. Administrators can also set a default StorageClass for PVCs that don't specify any class.</li> </ul>"},{"location":"storage/storage-classes/#default-storageclass","title":"Default StorageClass","text":"<ul> <li>If a PersistentVolumeClaim (PVC) doesn't specify a storageClassName, the cluster's default StorageClass is used. Only one default StorageClass can exist in a cluster.</li> </ul>"},{"location":"storage/storage-classes/#provisioner","title":"Provisioner","text":"<ul> <li>Specifies what volume plugin is used for provisioning PVs. Both internal and external provisioners can be used. For example, NFS doesn't have an internal provisioner but can use an external one.</li> </ul>"},{"location":"storage/storage-classes/#reclaim-policy","title":"Reclaim Policy","text":"<ul> <li>Specifies what happens to a dynamically created PV when it is released. The options are <code>Delete</code> or <code>Retain</code>.</li> </ul>"},{"location":"storage/storage-classes/#allow-volume-expansion","title":"Allow Volume Expansion","text":"<ul> <li>Indicates whether a PV can be expanded. This is controlled by the allowVolumeExpansion field in the StorageClass.</li> </ul>"},{"location":"storage/storage-classes/#mount-options","title":"Mount Options","text":"<ul> <li>Specifies mount options for dynamically created PVs. If an invalid mount option is given, the PV mount will fail.</li> </ul>"},{"location":"storage/storage-classes/#volume-binding-mode","title":"Volume Binding Mode","text":"<ul> <li>Controls when volume binding and provisioning occur. The default is <code>Immediate</code> mode, but <code>WaitForFirstConsumer</code> mode can be used for topology-constrained storage backends.</li> </ul>"},{"location":"storage/storage-classes/#allowed-topologies","title":"Allowed Topologies","text":"<ul> <li>Used to restrict the topology of provisioned volumes to specific zones.</li> </ul>"},{"location":"storage/storage-classes/#parameters","title":"Parameters","text":"<ul> <li>Describes additional provisioning parameters that are specific to the provisioner. For example, AWS EBS-specific parameters like <code>type</code>, <code>iopsPerGB</code>, etc.</li> </ul>"},{"location":"storage/storage-classes/#aws-ebs","title":"AWS EBS","text":"<ul> <li>Provides an example of how to define a StorageClass for AWS EBS, including various parameters like <code>type</code>, <code>iopsPerGB</code>, and <code>fsType</code>.</li> </ul>"},{"location":"storage/storage-classes/#gce-pd","title":"GCE PD","text":"<ul> <li>Similar to AWS but for Google Cloud's Persistent Disk. Includes parameters like <code>type</code>, <code>fstype</code>, and <code>replication-type</code>.</li> </ul>"},{"location":"storage/storage-classes/#nfs","title":"NFS","text":"<ul> <li>Explains that Kubernetes doesn't have an internal NFS provisioner and provides examples of how to use an external NFS provisioner.</li> </ul>"},{"location":"storage/storage-classes/#vsphere","title":"vSphere","text":"<ul> <li>Discusses two types of provisioners for vSphere and provides examples of how to define a StorageClass for vSphere.</li> </ul>"},{"location":"storage/storage-classes/#ceph-rbd","title":"Ceph RBD","text":"<ul> <li>Notes that the internal provisioner for Ceph RBD is deprecated and provides an example of a StorageClass for Ceph.</li> <li>Azure Disk: Provides an example of a StorageClass for Azure Disk but the content is truncated.</li> </ul>"},{"location":"storage/volume-health-monitoring/","title":"Volume Health Monitoring","text":""},{"location":"storage/volume-health-monitoring/#feature-state","title":"Feature State","text":"<p>The feature is in alpha state as of Kubernetes v1.21.</p>"},{"location":"storage/volume-health-monitoring/#overview","title":"Overview","text":"<p>Volume Health Monitoring in Kubernetes is part of the Container Storage Interface (CSI). It allows CSI Drivers to detect abnormal conditions in the underlying storage systems and report them as events on Persistent Volume Claims (PVCs) or Pods.</p>"},{"location":"storage/volume-health-monitoring/#components","title":"Components","text":"<p>The feature is implemented in two main components: 1. External Health Monitor Controller: This controller watches for abnormal volume conditions and reports them on the related PVC. 2. Kubelet: It also plays a role in volume health monitoring.</p>"},{"location":"storage/volume-health-monitoring/#controller-side-monitoring","title":"Controller-Side Monitoring","text":"<p>If a CSI Driver supports this feature from the controller side, an event will be reported on the related PVC when an abnormal volume condition is detected. The External Health Monitor controller also watches for node failure events. You can enable node failure monitoring by setting the <code>enable-node-watcher</code> flag to true. When a node failure is detected, an event is reported on the PVC to indicate that pods using this PVC are on a failed node.</p>"},{"location":"storage/volume-health-monitoring/#node-side-monitoring","title":"Node-Side Monitoring","text":"<p>If a CSI Driver supports this feature from the node side, an event will be reported on every Pod using the PVC when an abnormal volume condition is detected.</p>"},{"location":"storage/volume-health-monitoring/#metrics","title":"Metrics","text":"<p>Volume Health information is also exposed as Kubelet VolumeStats metrics. A new metric <code>kubelet_volume_stats_health_status_abnormal</code> is added, which includes two labels: <code>namespace</code> and <code>persistentvolumeclaim</code>. The count is either 1 or 0, where 1 indicates the volume is unhealthy and 0 indicates the volume is healthy.</p>"},{"location":"storage/volume-snapshot-classes/","title":"Volume Snapshot Classes","text":""},{"location":"storage/volume-snapshot-classes/#introduction","title":"Introduction","text":"<ul> <li>The concept of VolumeSnapshotClass in Kubernetes is similar to that of StorageClass. While StorageClass allows administrators to define \"classes\" of storage for provisioning volumes, VolumeSnapshotClass serves the same purpose but for provisioning volume snapshots.</li> </ul>"},{"location":"storage/volume-snapshot-classes/#the-volumesnapshotclass-resource","title":"The VolumeSnapshotClass Resource","text":"<ul> <li>A VolumeSnapshotClass contains three main fields:</li> <li>Driver: Specifies the CSI volume plugin used for provisioning VolumeSnapshots.</li> <li>DeletionPolicy: Configures what happens to a VolumeSnapshotContent when the associated VolumeSnapshot is deleted. It can be either <code>Retain</code> or <code>Delete</code>.</li> <li>Parameters: Describes additional configurations for volume snapshots belonging to this class.</li> </ul> <p>The name of the VolumeSnapshotClass object is significant as it is used by users to request a particular class. Once created, these objects cannot be updated. Here's an example YAML configuration:</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: csi-hostpath-snapclass\ndriver: hostpath.csi.k8s.io\ndeletionPolicy: Delete\nparameters:\n</code></pre>"},{"location":"storage/volume-snapshot-classes/#default-volumesnapshotclass","title":"Default VolumeSnapshotClass","text":"<ul> <li>Administrators can specify a default VolumeSnapshotClass for those VolumeSnapshots that don't request any particular class. This is done by adding an annotation <code>snapshot.storage.kubernetes.io/is-default-class: \"true\"</code>.</li> </ul> <p>Example: <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: csi-hostpath-snapclass\n  annotations:\n    snapshot.storage.kubernetes.io/is-default-class: \"true\"\ndriver: hostpath.csi.k8s.io\ndeletionPolicy: Delete\nparameters:\n</code></pre></p>"},{"location":"storage/volume-snapshot-classes/#deletion-policy","title":"Deletion Policy","text":"<ul> <li>The <code>deletionPolicy</code> can be either <code>Retain</code> or <code>Delete</code>:</li> <li><code>Retain</code>: The underlying snapshot and VolumeSnapshotContent remain even if the VolumeSnapshot is deleted.</li> <li><code>Delete</code>: Both the underlying storage snapshot and the VolumeSnapshotContent object are deleted when the VolumeSnapshot is deleted.</li> </ul>"},{"location":"storage/volume-snapshots/","title":"Volume Snapshots","text":""},{"location":"storage/volume-snapshots/#introduction","title":"Introduction","text":"<p>In Kubernetes, a VolumeSnapshot represents a snapshot of a volume on a storage system. The document assumes familiarity with Kubernetes persistent volumes. VolumeSnapshotContent and VolumeSnapshot API resources are provided to create volume snapshots. A VolumeSnapshotContent is a snapshot taken from a volume in the cluster, provisioned by an administrator. It is a resource in the cluster, similar to a PersistentVolume.</p>"},{"location":"storage/volume-snapshots/#api-objects-and-support","title":"API Objects and Support","text":"<p>VolumeSnapshot, VolumeSnapshotContent, and VolumeSnapshotClass are Custom Resource Definitions (CRDs), not part of the core API. VolumeSnapshot support is only available for CSI drivers. A snapshot controller and a sidecar helper container called csi-snapshotter are deployed as part of the VolumeSnapshot deployment process. The snapshot controller watches VolumeSnapshot and VolumeSnapshotContent objects and is responsible for their creation and deletion.</p>"},{"location":"storage/volume-snapshots/#lifecycle","title":"Lifecycle","text":"<p>VolumeSnapshotContents are resources in the cluster, while VolumeSnapshots are requests for those resources. Snapshots can be provisioned in two ways: pre-provisioned or dynamically provisioned. In pre-provisioned, a cluster administrator creates VolumeSnapshotContents with details of the real volume snapshot on the storage system. In dynamic provisioning, a snapshot is taken from a PersistentVolumeClaim.</p>"},{"location":"storage/volume-snapshots/#binding","title":"Binding","text":"<p>The snapshot controller handles the binding of a VolumeSnapshot object with an appropriate VolumeSnapshotContent object. The binding is a one-to-one mapping.</p>"},{"location":"storage/volume-snapshots/#protection","title":"Protection","text":"<p>While taking a snapshot of a PersistentVolumeClaim, that PersistentVolumeClaim is in-use. Deletion of the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.</p>"},{"location":"storage/volume-snapshots/#deletion","title":"Deletion","text":"<p>Deletion is triggered by deleting the VolumeSnapshot object, and the DeletionPolicy will be followed. If the DeletionPolicy is Delete, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object.</p>"},{"location":"storage/volume-snapshots/#volumesnapshots-and-volumesnapshotcontents-specs","title":"VolumeSnapshots and VolumeSnapshotContents Specs","text":"<p>Each VolumeSnapshot contains a spec and a status. For dynamically provisioning a snapshot, <code>volumeHandle</code> is the unique identifier of the volume created on the storage backend. For pre-provisioned snapshots, <code>snapshotHandle</code> is the unique identifier of the volume snapshot created on the storage backend.</p>"},{"location":"storage/volume-snapshots/#converting-volume-mode","title":"Converting Volume Mode","text":"<p>If the VolumeSnapshots API supports the <code>sourceVolumeMode</code> field, then it has the capability to prevent unauthorized users from converting the mode of a volume.</p>"},{"location":"storage/volume-snapshots/#provisioning-volumes-from-snapshots","title":"Provisioning Volumes from Snapshots","text":"<p>You can provision a new volume, pre-populated with data from a snapshot, by using the <code>dataSource</code> field in the PersistentVolumeClaim object.</p>"},{"location":"storage/volumes/","title":"Background","text":"<ul> <li>Kubernetes volumes are essential for managing data within pods.</li> <li>They are abstracted from the underlying storage, making it easier to handle storage in a containerized environment.</li> <li>Volumes allow data sharing between containers within a pod and enable persistence of data.  </li> </ul>"},{"location":"storage/volumes/#types-of-volumes","title":"Types of Volumes","text":""},{"location":"storage/volumes/#hostpath","title":"hostPath","text":"<ul> <li>Mounts files or directories from the node's file system into pods.</li> <li>Useful for accessing node-specific resources.</li> <li>Note that this can pose security and portability concerns.</li> </ul>"},{"location":"storage/volumes/#configmap","title":"configMap","text":"<ul> <li>ConfigMap volumes enable pods to access configuration data.</li> <li>Ideal for injecting configuration settings, environment variables, or configuration files into pods.</li> <li>Enhances pod flexibility by separating configuration from container images.</li> </ul>"},{"location":"storage/volumes/#downwardapi","title":"downwardAPI","text":"<ul> <li>Downward API volumes expose pod and container metadata as files.</li> <li>Pods can consume metadata like pod name, namespace, labels, and annotations.</li> <li>Allows for dynamic configuration based on pod context.</li> </ul>"},{"location":"storage/volumes/#emptydir","title":"emptyDir","text":"<ul> <li>An ephemeral volume created when a pod is assigned to a node.</li> <li>Useful for temporary storage needs within a pod.</li> <li>Data in emptyDir volumes is lost when the pod is removed.</li> </ul>"},{"location":"storage/volumes/#fc-fibre-channel","title":"fc (Fibre Channel)","text":"<ul> <li>Connects pods to Fibre Channel storage devices.</li> <li>Requires specialized hardware and drivers for Fibre Channel connectivity.</li> <li>Typically used in enterprise environments with Fibre Channel storage infrastructure.</li> </ul>"},{"location":"storage/volumes/#cephfs","title":"cephfs","text":"<ul> <li>Allows pods to mount the Ceph File System.</li> <li>Ceph is a distributed storage system that provides scalability and data redundancy.</li> <li>Useful for applications requiring shared file storage.</li> </ul>"},{"location":"storage/volumes/#awselasticblockstore-removed","title":"awsElasticBlockStore (Removed)","text":"<ul> <li>This volume type was used for managing AWS Elastic Block Store (EBS) volumes.</li> <li>EBS volumes provide block-level storage for AWS instances.</li> <li>Deprecated in favor of using the Container Storage Interface (CSI) or other storage options supported by AWS.</li> </ul>"},{"location":"storage/volumes/#azuredisk-removed","title":"azureDisk (Removed)","text":"<ul> <li>Used for attaching Azure Disk storage to pods.</li> <li>Azure Disks are durable and scalable storage options in Azure.</li> <li>Deprecated, and users are encouraged to use CSI drivers for Azure or other suitable options.</li> </ul>"},{"location":"storage/volumes/#cinder-removed","title":"cinder (Removed)","text":"<ul> <li>Cinder volumes were used for OpenStack Cinder block storage.</li> <li>Cinder provides block storage management for OpenStack.</li> <li>Deprecated; recommended to use CSI drivers or other OpenStack volume solutions.</li> </ul>"},{"location":"storage/volumes/#glusterfs-removed","title":"glusterfs (Removed)","text":"<ul> <li>Previously used for mounting GlusterFS distributed file systems.</li> <li>GlusterFS provides scalable and distributed storage.</li> <li>Deprecated; use CSI drivers or alternative GlusterFS options.</li> </ul>"},{"location":"storage/volumes/#azurefile-deprecated","title":"azureFile (Deprecated)","text":"<ul> <li>Previously used for mounting Azure File Shares in pods.</li> <li>Azure Files provide managed file shares in Azure.</li> <li>Deprecated; consider using CSI drivers for Azure or other alternatives.</li> </ul>"},{"location":"storage/volumes/#gcepersistentdisk-deprecated","title":"gcePersistentDisk (Deprecated)","text":"<ul> <li>Previously used for attaching Google Compute Engine (GCE) Persistent Disks.</li> <li>GCE Persistent Disks offer durable block storage in Google Cloud.</li> <li>Deprecated; consider using CSI drivers for GCE or other suitable GCE storage options.</li> </ul>"},{"location":"storage/volumes/#gitrepo-deprecated","title":"gitRepo (Deprecated)","text":"<ul> <li>Deprecated volume type that clones a Git repository into a volume.</li> <li>Rarely used in practice due to better alternatives like init containers or Git-based CI/CD workflows.</li> </ul>"},{"location":"workloads/daemonsets/","title":"Daemonsets","text":""},{"location":"workloads/daemonsets/#what-is-a-daemonset","title":"What is a DaemonSet?","text":"<ul> <li>A DaemonSet ensures that all (or some) nodes in a Kubernetes cluster run a copy of a specific Pod.</li> <li>Typical uses include running a cluster storage daemon, logs collection daemon, or node monitoring daemon on every node.</li> </ul>"},{"location":"workloads/daemonsets/#writing-a-daemonset-spec","title":"Writing a DaemonSet Spec","text":"<ul> <li>You can describe a DaemonSet in a YAML file.</li> <li>Required fields include <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code>.</li> </ul>"},{"location":"workloads/daemonsets/#pod-template","title":"Pod Template","text":"<ul> <li>The <code>.spec.template</code> is a Pod template with the same schema as a Pod.</li> <li>It must have a <code>RestartPolicy</code> equal to <code>Always</code>.</li> </ul>"},{"location":"workloads/daemonsets/#pod-selector","title":"Pod Selector","text":"<ul> <li>The <code>.spec.selector</code> field is a pod selector that must match the labels of the <code>.spec.template</code>.</li> </ul>"},{"location":"workloads/daemonsets/#running-pods-on-select-nodes","title":"Running Pods on Select Nodes","text":"<ul> <li>You can specify node selectors or affinities to control on which nodes the Pods will run.</li> </ul>"},{"location":"workloads/daemonsets/#how-daemon-pods-are-scheduled","title":"How Daemon Pods are Scheduled","text":"<ul> <li>The DaemonSet controller adds spec.affinity.nodeAffinity to match the target host.</li> <li>Different schedulers can be specified for the Pods.</li> </ul>"},{"location":"workloads/daemonsets/#taints-and-tolerations","title":"Taints and Tolerations","text":"<ul> <li>DaemonSet controller automatically adds a set of tolerations to DaemonSet Pods to ensure they can be scheduled even under various node conditions.</li> </ul>"},{"location":"workloads/daemonsets/#communicating-with-daemon-pods","title":"Communicating with Daemon Pods","text":"<ul> <li>Various patterns like <code>Push</code>, <code>NodeIP</code> and <code>Known Port</code>, <code>DNS</code>, and <code>Service</code> can be used for communication.</li> </ul>"},{"location":"workloads/daemonsets/#updating-a-daemonset","title":"Updating a DaemonSet","text":"<ul> <li>Node labels can be changed, and the DaemonSet will update accordingly.</li> <li>Rolling updates and rollbacks can be performed.</li> </ul>"},{"location":"workloads/daemonsets/#alternatives-to-daemonset","title":"Alternatives to DaemonSet","text":"<ul> <li>Daemon processes can also be run directly on nodes using init scripts, but DaemonSets offer several advantages like monitoring, logging, and resource isolation.</li> </ul>"},{"location":"workloads/daemonsets/#deployments-vs-daemonsets","title":"Deployments vs DaemonSets","text":"<ul> <li>Use Deployments for stateless services and DaemonSets for node-level functionalities.</li> </ul>"},{"location":"workloads/deployments/","title":"Deployments","text":"<p>A Kubernetes Deployment is a higher-level abstraction designed to manage the desired state of a set of replicated Pods. It allows you to describe an intended state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. Here are some key features and functionalities:  </p>"},{"location":"workloads/deployments/#key-features","title":"Key Features","text":"<ul> <li>Replica Management: You can specify the number of Pod replicas you want to run.</li> <li>Updates: Allows for rolling updates to Pods, ensuring zero downtime.</li> <li>Rollbacks: If something goes wrong, you can roll back to a previous stable version.</li> <li>Scaling: You can easily scale your application up or down.</li> <li>Self-healing: Automatically replaces failed or unhealthy Pods.</li> </ul>"},{"location":"workloads/deployments/#common-operations","title":"Common Operations","text":"<ul> <li>Create a Deployment: Usually done through a YAML file that describes the Deployment.</li> <li>Inspect a Deployment: Using commands like <code>kubectl get deployments</code> or <code>kubectl describe deployment &lt;deployment-name&gt;</code>.</li> <li>Update a Deployment: You can update the image or other fields in the YAML and apply it.</li> <li>Rollback a Deployment: Using <code>kubectl rollout undo deployment &lt;deployment-name&gt;</code>.</li> <li>Scale a Deployment: Using <code>kubectl scale deployment &lt;deployment-name&gt; --replicas=number</code>.</li> </ul>"},{"location":"workloads/deployments/#yaml-example","title":"YAML Example","text":"<p>Here's a simple example of a Deployment YAML file:</p> <p><code>yaml apiVersion: apps/v1 kind: Deployment metadata:   name: nginx-deployment spec:   replicas: 3   selector:     matchLabels:       app: nginx   template:     metadata:       labels:         app: nginx     spec:       containers:       - name: nginx         image: nginx:1.14.2</code></p>"},{"location":"workloads/jobs/","title":"Workloads","text":""},{"location":"workloads/jobs/#running-an-example-job","title":"Running an example Job","text":"<ul> <li>Use <code>kubectl create -f</code> to create a Job from a YAML file.</li> <li><code>kubectl get jobs</code> to list all Jobs and their statuses.</li> </ul>"},{"location":"workloads/jobs/#writing-a-job-spec","title":"Writing a Job spec","text":"<ul> <li><code>metadata.name</code> to specify the Job name.</li> <li><code>spec.template.spec.containers[].image</code> to specify the container image.</li> <li><code>spec.template.spec.restartPolicy</code> must be either <code>Never</code> or <code>OnFailure</code>.</li> </ul>"},{"location":"workloads/jobs/#job-labels","title":"Job Labels","text":"<ul> <li>Labels are key-value pairs attached to Jobs.</li> <li>Useful for organizing and querying Jobs.</li> </ul>"},{"location":"workloads/jobs/#pod-template","title":"Pod Template","text":"<ul> <li>Nested inside the Job spec under spec.template.</li> <li>Specifies the Pod's containers, volumes, and other configurations.</li> </ul>"},{"location":"workloads/jobs/#pod-selector","title":"Pod selector","text":"<ul> <li>Automatically generated based on Job's <code>metadata.labels</code>.</li> <li>Do not set <code>.spec.selector</code> field manually unless you know what you're doing.</li> </ul>"},{"location":"workloads/jobs/#parallel-execution-for-jobs","title":"Parallel execution for Jobs","text":"<ul> <li><code>spec.parallelism</code>: Number of Pods running simultaneously.</li> <li><code>spec.completions</code>: Number of Pods that must complete successfully for the Job to be marked as complete.</li> </ul>"},{"location":"workloads/jobs/#completion-mode","title":"Completion mode","text":"<ul> <li>Indexed Jobs: Each Pod gets a unique index between 0 and spec.completions-1. NonIndexed: No unique identifiers for Pods.</li> </ul>"},{"location":"workloads/jobs/#handling-pod-and-container-failures","title":"Handling Pod and container failures","text":"<ul> <li><code>spec.backoffLimit</code>: Number of allowed failures before Job is marked as failed.</li> <li><code>spec.activeDeadlineSeconds</code>: Time in seconds that a Job is allowed to run.</li> </ul>"},{"location":"workloads/jobs/#pod-backoff-failure-policy","title":"Pod backoff failure policy","text":"<ul> <li>Exponential backoff for restarting failed Pods. Controlled by spec.backoffLimit and spec.activeDeadlineSeconds.</li> </ul>"},{"location":"workloads/jobs/#backoff-limit-per-index","title":"Backoff limit per index","text":"<ul> <li>In Indexed Jobs, each index has its own backoff limit and retry mechanism.</li> </ul>"},{"location":"workloads/jobs/#pod-failure-policy","title":"Pod failure policy","text":"<ul> <li>No explicit failure policy, but can be managed using <code>spec.backoffLimit</code> and <code>spec.activeDeadlineSeconds</code>.</li> </ul>"},{"location":"workloads/jobs/#job-termination-and-cleanup","title":"Job termination and cleanup","text":"<ul> <li>Deleting a Job will also delete all its Pods.</li> <li>Use <code>kubectl delete job --selector=</code> to delete multiple Jobs.</li> </ul>"},{"location":"workloads/jobs/#clean-up-finished-jobs-automatically","title":"Clean up finished jobs automatically","text":"<ul> <li><code>.spec.ttlSecondsAfterFinished</code>: Time-to-live in seconds after Job completion, after which the Job and its Pods are deleted.</li> </ul>"},{"location":"workloads/jobs/#ttl-mechanism-for-finished-jobs","title":"TTL mechanism for finished Jobs","text":"<ul> <li>TTL controller in Kubernetes takes care of this.</li> <li>Only works if the feature gate <code>TTLAfterFinished</code> is enabled.</li> </ul>"},{"location":"workloads/jobs/#job-patterns","title":"Job patterns","text":"<ul> <li>One-off Jobs: Run once and terminate.</li> <li>CronJobs: Scheduled Jobs, defined using cron syntax.</li> </ul>"},{"location":"workloads/jobs/#ttl-after-finished-controller","title":"TTL-After-Finished Controller","text":"<ul> <li>Provides a TTL (time to live) mechanism to limit the lifetime of Job objects that have finished execution.</li> </ul>"},{"location":"workloads/jobs/#cleanup-for-finished-jobs","title":"Cleanup for Finished Jobs","text":"<ul> <li>Supported only for Jobs.</li> <li>You can specify <code>.spec.ttlSecondsAfterFinished</code> field to clean up finished Jobs automatically.</li> <li>The timer starts once the Job status changes to Complete or Failed.</li> <li>After TTL expires, the Job becomes eligible for cascading removal, including its dependent objects.</li> <li>Kubernetes honors object lifecycle guarantees, such as waiting for finalizers.</li> </ul>"},{"location":"workloads/jobs/#setting-ttl","title":"Setting TTL","text":"<ul> <li>You can set the TTL seconds at any time.</li> <li>Can be specified in the Job manifest.</li> <li>Can be manually set for existing, already finished Jobs.</li> <li>Can use a mutating admission webhook to set this field dynamically at Job creation time or after the Job has finished.</li> <li>You can write your own controller to manage the cleanup TTL for Jobs based on selectors.</li> </ul>"},{"location":"workloads/jobs/#caveats","title":"Caveats","text":"<ul> <li>Updating TTL for Finished Jobs: You can modify the TTL period even after the job is created or has finished. However, retention is not guaranteed if you extend the TTL after it has already expired.</li> <li>Time Skew: The feature is sensitive to time skew in your cluster, which may cause the control plane to clean up Job objects at the wrong time.</li> </ul>"},{"location":"workloads/jobs/#cronjobs","title":"Cronjobs","text":"<ul> <li>CronJobs are used for performing regular scheduled actions like backups, report generation, etc.</li> </ul>"},{"location":"workloads/jobs/#schedule-syntax","title":"Schedule Syntax","text":"<ul> <li>Uses Cron syntax for scheduling.</li> <li>Extended \"Vixie cron\" step values are supported.</li> <li>Macros like <code>@monthly</code>, <code>@weekly</code>, etc., can also be used.</li> </ul>"},{"location":"workloads/jobs/#job-template","title":"Job Template","text":"<ul> <li>Defines a template for the Jobs that the CronJob creates.</li> <li>Same schema as a Job but nested and without <code>apiVersion</code> or <code>kind</code>.</li> </ul>"},{"location":"workloads/jobs/#deadline-for-delayed-job-start","title":"Deadline for Delayed Job Start","text":"<ul> <li>Optional <code>.spec.startingDeadlineSeconds</code> field.</li> <li>Defines a deadline for starting the Job if it misses its scheduled time.</li> </ul>"},{"location":"workloads/jobs/#concurrency-policy","title":"Concurrency Policy","text":"<ul> <li>Optional <code>.spec.concurrencyPolicy</code> field.</li> <li>Options: <code>Allow</code> (default), <code>Forbid</code>, <code>Replace</code>.</li> </ul>"},{"location":"workloads/jobs/#schedule-suspension","title":"Schedule Suspension","text":"<ul> <li>Optional <code>.spec.suspend</code> field to suspend execution of Jobs.</li> </ul>"},{"location":"workloads/jobs/#jobs-history-limits","title":"Jobs History Limits","text":"<ul> <li><code>.spec.successfulJobsHistoryLimit</code> and <code>.spec.failedJobsHistoryLimit</code> fields are optional.</li> </ul>"},{"location":"workloads/jobs/#time-zones","title":"Time Zones","text":"<ul> <li>Time zones can be specified using <code>.spec.timeZone</code>.</li> </ul>"},{"location":"workloads/jobs/#job-creation","title":"Job Creation","text":"<ul> <li>A CronJob creates a Job object approximately once per execution time of its schedule.</li> </ul>"},{"location":"workloads/replicaset/","title":"ReplicaSets","text":"<p>A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.  </p>"},{"location":"workloads/replicaset/#what-is-a-replicaset","title":"What is a ReplicaSet?","text":"<p>A ReplicaSet is a Kubernetes controller that ensures a specified number of pod replicas are running at any given time.</p>"},{"location":"workloads/replicaset/#purpose","title":"Purpose","text":"<p>It is designed to maintain high availability and fault tolerance for pods.</p>"},{"location":"workloads/replicaset/#relationship-with-pods","title":"Relationship with Pods","text":"<p>A ReplicaSet creates and deletes pods as needed to meet the desired replica count.</p>"},{"location":"workloads/replicaset/#labels-and-selectors","title":"Labels and Selectors","text":"<p>ReplicaSets use labels and selectors to identify which pods to manage.</p>"},{"location":"workloads/replicaset/#scaling","title":"Scaling","text":"<p>You can manually scale a ReplicaSet or use it with an autoscaler.</p>"},{"location":"workloads/replicaset/#yaml-configuration","title":"YAML Configuration","text":"<p>A ReplicaSet is defined in a YAML file, specifying the desired number of replicas and the pod template.</p>"},{"location":"workloads/replicaset/#rolling-updates-and-rollbacks","title":"Rolling Updates and Rollbacks","text":"<p>While ReplicaSets themselves don't support rolling updates, they are often used with Deployments that do.</p>"},{"location":"workloads/replicaset/#ownership","title":"Ownership","text":"<p>A ReplicaSet is considered the \"owner\" of the pods it manages, and this ownership info is stored in the pod's metadata.</p>"},{"location":"workloads/replicaset/#manual-intervention","title":"Manual Intervention","text":"<p>It's possible to manually delete pods managed by a ReplicaSet, but it's generally not recommended unless you know what you're doing.</p>"},{"location":"workloads/replicaset/#limitations","title":"Limitations","text":"<p>ReplicaSets do not support pod versioning, unlike Deployments.</p>"},{"location":"workloads/replicaset/#use-cases","title":"Use Cases","text":"<p>Ideal for stateless applications where pods are interchangeable.</p>"},{"location":"workloads/replicaset/#best-practices","title":"Best Practices","text":"<p>It's generally better to use Deployments, which use ReplicaSets under the hood but offer more features like rolling updates.</p>"},{"location":"workloads/replicationcontroller/","title":"ReplicationController","text":""},{"location":"workloads/replicationcontroller/#purpose-and-functionality","title":"Purpose and Functionality","text":"<ul> <li>Ensures a specified number of pod replicas are running at any given time.</li> <li>Automatically replaces pods that fail, are deleted, or are terminated.</li> </ul>"},{"location":"workloads/replicationcontroller/#how-it-works","title":"How it Works","text":"<ul> <li>If there are too many pods, it terminates the extra ones.</li> <li>If there are too few, it starts more.</li> </ul>"},{"location":"workloads/replicationcontroller/#abbreviation","title":"Abbreviation","text":"<ul> <li>Often abbreviated to <code>rc</code> in discussions and kubectl commands.</li> </ul>"},{"location":"workloads/replicationcontroller/#use-cases","title":"Use Cases","text":"<ul> <li>Can run one instance of a Pod indefinitely.</li> <li>Can run several identical replicas of a replicated service, like web servers.</li> </ul>"},{"location":"workloads/replicationcontroller/#configuration-example","title":"Configuration Example","text":"<ul> <li>YAML configuration specifies the number of <code>replicas</code>, <code>selector</code>, and pod <code>template</code>.</li> </ul>"},{"location":"workloads/replicationcontroller/#commands","title":"Commands","text":"<ul> <li><code>kubectl apply -f &lt;config-file&gt;</code> to apply the configuration.</li> <li><code>kubectl describe replicationcontrollers/rc-name</code> to check the status.</li> </ul>"},{"location":"workloads/replicationcontroller/#pod-template","title":"Pod Template","text":"<ul> <li><code>.spec.template</code> is a pod template with the same schema as a Pod.</li> </ul>"},{"location":"workloads/replicationcontroller/#labels-and-selectors","title":"Labels and Selectors","text":"<ul> <li><code>.spec.selector</code> is a label selector that manages pods with matching labels.</li> </ul>"},{"location":"workloads/replicationcontroller/#scaling","title":"Scaling","text":"<ul> <li><code>.spec.replicas</code> specifies the number of pods that should run concurrently.</li> </ul>"},{"location":"workloads/replicationcontroller/#deletion","title":"Deletion","text":"<ul> <li>kubectl delete scales the ReplicationController to zero and waits for pod deletion.</li> </ul>"},{"location":"workloads/replicationcontroller/#alternatives","title":"Alternatives","text":"<ul> <li>ReplicaSet and Deployment are the next-generation alternatives.</li> </ul>"},{"location":"workloads/replicationcontroller/#daemonset-and-job","title":"DaemonSet and Job","text":"<ul> <li>For machine-level functions, use DaemonSet.</li> <li>For pods expected to terminate on their own, use Job.</li> </ul>"},{"location":"workloads/resources/","title":"Resources","text":""},{"location":"workloads/resources/#introduction","title":"Introduction","text":"<ul> <li>Kubernetes offers built-in APIs for declarative management of workloads.</li> <li>Workloads run as containers inside Pods.</li> <li>Kubernetes control plane manages Pods based on workload object specifications.</li> </ul>"},{"location":"workloads/resources/#deployment-and-replicaset","title":"Deployment and ReplicaSet","text":"<ul> <li>Most common way to run applications on the cluster.</li> <li>Good for managing stateless application workloads.</li> <li>Pods in a Deployment are interchangeable.</li> <li>Replaces the legacy <code>ReplicationController</code> API.</li> </ul>"},{"location":"workloads/resources/#statefulset","title":"StatefulSet","text":"<ul> <li>Manages one or more Pods running the same application code.</li> <li>Pods have a distinct identity, unlike Deployments.</li> <li>Commonly used to link Pods with their persistent storage via <code>PersistentVolume</code>.</li> <li>Replacement Pods connect to the same <code>PersistentVolume</code>.</li> </ul>"},{"location":"workloads/resources/#daemonset","title":"DaemonSet","text":"<ul> <li>Defines Pods that provide node-specific facilities.</li> <li>Useful for running drivers or other node-level services.</li> <li>Can run across every node or a subset of nodes in the cluster.</li> </ul>"},{"location":"workloads/resources/#job-and-cronjob","title":"Job and CronJob","text":"<ul> <li>Job represents a one-off task that runs to completion.</li> <li>CronJob represents tasks that repeat according to a schedule.</li> </ul>"}]}