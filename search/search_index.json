{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to K8s Guide","text":"<p>Content current as of Kubernetes verison 1.28</p> <p>Welcome to k8s.guide, your concise companion through the world of Kubernetes. My aim was to provide a simplified, digestible, and easy-to-navigate version of the official Kubernetes documentation.</p> <p></p> <p>Legal discalimer:  </p> <ul> <li> <p>\"Kubernetes\", \"K8s\" and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  </p> </li> <li> <p>Neither myself nor this site are officially associated with the Linux Foundation.  </p> </li> </ul>"},{"location":"administration/api-priority-and-fairness/","title":"API Priority and Fairness","text":""},{"location":"administration/api-priority-and-fairness/#concepts","title":"Concepts","text":"<ul> <li> <p>Priority Level: This is a configuration that defines how requests that match the level are to be handled. It specifies things like the concurrency shares, the queue length, and the queuing discipline.</p> </li> <li> <p>Flow Schema: This is used to classify incoming requests. It specifies conditions like the verbs (<code>GET</code>, <code>POST</code>, etc.), the resources (<code>Pods</code>, <code>Services</code>, etc.), and the namespaces that the requests are coming from. Once a request matches a Flow Schema, it is then handled according to its associated Priority Level.</p> </li> </ul>"},{"location":"administration/api-priority-and-fairness/#configuration","title":"Configuration","text":"<p>FlowSchema: - <code>spec.matches</code>: Defines what requests will match this schema. You can specify multiple criteria like HTTP verbs, API groups, resources, etc. - <code>spec.priorityLevelConfiguration.name</code>: Specifies the name of the Priority Level to use for requests that match this schema. </p> <p>PriorityLevelConfiguration: - <code>spec.type</code>: Can be either \"Exempt\" (ignores all other fields and never queues) or \"Limited\" (respects other fields). - <code>spec.assuredConcurrencyShares</code>: For \"Limited\" type, this sets the weight of this priority level vs others. - <code>spec.queues</code>: For \"Limited\" type, this sets the number of queues for this priority level. - <code>spec.queueLengthLimit</code>: For \"Limited\" type, this sets the max size of each queue. - <code>spec.handSize</code>: For \"Limited\" type, this sets the number of less loaded queues that a given request is randomly assigned to.</p>"},{"location":"administration/api-priority-and-fairness/#example-configuration","title":"Example Configuration","text":"<pre><code># PriorityLevelConfiguration: \\\nCatch-all priority level\napiVersion: flowcontrol.apiserver.\\\nk8s.io/v1beta1\nkind: PriorityLevelConfiguration\nmetadata:\n  name: catch-all\nspec:\n  type: Limited\n  assuredConcurrencyShares: 1\n  queues: 128\n  queueLengthLimit: 100\n  handSize: 6\n\n---\n# FlowSchema: Catch-all flow schema\napiVersion: flowcontrol.apiserver \\\n.k8s.io/v1beta1\nkind: FlowSchema\nmetadata:\n  name: catch-all\nspec:\n  priorityLevelConfiguration:\n    name: catch-all\n  matchingPrecedence: 1000  # a fairly \\\n  low precedence\n  rules:\n    - subjects:\n        - kind: Group\n          name: system:masters\n      rule:\n        verbs:\n          - '*'\n        apiGroups:\n          - '*'\n        resources:\n          - '*'\n</code></pre>"},{"location":"administration/api-priority-and-fairness/#best-practices","title":"Best Practices","text":"<ul> <li> <p>Be Cautious: Misconfiguration can lead to degraded API server performance. Therefore, it's crucial to test configurations under simulated conditions to understand their impact.</p> </li> <li> <p>Monitoring: Keep an eye on the API server's performance metrics to ensure that the Priority and Fairness configurations are having the desired effect.</p> </li> </ul>"},{"location":"administration/certificates/","title":"Certificates","text":""},{"location":"administration/certificates/#easyrsa","title":"easyrsa","text":"<ul> <li>Download and initialize easyrsa3.</li> <li>Generate a new Certificate Authority (CA).</li> <li>Generate the server certificate and key.</li> </ul>"},{"location":"administration/certificates/#openssl","title":"openssl","text":"<ul> <li>Generate a <code>ca.key</code> with 2048 bits.</li> <li>Create a <code>ca.crt</code> based on ca.key.</li> <li>Generate a <code>server.key</code> with 2048 bits.</li> <li>Create a config file for generating a Certificate Signing Request (CSR).</li> <li>Generate the certificate signing request based on the config file.</li> <li>Generate the server certificate using <code>ca.key</code>, <code>ca.crt</code>, and <code>server.csr</code>.</li> </ul>"},{"location":"administration/certificates/#cfssl","title":"cfssl","text":"<ul> <li>Download and prepare the command-line tools.</li> <li>Create a directory to hold the artifacts and initialize cfssl.</li> <li>Generate CA key (<code>ca-key.pem</code>) and certificate (<code>ca.pem</code>).</li> <li>Generate the key and certificate for the API server.</li> </ul>"},{"location":"administration/cluster-networking/","title":"Cluster Networking","text":""},{"location":"administration/cluster-networking/#highly-coupled-container-to-container-communications","title":"Highly-coupled container-to-container communications","text":"<p>In Kubernetes, tightly coupled application components can be packed into a Pod. Containers in the same Pod share the same network namespace, which means they can communicate with each other via localhost. This makes inter-container communication seamless.</p>"},{"location":"administration/cluster-networking/#pod-to-pod-communications","title":"Pod-to-Pod communications","text":"<p>This is the core focus of the document. In Kubernetes, every Pod gets its own IP address, and there's no need for explicit links between Pods or mapping container ports to host ports. This makes it easier to run services that bind to specific ports without conflicts. The Pod-to-Pod communication is made possible through a networking model that may be implemented differently depending on the network plugin used.</p>"},{"location":"administration/cluster-networking/#pod-to-service-communications","title":"Pod-to-Service communications","text":"<p>Kubernetes Services are an abstraction that defines a logical set of Pods and enables external traffic exposure, load balancing, and service discovery for those Pods. Services allow Pods to reliably communicate with each other without needing to know the IP address of the Pod they are talking to.</p>"},{"location":"administration/cluster-networking/#external-to-service-communications","title":"External-to-Service communications","text":"<p>Kubernetes Services also allow for external traffic to enter the cluster. This is usually done through various types of Service types like ClusterIP, NodePort, and LoadBalancer, each serving a different use-case and environment.</p> <p> Container Network Interface (CNI) plugins are responsible for attaching Pods to the host network, and they come in various flavors. Some offer basic Pod networking, while others offer extended features like network policy enforcement, encryption, and more.</p> <p> There are ongoing efforts to improve networking, led by the Special Interest Group for Networking (SIG-Network). They are actively working on Kubernetes Enhancement Proposals (KEPs) to introduce new features and improvements in networking.</p>"},{"location":"administration/installing-addons/","title":"Installing Addons","text":""},{"location":"administration/installing-addons/#networking-and-network-policy","title":"Networking and Network Policy","text":"<ul> <li>ACI: Integrated container networking and network security with Cisco ACI.</li> <li>Antrea: Provides networking and security services, leveraging Open vSwitch. It's a CNCF project at the Sandbox level.</li> <li>Calico: A networking and network policy provider with various networking options.</li> <li>Canal: Unites Flannel and Calico.</li> <li>Cilium: Offers networking, observability, and security with an eBPF-based data plane. It's a CNCF project at the Incubation level.</li> <li>CNI-Genie: Allows Kubernetes to connect to multiple CNI plugins.</li> <li>Contiv: Provides configurable networking and a rich policy framework.</li> <li>Contrail: An open-source, multi-cloud network virtualization and policy management platform.</li> <li>Flannel: An overlay network provider.</li> <li>Knitter: Supports multiple network interfaces in a pod.</li> <li>Multus: Supports multiple networks in Kubernetes.</li> <li>OVN-Kubernetes: Based on OVN, provides an overlay-based networking implementation.</li> <li>Nodus: An OVN-based CNI controller plugin for Service function chaining (SFC).</li> <li>NSX-T Container Plug-in (NCP): Integration between VMware NSX-T and Kubernetes.</li> <li>Nuage: An SDN platform with policy-based networking.</li> <li>Romana: A Layer 3 networking solution.</li> <li>Weave Net: Provides networking and network policy.</li> </ul>"},{"location":"administration/installing-addons/#service-discovery","title":"Service Discovery","text":"<ul> <li>CoreDNS: A flexible, extensible DNS server for in-cluster DNS for pods.</li> </ul>"},{"location":"administration/installing-addons/#visualization-control","title":"Visualization &amp; Control","text":"<ul> <li>Dashboard: A web interface for Kubernetes. Weave Scope: A tool for visualizing containers, Pods, Services, etc.</li> </ul>"},{"location":"administration/installing-addons/#infrastructure","title":"Infrastructure","text":"<ul> <li>KubeVirt: An add-on to run virtual machines on Kubernetes, usually on bare-metal clusters. Node Problem Detector: Reports system issues on Linux nodes.</li> </ul>"},{"location":"administration/logging-architecture/","title":"Logging Architecture","text":""},{"location":"administration/logging-architecture/#application-logs","title":"Application Logs","text":"<p>Standard Output and Standard Error Streams: Applications in Kubernetes should write logs to the standard output (stdout) and standard error (stderr) streams. This allows Kubernetes to capture these logs regardless of the logging library or format used by the application.</p>"},{"location":"administration/logging-architecture/#cluster-level-logging","title":"Cluster-level Logging","text":"<ul> <li>Separate Storage: Logs should be stored in a dedicated storage system separate from nodes, pods, or containers. This ensures that logs are retained even if the associated Kubernetes objects are deleted.</li> <li>No Native Solution: Kubernetes itself doesn't offer a built-in log storage solution, but you can integrate third-party tools like Elasticsearch, Logstash, and Kibana (ELK stack) or other logging solutions.</li> </ul>"},{"location":"administration/logging-architecture/#pod-and-container-logs","title":"Pod and Container Logs","text":"<p>Log Capture: Kubernetes captures logs for each container within a running Pod. These logs are available through the <code>kubectl logs</code> command.</p>"},{"location":"administration/logging-architecture/#how-nodes-handle-container-logs","title":"How Nodes Handle Container Logs","text":"<p>Container Runtimes: Container runtimes like Docker or containerd capture the standard output and standard error streams and redirect them to a logging driver, which is usually configured to write to a file on disk.</p>"},{"location":"administration/logging-architecture/#log-rotation","title":"Log Rotation","text":"<p>Kubelet Configuration: The <code>kubelet</code>, running on each node, can be configured to automatically rotate logs to prevent filling up storage space. This is particularly useful for long-running Pods or high-verbosity logging.</p>"},{"location":"administration/logging-architecture/#system-component-logs","title":"System Component Logs","text":"<ul> <li>Running in Containers: Some system components may run in containers and write logs to the standard output and standard error streams.</li> <li>Directly Running: Some components may run directly on the node and write logs to files in specific directories.</li> </ul>"},{"location":"administration/logging-architecture/#log-locations","title":"Log Locations","text":"<p>OS Dependent: The locations where logs are stored depend on the operating system. For example, in a system using <code>systemd</code>, you might find logs in <code>/var/log</code>.</p>"},{"location":"administration/logging-architecture/#cluster-level-logging-architectures","title":"Cluster-level Logging Architectures","text":"<ul> <li>Node-level Logging Agent: An agent running on each node can be responsible for capturing logs and sending them to a centralized logging solution.</li> <li>Dedicated Sidecar Container: A sidecar container within each Pod can capture and forward logs.</li> <li>Direct Push to Backend: Applications can be configured to push logs directly to a logging backend, bypassing the need for a separate logging agent.</li> </ul>"},{"location":"administration/managing-resources/","title":"Managing Resources","text":""},{"location":"administration/managing-resources/#organizing-resource-configurations","title":"Organizing Resource Configurations","text":"<ul> <li>Multiple Resources in One File: You can include configurations for different resources like Pods, Services, and Deployments in a single YAML or JSON file. Each resource is separated by <code>--</code>.</li> <li>Best Practices: It's recommended to group configurations for resources that are tightly coupled into the same file. This makes it easier to manage them as a unit.</li> </ul>"},{"location":"administration/managing-resources/#bulk-operations-in-kubectl","title":"Bulk Operations in kubectl","text":"<ul> <li>Bulk Create: You can create multiple resources at once by specifying a directory with <code>kubectl apply -f /</code>.</li> <li>Bulk Delete: Similarly, you can delete all resources in a directory with <code>kubectl delete -f /</code>.</li> <li>Selectors: You can use label selectors to operate on a subset of resources that match the labels.</li> </ul>"},{"location":"administration/managing-resources/#canary-deployments","title":"Canary Deployments","text":"<ul> <li>Concept: Canary deployments allow you to roll out a new version of an application alongside the stable version to test its performance and reliability.</li> <li>Implementation: You can manage canary deployments using Kubernetes Deployments by controlling the number of replicas for the new and old versions.</li> </ul>"},{"location":"administration/managing-resources/#updating-annotations","title":"Updating Annotations","text":"<ul> <li>Annotations are key-value pairs that you can attach to objects but are not used for filtering and selecting objects.</li> <li>Usage: You can use kubectl annotate to add or update annotations. For example, <code>kubectl annotate pods my-pod icon-url=http://goo.gl/XXBTWq</code>.</li> </ul>"},{"location":"administration/managing-resources/#scaling-your-application","title":"Scaling Your Application","text":"<ul> <li>Manual Scaling: You can manually scale the number of pod replicas using kubectl scale. For example, <code>kubectl scale --replicas=3 rs/foo</code>.</li> <li>Auto-Scaling: Kubernetes supports automatic scaling based on CPU usage or other select metrics through the Horizontal Pod Autoscaler.</li> </ul>"},{"location":"administration/managing-resources/#in-place-updates-of-resources","title":"In-place Updates of Resources","text":"<ul> <li>Apply: The <code>kubectl apply</code> command is used to apply changes made to a resource in a non-disruptive way.</li> <li>Edit and Patch: You can also use <code>kubectl edit</code> to edit resources directly or kubectl patch to apply partial updates.</li> </ul>"},{"location":"administration/managing-resources/#disruptive-updates","title":"Disruptive Updates","text":"<ul> <li>Force Replace: If you need to replace a resource, you can use <code>kubectl replace --force</code>, but this will cause a service disruption.</li> </ul>"},{"location":"administration/managing-resources/#updating-without-service-outage","title":"Updating Without Service Outage","text":"<p>Zero Downtime: Kubernetes Deployments enable you to update your application without any downtime by ensuring that at least a certain number of Pods are running at all times.</p>"},{"location":"administration/metrics-for-kubernetes-components/","title":"Metrics","text":"<p>Prometheus Format: Kubernetes components emit metrics in Prometheus format, which is structured plain text readable by both humans and machines. </p> <p>Metrics Endpoint: Metrics are generally available on the <code>/metrics</code> endpoint of the HTTP server. For components that don't expose this endpoint by default, it can be enabled using the <code>-bind-address</code> flag. </p> <p>Components: Metrics are available for various components like <code>kube-controller-manager</code>, <code>kube-proxy</code>, <code>kube-apiserver</code>, <code>kube-scheduler</code>, and <code>kubelet</code>. </p> <p>RBAC Authorization: If your cluster uses RBAC, reading metrics requires authorization via a user, group, or ServiceAccount with a ClusterRole that allows accessing <code>/metrics</code>. </p> <p>Metric Lifecycle: Metrics go through different stages Alpha, Stable, Deprecated, Hidden, and Deleted. Each stage has its own set of rules and stability guarantees. </p> <p>Command-Line Flags: Admins can enable hidden metrics through a command-line flag on a specific binary, using the flag <code>show-hidden-metrics-for-version</code>. </p> <p>Component Metrics: Specific metrics are available for the <code>kube-controller-manager</code> and <code>kube-scheduler</code>, providing insights into the performance and health of these components. </p> <p>Disabling Metrics: Metrics can be turned off via the command-line flag <code>-disabled-metrics</code> if they are causing performance issues. </p> <p>Metric Cardinality Enforcement: To limit resource use, you can use the <code>-allow-label-value</code> command-line option to dynamically configure an allow-list of label values for a metric.</p>"},{"location":"administration/proxies-in-kubernetes/","title":"Proxies","text":""},{"location":"administration/proxies-in-kubernetes/#kubectl-proxy","title":"kubectl proxy","text":"<ul> <li>Runs on a user's desktop or in a pod.</li> <li>Proxies from a localhost address to the Kubernetes API server.</li> <li>Uses HTTP for client-to-proxy communication.</li> <li>Uses HTTPS for proxy-to-API server communication.</li> <li>Locates the API server and adds authentication headers.</li> </ul>"},{"location":"administration/proxies-in-kubernetes/#apiserver-proxy","title":"apiserver proxy","text":"<ul> <li>Acts as a bastion built into the API server.</li> <li>Connects a user outside of the cluster to cluster IPs that might otherwise be unreachable.</li> <li>Runs in the API server processes.</li> <li>Uses HTTPS (or HTTP if configured) for client-to-proxy communication.</li> <li>Proxy-to-target may use HTTP or HTTPS, depending on available information.</li> <li>Can be used to reach a Node, Pod, or Service.</li> <li>Performs load balancing when used to reach a Service.</li> </ul>"},{"location":"administration/proxies-in-kubernetes/#kube-proxy","title":"kube proxy","text":"<ul> <li>Runs on each node.</li> <li>Proxies UDP, TCP, and SCTP.</li> <li>Does not understand HTTP.</li> <li>Provides load balancing.</li> <li>Used only to reach services.</li> </ul>"},{"location":"administration/proxies-in-kubernetes/#proxyload-balancer-in-front-of-api-servers","title":"Proxy/Load-balancer in front of API server(s)","text":"<ul> <li>Existence and implementation vary from cluster to cluster (e.g., nginx).</li> <li>Sits between all clients and one or more API servers.</li> <li>Acts as a load balancer if there are multiple API servers.</li> </ul>"},{"location":"administration/proxies-in-kubernetes/#cloud-load-balancers-on-external-services","title":"Cloud Load Balancers on external services","text":"<ul> <li>Provided by some cloud providers (e.g., AWS ELB, Google Cloud Load Balancer).</li> <li>Created automatically when the Kubernetes service has type LoadBalancer.</li> <li>Usually supports UDP/TCP only.</li> <li>SCTP support depends on the cloud provider's implementation.</li> </ul> <p>Proxies have replaced redirect capabilities, which have been deprecated.</p>"},{"location":"administration/system-logs/","title":"System Logs","text":""},{"location":"administration/system-logs/#klog-library","title":"Klog Library","text":"<ul> <li>Klog is the Kubernetes logging library used to generate log messages for system components.</li> <li>Kubernetes is in the process of simplifying logging, and certain klog command-line flags are deprecated starting with Kubernetes v1.23 and will be removed in v1.26.</li> </ul>"},{"location":"administration/system-logs/#output-redirection","title":"Output Redirection","text":"<ul> <li>Output is generally written to <code>stderr</code> and is expected to be handled by the component invoking a Kubernetes component.</li> <li>For environments where traditional output redirection options are not available, kube-log-runner can be used as a wrapper to redirect output.</li> </ul>"},{"location":"administration/system-logs/#structured-logging","title":"Structured Logging","text":"<ul> <li>Introduced in Kubernetes v1.23 (beta), structured logging allows for a uniform structure in log messages, making it easier to store and process logs.</li> <li>The default formatting is backward-compatible with traditional klog.</li> </ul>"},{"location":"administration/system-logs/#contextual-logging","title":"Contextual Logging","text":"<ul> <li>Introduced in Kubernetes v1.24 (alpha), contextual logging builds on structured logging and allows developers to add additional information to log entries.</li> <li>It is currently gated behind the StructuredLogging feature gate and is disabled by default.</li> </ul>"},{"location":"administration/system-logs/#json-log-format","title":"JSON Log Format","text":"<p>Available since Kubernetes v1.19 (alpha), the <code>-logging-format=json</code> flag changes the log format to JSON.</p>"},{"location":"administration/system-logs/#log-verbosity-level","title":"Log Verbosity Level","text":"<p>Controlled by the <code>v</code> flag, increasing the value logs increasingly less severe events.</p>"},{"location":"administration/system-logs/#log-location","title":"Log Location","text":"<p>Logs can be written to <code>journald</code> or <code>.log</code> files in the <code>/var/log</code> directory, depending on whether the system component runs in a container.</p>"},{"location":"administration/system-logs/#log-query","title":"Log Query","text":"<p>Introduced in Kubernetes v1.27 (alpha), this feature allows viewing logs of services running on the node, provided certain feature gates and configuration options are enabled.</p>"},{"location":"administration/traces-for-kubernetes-components/","title":"Traces","text":""},{"location":"administration/traces-for-kubernetes-components/#configuration-example","title":"Configuration Example","text":"<p>Here's an example configuration for collecting spans and logging them to standard output:</p> <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\nexporters:\n  logging:\n    logLevel: debug\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [logging]\n</code></pre>"},{"location":"administration/traces-for-kubernetes-components/#component-traces","title":"Component Traces","text":"<ul> <li>kube-apiserver traces: The kube-apiserver generates spans for incoming HTTP requests and for outgoing requests to webhooks, etcd, and re-entrant requests. It propagates the W3C Trace Context with outgoing requests but does not make use of the trace context attached to incoming requests.</li> <li>kubelet traces: The kubelet's CRI interface and authenticated HTTP servers are instrumented to generate trace spans. The endpoint and sampling rate are configurable. Trace context propagation is also configured.</li> </ul>"},{"location":"administration/traces-for-kubernetes-components/#enabling-tracing-in-kube-apiserver","title":"Enabling Tracing in kube-apiserver","text":"<p>To enable tracing in the kube-apiserver, you can provide a tracing configuration file using the <code>--tracing-config-file=</code> flag.</p>"},{"location":"administration/traces-for-kubernetes-components/#enabling-tracing-in-the-kubelet","title":"Enabling Tracing in the kubelet","text":"<p>To enable tracing in the kubelet, you can apply the tracing configuration. Here's an example snippet:</p> <pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nfeatureGates:\n  KubeletTracing: true\ntracing:\n  samplingRatePerMillion: 100\n</code></pre>"},{"location":"administration/traces-for-kubernetes-components/#stability-and-performance-overhead","title":"Stability and Performance Overhead","text":"<p>Tracing instrumentation is still under active development and may change in various ways. Also, exporting spans comes with a small performance overhead on the networking and CPU side.</p>"},{"location":"architecture/cgroupv2/","title":"cgroup v2","text":"<p>On Linux, control groups constrain resources that are allocated to processes. The kubelet and the underlying container runtime need to interface with cgroups to enforce resource management for pods and containers which includes cpu/memory requests and limits for containerized workloads. There are two versions of cgroups in Linux: <code>cgroup v1</code> and <code>cgroup v2</code>. <code>cgroup v2</code> is the new generation of the cgroup API.</p>"},{"location":"architecture/cgroupv2/#what-is-cgroup-v2","title":"What is <code>cgroup v2</code>?","text":"<ul> <li><code>cgroup v2</code> is the next version of the Linux cgroup API.</li> <li>Provides a unified control system with enhanced resource management capabilities.</li> <li>Offers improvements like a single unified hierarchy design in API, safer sub-tree delegation to containers, and enhanced resource allocation management.</li> </ul>"},{"location":"architecture/cgroupv2/#using-cgroup-v2","title":"Using <code>cgroup v2</code>","text":"<ul> <li>Recommended to use a Linux distribution that enables and uses <code>cgroup v2</code> by default.</li> </ul>"},{"location":"architecture/cgroupv2/#requirements","title":"Requirements","text":"<ul> <li>OS distribution should enable <code>cgroup v2</code>.</li> <li>Linux Kernel version should be 5.8 or later.</li> <li>Container runtime should support <code>cgroup v2</code>, e.g., <code>containerd v1.4</code> and later, <code>cri-o v1.20</code> and later.</li> </ul>"},{"location":"architecture/cgroupv2/#linux-distribution-cgroup-v2-support","title":"Linux Distribution <code>cgroup v2</code> support","text":"<ul> <li>Linux distributions that support <code>cgroup v2</code>, such as Container Optimized OS, Ubuntu, Debian GNU/Linux, Fedora, Arch Linux, and RHEL.</li> </ul>"},{"location":"architecture/cgroupv2/#migrating-to-cgroup-v2","title":"Migrating to <code>cgroup v2</code>","text":"<ul> <li>Ensure you meet the requirements and then upgrade to a kernel version that enables <code>cgroup v2</code> by default.</li> <li>The kubelet automatically detects <code>cgroup v2</code> and performs accordingly.</li> </ul>"},{"location":"architecture/cgroupv2/#identify-the-cgroup-version-on-linux-nodes","title":"Identify the cgroup version on Linux Nodes","text":"<ul> <li>To check which cgroup version your distribution uses, you can run specific commands like: <pre><code>stat -fc %T /sys/fs/cgroup/.\n</code></pre></li> </ul>"},{"location":"architecture/cloud-controller-manager/","title":"Cloud Controller Manager","text":"<pre><code>graph TB\n    subgraph Kubernetes Control Plane\n        cm[Cloud Controller Manager]\n        api[API Server]\n        etcd[(ETCD)]\n        cm --&gt;|interacts with| api\n        api --&gt; etcd\n    end\n\n    subgraph Cloud Infrastructure\n        nodeController[Node Controller]\n        routeController[Route Controller]\n        serviceController[Service Controller]\n        cloudResources[Cloud Resources]\n        nodeController --&gt; cloudResources\n        routeController --&gt; cloudResources\n        serviceController --&gt; cloudResources\n    end\n\n    cm --&gt;|manages| nodeController\n    cm --&gt;|manages| routeController\n    cm --&gt;|manages| serviceController\n\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:2px;\n    class cm,nodeController,routeController,serviceController k8s;\n</code></pre>"},{"location":"architecture/cloud-controller-manager/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<ul> <li>A Kubernetes control plane component that embeds cloud-specific control logic.</li> <li>Decouples the interoperability logic between Kubernetes and underlying cloud infrastructure.</li> <li>Allows cloud providers to release features at a different pace compared to the main Kubernetes project.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#design","title":"Design","text":"<ul> <li>Runs in the control plane as a replicated set of processes, usually as containers in Pods.</li> <li>Implements multiple controllers in a single process.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#node-controller","title":"Node Controller","text":"<ul> <li>Updates Node objects when new servers are created in the cloud infrastructure.</li> <li>Annotates and labels the Node object with cloud-specific information.</li> <li>Verifies the node's health and deletes the Node object if the server has been deleted from the cloud.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#route-controller","title":"Route Controller","text":"<ul> <li>Configures routes in the cloud so that containers on different nodes can communicate.</li> <li>May also allocate blocks of IP addresses for the Pod network.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#service-controller","title":"Service Controller","text":"<ul> <li>Integrates with cloud infrastructure components like managed load balancers and IP addresses.</li> <li>Sets up load balancers and other infrastructure when a Service resource requires them.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#api-object-access","title":"API Object Access","text":"<ul> <li>Requires specific access levels to various API objects like Node, Service, and Endpoints.</li> <li>For example, full access to read and modify Node objects, and list and watch access to Service objects.</li> </ul>"},{"location":"architecture/cloud-controller-manager/#rbac-clusterrole","title":"RBAC ClusterRole","text":"<ul> <li>Defines the permissions required for the cloud controller manager, such as creating events and service accounts.</li> </ul>"},{"location":"architecture/controllers/","title":"Controllers","text":""},{"location":"architecture/controllers/#control-loop","title":"Control Loop","text":"<ul> <li>A non-terminating loop that regulates the state of a system.</li> <li>Example: A thermostat in a room that adjusts the temperature to reach the desired state.</li> </ul>"},{"location":"architecture/controllers/#controller-pattern","title":"Controller Pattern","text":"<ul> <li>Tracks at least one Kubernetes resource type.</li> <li>Responsible for making the current state align with the desired state specified in the resource's spec field.</li> </ul>"},{"location":"architecture/controllers/#control-via-api-server","title":"Control via API Server","text":"<ul> <li>Controllers interact with the cluster API server to manage state.</li> <li>Example: The Job controller creates or removes Pods via the API server to complete a task.</li> </ul>"},{"location":"architecture/controllers/#direct-control","title":"Direct Control","text":"<ul> <li>Some controllers interact with external systems to achieve the desired state.</li> <li>Example: A controller that scales the number of nodes in a cluster by interacting with cloud provider APIs.</li> </ul>"},{"location":"architecture/controllers/#desired-vs-current-state","title":"Desired vs. Current State","text":"<ul> <li>Kubernetes aims for a cloud-native approach, handling constant change.</li> <li>Controllers work to bring the current state closer to the desired state, even if the cluster is never in a stable state.</li> </ul>"},{"location":"architecture/controllers/#design-principles","title":"Design Principles","text":"<ul> <li>Kubernetes uses multiple controllers for different aspects of cluster state.</li> <li>Allows for resilience, as one controller can take over if another fails.</li> </ul>"},{"location":"architecture/controllers/#ways-of-running-controllers","title":"Ways of Running Controllers","text":"<ul> <li>Built-in controllers run inside the kube-controller-manager.</li> <li>Custom controllers can run either inside or outside the Kubernetes cluster.</li> </ul> <pre><code>graph LR\n    subgraph Kubernetes Cluster\n        apiServer[API Server]\n        controller[Controller]\n        resource[Resource Spec]\n        actualState[Actual State]\n        desiredState[Desired State]\n\n        resource --&gt;|defines| desiredState\n        apiServer --&gt;|observes| actualState\n        actualState --&gt;|reported via kubelet| apiServer\n        desiredState --&gt; controller\n    end\n\n    apiServer --&gt;|notifies of state changes| controller\n    controller --&gt;|attempts to match| desiredState\n\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:2px;\n    class apiServer,controller k8s;\n</code></pre>"},{"location":"architecture/cri/","title":"Container Runtime Interface","text":""},{"location":"architecture/cri/#what-is-cri","title":"What is CRI?","text":"<ul> <li>CRI is a plugin interface that allows the kubelet to use various container runtimes.</li> <li>It eliminates the need to recompile cluster components for different runtimes.</li> <li>A working container runtime is required on each Node for the kubelet to launch Pods and their containers.</li> </ul>"},{"location":"architecture/cri/#protocol","title":"Protocol","text":"<ul> <li>CRI defines the main gRPC protocol for communication between the kubelet and the container runtime.</li> <li>The kubelet acts as a client when connecting to the container runtime via gRPC.</li> </ul>"},{"location":"architecture/cri/#api-feature-state","title":"API Feature State","text":"<ul> <li>As of Kubernetes v1.23, CRI is considered stable.</li> <li>The kubelet uses command-line flags like <code>--image-service-endpoint</code> to configure runtime and image service endpoints.</li> </ul>"},{"location":"architecture/cri/#cri-version-support","title":"CRI Version Support","text":"<ul> <li>For Kubernetes v1.28, the kubelet prefers to use <code>CRI v1</code>.</li> <li>If a runtime doesn't support v1, the kubelet negotiates an older supported version.</li> <li><code>CRI v1alpha2</code> is considered deprecated.</li> </ul>"},{"location":"architecture/cri/#upgrading","title":"Upgrading","text":"<ul> <li>During a Kubernetes upgrade, the kubelet tries to automatically select the latest CRI version.</li> <li>If that fails, fallback mechanisms are in place.</li> <li>If a gRPC re-dial is required due to a container runtime upgrade, the runtime must support the initially selected version, or the re-dial will fail.</li> </ul>"},{"location":"architecture/garbage-collection/","title":"Garbage Collection","text":""},{"location":"architecture/garbage-collection/#what-is-garbage-collection","title":"What is Garbage Collection?","text":"<ul> <li>Collective term for mechanisms that clean up cluster resources.</li> <li>Targets terminated pods, completed jobs, objects without owner references, unused containers and images, and more.</li> </ul>"},{"location":"architecture/garbage-collection/#owners-and-dependents","title":"Owners and Dependents","text":"<ul> <li>Objects in Kubernetes link to each other through owner references.</li> <li>Owner references help the control plane and other API clients clean up related resources before deleting an object.</li> </ul>"},{"location":"architecture/garbage-collection/#cascading-deletion","title":"Cascading Deletion","text":"<p>Two types: Foreground and Background. - Foreground: Owner object first enters a \"deletion in progress\" state, and dependents are deleted before the owner. - Background: Owner object is deleted immediately, and dependents are cleaned up in the background.</p>"},{"location":"architecture/garbage-collection/#orphaned-dependents","title":"Orphaned Dependents","text":"<ul> <li>Dependents left behind when an owner object is deleted are called orphan objects.</li> <li>By default, Kubernetes deletes dependent objects, but this behavior can be overridden.</li> </ul>"},{"location":"architecture/garbage-collection/#garbage-collection-of-unused-containers-and-images","title":"Garbage Collection of Unused Containers and Images","text":"<ul> <li>Kubelet performs garbage collection on unused images every five minutes and on unused containers every minute.</li> <li>Configurable options include <code>HighThresholdPercent</code> and <code>LowThresholdPercent</code> for disk usage.</li> </ul>"},{"location":"architecture/garbage-collection/#container-garbage-collection","title":"Container Garbage Collection","text":"<ul> <li>Variables like <code>MinAge</code>, <code>MaxPerPodContainer</code>, and <code>MaxContainers</code> can be defined to control container garbage collection.</li> <li>Kubelet adjusts <code>MaxPerPodContainer</code> if it conflicts with <code>MaxContainers</code>.</li> </ul>"},{"location":"architecture/garbage-collection/#configuring-garbage-collection","title":"Configuring Garbage Collection","text":"<ul> <li>Options specific to controllers managing resources can be configured for garbage collection.</li> </ul>"},{"location":"architecture/leases/","title":"Leases","text":""},{"location":"architecture/leases/#leases-in-distributed-systems","title":"Leases in Distributed Systems","text":"<ul> <li>Leases provide a mechanism to lock shared resources and coordinate activities.</li> <li>In Kubernetes, represented by Lease objects in the <code>coordination.k8s.io</code> API Group.</li> </ul>"},{"location":"architecture/leases/#node-heartbeats","title":"Node Heartbeats","text":"<ul> <li>The Lease API is used to communicate kubelet node heartbeats to the Kubernetes API server.</li> <li>Each Node has a corresponding Lease object in the <code>kube-node-lease</code> namespace.</li> <li>The <code>spec.renewTime</code> field is updated with each heartbeat, and the control plane uses this timestamp to determine Node availability.</li> </ul>"},{"location":"architecture/leases/#leader-election","title":"Leader Election","text":"<ul> <li>Leases ensure only one instance of a component runs at a given time.</li> <li>Used by control plane components like <code>kube-controller-manager</code> and <code>kube-scheduler</code> in HA configurations.</li> </ul>"},{"location":"architecture/leases/#api-server-identity","title":"API Server Identity","text":"<ul> <li>Starting in Kubernetes v1.26, each <code>kube-apiserver</code> uses the Lease API to publish its identity.</li> <li>Enables future capabilities that may require coordination between each <code>kube-apiserver</code>.</li> </ul>"},{"location":"architecture/leases/#workloads","title":"Workloads","text":"<ul> <li>Custom workloads can define their own use of Leases for leader election or coordination.</li> <li>Good practice to name the Lease in a way that is linked to the component or product.</li> </ul>"},{"location":"architecture/leases/#garbage-collection","title":"Garbage Collection","text":"<ul> <li>Expired leases from <code>kube-apiservers</code> that no longer exist are garbage-collected by new <code>kube-apiservers</code> after 1 hour.</li> </ul>"},{"location":"architecture/leases/#feature-gate","title":"Feature Gate","text":"<ul> <li>API server identity leases can be disabled by disabling the <code>APIServerIdentity</code> feature gate.</li> </ul>"},{"location":"architecture/mixed-version-proxy/","title":"Mixed Version Proxy","text":""},{"location":"architecture/mixed-version-proxy/#feature-state","title":"Feature State","text":"<ul> <li>As of Kubernetes v1.28, the Mixed Version Proxy is an alpha feature.</li> <li>Allows an API Server to proxy resource requests to other peer API servers running different Kubernetes versions.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#use-case","title":"Use Case","text":"<ul> <li>Useful for clusters with multiple API servers running different versions, especially during long-lived rollouts to new Kubernetes releases.</li> <li>Helps in directing resource requests to the correct <code>kube-apiserver</code>, preventing unexpected 404 errors during upgrades.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#enabling-mixed-version-proxy","title":"Enabling Mixed Version Proxy","text":"<ul> <li>Enable the UnknownVersionInteroperabilityProxy feature gate when starting the API Server.</li> <li>Requires specific command-line arguments like <code>--peer-ca-file</code>, <code>--proxy-client-cert-file</code>, <code>--proxy-client-key-file</code>, and <code>--requestheader-client-ca-file</code>.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#proxy-transport-and-authentication","title":"Proxy Transport and Authentication","text":"<ul> <li>Source <code>kube-apiserver</code> uses existing flags -proxy-client-cert-file and -proxy-client-key-file to present its identity.</li> <li>Destination <code>kube-apiserver</code> verifies the peer connection based on the -requestheader-client-ca-file argument.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#configuration-for-peer-api-server-connectivity","title":"Configuration for Peer API Server Connectivity","text":"<ul> <li>Use <code>--peer-advertise-ip</code> and <code>--peer-advertise-port</code> to set the network location for proxying requests.</li> <li>If unspecified, it defaults to the values from <code>--advertise-address</code> or <code>--bind-address</code>.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#mixed-version-proxying-mechanism","title":"Mixed Version Proxying Mechanism","text":"<ul> <li>Special filter in the aggregation layer identifies API groups/versions/resources that the local server doesn't recognize.</li> <li>Attempts to proxy those requests to a peer API server capable of handling them.</li> <li>If the peer API server fails to respond, a <code>503 (\"Service Unavailable\")</code> error is returned.</li> </ul>"},{"location":"architecture/mixed-version-proxy/#how-it-works-under-the-hood","title":"How it Works Under the Hood","text":"<ul> <li>Uses the internal <code>StorageVersion</code> API to check which API servers can serve the requested resource.</li> <li>If no peer is known for that API group/version/resource, a <code>404 (\"Not Found\")</code> response is returned.</li> <li>If the selected peer fails to respond, a <code>503 (\"Service Unavailable\")</code> error is returned.</li> </ul>"},{"location":"architecture/node-communication/","title":"Node Communication","text":""},{"location":"architecture/node-communication/#node-to-control-plane","title":"Node to Control Plane","text":"<ul> <li>Follows a \"hub-and-spoke\" API pattern.</li> <li>All API usage from nodes or their pods terminates at the API server.</li> <li>API server listens on a secure HTTPS port, typically 443.</li> <li>Nodes should have the public root certificate and valid client credentials for secure connection.</li> </ul>"},{"location":"architecture/node-communication/#api-server-to-kubelet","title":"API Server to Kubelet","text":"<ul> <li>Used for fetching logs, attaching to running pods, and port-forwarding.</li> <li>Connections terminate at the kubelet's HTTPS endpoint.</li> <li>To secure the connection, use the <code>--kubelet-certificate-authority</code> flag for the API server.</li> <li>Kubelet authentication and/or authorization should be enabled.</li> </ul>"},{"location":"architecture/node-communication/#api-server-to-nodes-pods-and-services","title":"API Server to Nodes, Pods, and Services","text":"<ul> <li>Connections default to plain HTTP and are neither authenticated nor encrypted.</li> <li>Can be run over HTTPS but will not validate the certificate or provide client credentials.</li> <li>Not safe to run over untrusted or public networks.</li> </ul>"},{"location":"architecture/node-communication/#ssh-tunnels","title":"SSH Tunnels","text":"<ul> <li>Supports SSH tunnels to protect control plane to nodes communication.</li> <li>API server initiates an SSH tunnel to each node and passes all traffic through the tunnel.</li> <li>Ensures traffic is not exposed outside the nodes' network.</li> </ul>"},{"location":"architecture/node-communication/#konnectivity-service","title":"Konnectivity Service","text":"<ul> <li>Provides TCP level proxy for control plane to cluster communication.</li> <li>Consists of the Konnectivity server in the control plane network and agents in the nodes network.</li> <li>After enabling, all control plane to nodes traffic goes through these connections.</li> </ul>"},{"location":"architecture/nodes/","title":"Nodes","text":""},{"location":"architecture/nodes/#what-are-nodes","title":"What are nodes?","text":"<ul> <li>Worker machines in a Kubernetes cluster.</li> <li>Run containerized applications managed by the Control Plane.</li> <li>Equipped with a <code>kubelet</code> agent that communicates with the master components.</li> </ul>"},{"location":"architecture/nodes/#control-plane-vs-worker-nodes","title":"Control Plane vs Worker Nodes","text":"<ul> <li>All nodes are managed by master components, collectively called the Control Plane.</li> <li>Control plane perations include adding nodes, updating node software, and decommissioning nodes. <pre><code>stateDiagram-v2\n    state control_plane {\n        kube_apiserver\n        kube_apiserver --&gt; etcd: stores data\n        kube_apiserver --&gt; kube_controller_manager: watches changes\n        kube_apiserver --&gt; scheduler: finds placement\n        scheduler --&gt; kube_apiserver: watches for pods needing scheduled\n        kube_controller_manager\n    }\n\n    state worker_nodes {\n        kubelet\n        kubelet --&gt; kube_proxy: configures networking\n        kubelet --&gt; container_runtime: runs containers\n        kube_proxy --&gt; iptables_BPF: manages networking rules\n        container_runtime\n    }\n\n    control_plane --&gt; worker_nodes: manages\n    worker_nodes --&gt; control_plane: reports\n</code></pre> <p>Don't worry if some of these components don't make sense - we'll get to them in later sections.</p> </li> </ul>"},{"location":"architecture/nodes/#node-name-uniqueness","title":"Node Name Uniqueness","text":"<ul> <li>Each node must have a unique identifier within the cluster.</li> <li>Ensures accurate scheduling and task allocation.</li> </ul>"},{"location":"architecture/nodes/#self-registration-of-nodes","title":"Self-registration of Nodes","text":"<ul> <li>Nodes can automatically register themselves upon joining the cluster.</li> <li>Facilitates dynamic scaling and resource allocation.</li> </ul>"},{"location":"architecture/nodes/#manual-node-administration","title":"Manual Node Administration","text":"<ul> <li>Admins can manually add or remove nodes using the Kubernetes API or CLI tools.</li> <li>Useful for fine-grained control over the cluster.</li> </ul>"},{"location":"architecture/nodes/#node-status","title":"Node Status","text":"<ul> <li>Provides detailed information about the node, including IP addresses, conditions (<code>Ready</code>, <code>OutOfDisk</code>, etc.), and resource capacity.</li> <li>Updated periodically by the node's Kubelet.</li> </ul>"},{"location":"architecture/nodes/#node-heartbeats","title":"Node Heartbeats","text":"<ul> <li>Regular signals sent from the Kubelet to the master to indicate the node's health.</li> <li>Failure to send a heartbeat within a certain time leads to node eviction.</li> </ul>"},{"location":"architecture/nodes/#node-controller","title":"Node Controller","text":"<ul> <li>A Control Plane component responsible for monitoring nodes.</li> <li>Handles node failures and triggers pod evictions if necessary.</li> </ul>"},{"location":"architecture/nodes/#rate-limits-on-eviction","title":"Rate Limits on Eviction","text":"<ul> <li>Configurable settings that control the speed at which pods are evicted from unhealthy nodes.</li> <li>Helps to avoid overwhelming the remaining healthy nodes.</li> </ul>"},{"location":"architecture/nodes/#resource-capacity-tracking","title":"Resource Capacity Tracking","text":"<ul> <li>Nodes report available resources like CPU, memory, and storage for better scheduling.</li> <li>Helps the scheduler in placing pods where resources are available.</li> </ul>"},{"location":"architecture/nodes/#node-topology","title":"Node Topology","text":"<ul> <li>Information about the physical or virtual layout of nodes in terms of regions, zones, and other cloud-provider specific metadata.</li> <li>Used for optimizing workload distribution and high availability.</li> </ul>"},{"location":"architecture/nodes/#graceful-node-shutdown","title":"Graceful Node Shutdown","text":"<ul> <li>A process that safely evicts pods before shutting down or rebooting a node.</li> <li>Ensures minimal impact on running applications and services.</li> </ul>"},{"location":"architecture/nodes/#pod-priority-based-graceful-node-shutdown","title":"Pod Priority based Graceful Node Shutdown","text":"<ul> <li>During a graceful shutdown, pods with higher priority are evicted last.</li> <li>Ensures that critical applications continue to run for as long as possible.</li> </ul>"},{"location":"architecture/nodes/#non-graceful-node-shutdown-handling","title":"Non-graceful Node Shutdown Handling","text":"<ul> <li>In cases of abrupt failures, all pods are immediately terminated.</li> <li>Risks include data loss and potential service disruption.</li> </ul>"},{"location":"architecture/nodes/#swap-memory-management","title":"Swap Memory Management","text":"<ul> <li>Kubernetes allows for the enabling or disabling of swap memory usage on nodes.</li> <li>Swap usage can impact application performance and pod scheduling decisions.</li> </ul>"},{"location":"configuration/best-practices/","title":"Best Practices","text":""},{"location":"configuration/best-practices/#general-configuration-tips","title":"General Configuration Tips","text":"<ul> <li>Always specify the latest stable API version in your configuration files.</li> <li>Store configuration files in version control to facilitate quick rollbacks and cluster restoration.</li> <li>Prefer YAML over JSON for configuration files as it's more user-friendly.</li> </ul>"},{"location":"configuration/best-practices/#naked-pods-vs-replicasets-deployments-and-jobs","title":"\"Naked\" Pods vs ReplicaSets, Deployments, and Jobs","text":"<ul> <li>Avoid using naked Pods that are not bound to a ReplicaSet or Deployment.</li> <li>Use Deployments for almost all scenarios as they ensure the desired number of Pods and specify a strategy for Pod replacement.</li> </ul>"},{"location":"configuration/best-practices/#services","title":"Services","text":"<ul> <li>Create a Service before its corresponding backend workloads (Deployments or ReplicaSets).</li> <li>Services provide environment variables to containers, which implies an ordering requirement: Services must be created before the - Pods that need them.</li> </ul>"},{"location":"configuration/best-practices/#using-labels","title":"Using Labels","text":"<ul> <li>Use semantic labels for your application or Deployment.</li> <li>Labels can be manipulated for debugging; removing labels will make the Pod invisible to controllers and Services.</li> </ul>"},{"location":"configuration/best-practices/#using-kubectl","title":"Using kubectl","text":"<ul> <li>Use <code>kubectl apply -f directory</code> to apply configurations from all .yaml, .yml, and .json files in a directory.</li> <li>Use label selectors for <code>get</code> and <code>delete</code> operations instead of specific object names.</li> </ul>"},{"location":"configuration/configmaps/","title":"ConfigMaps","text":""},{"location":"configuration/configmaps/#what-is-a-configmap","title":"What is a ConfigMap?","text":"<p>A ConfigMap is an API object in Kubernetes used to store non-confidential data in key-value pairs. It allows you to decouple environment-specific configuration from your container images, making your applications easily portable.</p>"},{"location":"configuration/configmaps/#motivation","title":"Motivation","text":"<p>The primary use case for ConfigMaps is to separate configuration data from application code. For example, you might have an environment variable named <code>DATABASE_HOST</code> that varies between your local development environment and the cloud. By using a ConfigMap, you can manage these settings independently of the container images and the application code.</p>"},{"location":"configuration/configmaps/#configmap-object","title":"ConfigMap Object","text":"<p>A ConfigMap object has <code>data</code> and <code>binaryData</code> fields, which accept key-value pairs. The <code>data</code> field is designed for UTF-8 strings, while <code>binaryData</code> is for base64-encoded strings. Starting from Kubernetes v1.19, you can make a ConfigMap immutable by adding an <code>immutable</code> field to its definition.</p>"},{"location":"configuration/configmaps/#configmaps-and-pods","title":"ConfigMaps and Pods","text":"<p>You can refer to a ConfigMap in a Pod spec to configure the container(s) based on the data in the ConfigMap. There are four ways to use a ConfigMap in a Pod: 1. Inside a container command and args 2. As environment variables for a container 3. As a read-only volume for the application to read 4. By writing code that uses the Kubernetes API to read the ConfigMap</p>"},{"location":"configuration/configmaps/#using-configmaps","title":"Using ConfigMaps","text":"<p>ConfigMaps can be mounted as data volumes or used by other parts of the system for configuration. You can specify which keys to include and the paths where they should be mounted when defining a Pod that uses a ConfigMap.</p>"},{"location":"configuration/configmaps/#immutable-configmaps","title":"Immutable ConfigMaps","text":"<p>From Kubernetes v1.21, you can set ConfigMaps as immutable, which prevents accidental or unwanted updates and reduces the load on the <code>kube-apiserver</code>.</p>"},{"location":"configuration/organizing-cluster-access/","title":"Organizing Cluster Access","text":""},{"location":"configuration/organizing-cluster-access/#kubeconfig-files","title":"Kubeconfig Files","text":"<p>Kubeconfig files are used to store information about clusters, users, namespaces, and authentication mechanisms. The <code>kubectl</code> command-line tool relies on these files to interact with the cluster. By default, <code>kubectl</code> looks for a file named <code>config</code> in the <code>$HOME/.kube</code> directory. You can specify other kubeconfig files by setting the <code>KUBECONFIG</code> environment variable or using the <code>--kubeconfig</code> flag.</p>"},{"location":"configuration/organizing-cluster-access/#supporting-multiple-clusters-users-and-authentication-mechanisms","title":"Supporting Multiple Clusters, Users, and Authentication Mechanisms","text":"<p>If you have multiple clusters and various authentication methods, kubeconfig files help you manage them. For instance: - A running kubelet might use certificates for authentication. - A user might use tokens. - Administrators might provide sets of certificates to individual users.</p>"},{"location":"configuration/organizing-cluster-access/#context","title":"Context","text":"<p>In a kubeconfig file, a context is used to group access parameters under a name. Each context has three parameters: cluster, namespace, and user. The <code>kubectl</code> tool uses parameters from the current context by default. You can switch contexts using the <code>kubectl config use-context</code> command.</p>"},{"location":"configuration/organizing-cluster-access/#the-kubeconfig-environment-variable","title":"The KUBECONFIG Environment Variable","text":"<p>This variable holds a list of kubeconfig files. On Linux and Mac, the list is colon-delimited, while on Windows, it's semicolon-delimited. If the variable doesn't exist, the default kubeconfig file <code>$HOME/.kube/config</code> is used. If it does exist, <code>kubectl</code> merges the files listed in the variable.</p>"},{"location":"configuration/organizing-cluster-access/#merging-kubeconfig-files","title":"Merging Kubeconfig Files","text":"<p><code>kubectl</code> follows specific rules when merging kubeconfig files: - If the <code>-kubeconfig</code> flag is set, only that file is used. - If the <code>KUBECONFIG</code> variable is set, the files listed are merged. - The first file to set a value wins, and conflicting entries are ignored.</p>"},{"location":"configuration/organizing-cluster-access/#file-references","title":"File References","text":"<p>Paths in a kubeconfig file are relative to the location of the kubeconfig file itself. On the command line, they are relative to the current working directory.</p>"},{"location":"configuration/organizing-cluster-access/#proxy-configuration","title":"Proxy Configuration","text":"<p>You can configure <code>kubectl</code> to use a proxy for each cluster by setting <code>proxy-url</code> in the kubeconfig file.</p>"},{"location":"configuration/resource-management/","title":"Resource Management","text":""},{"location":"configuration/resource-management/#resource-management-for-pods-and-containers","title":"Resource Management for Pods and Containers","text":"<p>In Kubernetes, each container in a Pod can have its resource requirements specified, such as CPU and memory. The Kubernetes scheduler (<code>kube-scheduler</code>) uses these requirements to decide which node is best suited to run the Pod. The <code>kubelet</code> on the node then enforces these limits to ensure the container doesn't use more than it's allocated.</p>"},{"location":"configuration/resource-management/#requests-and-limits","title":"Requests and Limits","text":"<p>\"Requests\" are the minimum resources that a container will be allocated. For example, if a container requests 500MB of memory, Kubernetes will only schedule it on a node with at least that much free memory. \"Limits,\" on the other hand, are the maximum resources that a container can use. If a container tries to use more than its limit, it may be terminated or throttled depending on the resource type.</p>"},{"location":"configuration/resource-management/#resource-types","title":"Resource Types","text":"<p>You can specify various types of resources, but CPU and memory are the most common. \"Huge pages\" are a Linux feature that allows the kernel to manage memory more efficiently for specific workloads.</p>"},{"location":"configuration/resource-management/#resource-requests-and-limits-of-pod-and-container","title":"Resource Requests and Limits of Pod and Container","text":"<p>In the Pod specification, you can define the resource requests and limits for each container. The Pod's overall resource request and limit are the sum of its containers' individual requests and limits. This aggregated information is used by the scheduler for Pod placement.</p>"},{"location":"configuration/resource-management/#resource-units-in-kubernetes","title":"Resource Units in Kubernetes","text":"<p>CPU resources are usually specified in \"millicores,\" where 1000 millicores equal 1 core. Memory is typically specified in bytes, but you can use suffixes like \"Mi\" or \"Gi\" for mebibytes or gibibytes, respectively.</p>"},{"location":"configuration/resource-management/#how-pods-with-resource-requests-are-scheduled","title":"How Pods with Resource Requests are Scheduled","text":"<p>When you create a Pod with resource requests, the scheduler looks for a node that has enough free resources to meet the Pod's requirements. If no such node exists, the Pod remains in a \"Pending\" state until resources become available.</p>"},{"location":"configuration/resource-management/#how-kubernetes-applies-resource-requests-and-limits","title":"How Kubernetes Applies Resource Requests and Limits","text":"<p>Once a Pod is scheduled, the <code>kubelet</code> on the node uses Linux features like cgroups to enforce the resource limits. This ensures that each container only uses the CPU and memory it's allocated.</p>"},{"location":"configuration/resource-management/#monitoring-compute-memory-resource-usage","title":"Monitoring Compute &amp; Memory Resource Usage","text":"<p>Kubernetes provides various metrics and tools to monitor resource usage. You can use built-in commands like <code>kubectl top</code> or deploy monitoring solutions like Prometheus to keep track of how much CPU and memory your Pods are using.</p>"},{"location":"configuration/resource-management/#local-ephemeral-storage","title":"Local Ephemeral Storage","text":"<p>Ephemeral storage is temporary disk space that a Pod can use during its lifecycle. It's local to the node and is deleted when the Pod is removed. You can specify requests and limits for ephemeral storage similar to CPU and memory.</p>"},{"location":"configuration/resource-management/#extended-resources","title":"Extended Resources","text":"<p>These are custom resources that you can define for specialized hardware or software requirements, such as GPUs or licenses. You can register these resources on nodes and then reference them in your Pod specifications.</p>"},{"location":"configuration/secrets/","title":"Secrets","text":""},{"location":"configuration/secrets/#what-is-a-secret","title":"What is a Secret?","text":"<p>A Kubernetes Secret is essentially a key-value store that holds sensitive information. Unlike ConfigMaps, which are designed for non-confidential data, Secrets are intended to store confidential data securely. They are used to manage sensitive information like API keys, passwords, and certificates. Secrets can be used by Pods to access this sensitive information without exposing it to the stack configuration.</p>"},{"location":"configuration/secrets/#types-of-secrets","title":"Types of Secrets","text":"<p>Kubernetes supports multiple types of Secrets, each designed for specific use-cases: - <code>Opaque</code>: This is the default type. It is used for arbitrary user-defined data. - <code>kubernetes.io/service-account-token</code>: Automatically generated and attached to service accounts. - <code>kubernetes.io/dockercfg</code>: Stores a serialized <code>.dockercfg</code> file required to authenticate against a Docker registry. - <code>kubernetes.io/dockerconfigjson</code>: Similar to dockercfg but for Docker's new <code>.dockerconfigjson</code> file. - <code>kubernetes.io/basic-auth</code>: Holds basic authentication credentials (username and password). - <code>kubernetes.io/ssh-auth</code>: Used for SSH authentication. - <code>kubernetes.io/tls</code>: Holds a TLS private key and certificate.</p>"},{"location":"configuration/secrets/#creating-a-secret","title":"Creating a Secret","text":"<p>You can create a Secret in multiple ways: - Using kubectl: <code>kubectl create secret generic my-secret --from-literal=key1=value1 --from-literal=key2=value2</code> - From a YAML file: You can define the Secret and its type in a YAML file and then apply it using <code>kubectl apply -f secret.yaml</code>.</p>"},{"location":"configuration/secrets/#using-secrets","title":"Using Secrets","text":"<p>Once a Secret is created, it can be consumed in various ways: - Environment Variables: Secrets can be exposed to a container as environment variables. - Volume Mount: Secrets can be mounted to a Pod as a read-only volume. - API: Pods can also access Secrets via the Kubernetes API.</p>"},{"location":"configuration/secrets/#best-practices","title":"Best Practices","text":"<p>For enhanced security: - Use Role-Based Access Control (RBAC) to restrict who can get/modify Secrets. - Don't store sensitive information in application code. - Use namespace to segregate Secrets relevant to different parts of your application.</p> <ol> <li> <p>Limitations Some limitations to be aware of: Secrets are stored in tmpfs on a node, which means they are not encrypted by default when at rest. A Secret is only sent to a node if a Pod on that node requires it, reducing the risk of exposure.</p> </li> <li> <p>Security Risks Security considerations include: Any user with the ability to create a Pod can potentially access any Secret that the Pod has permission to access. Therefore, it's crucial to limit who can create Pods or access Secrets.</p> </li> </ol>"},{"location":"containers/container-environment/","title":"Environment","text":""},{"location":"containers/container-environment/#container-environment","title":"Container Environment","text":"<ul> <li>The Kubernetes Container environment provides several important resources to Containers:<ul> <li>Filesystem: A combination of an image and one or more volumes.</li> <li>Container Information: Information about the Container itself.</li> <li>Cluster Information: Information about other objects in the cluster.</li> </ul> </li> </ul>"},{"location":"containers/container-environment/#container-information","title":"Container Information","text":"<ul> <li>The hostname of a Container is the name of the Pod in which the Container is running. This can be accessed through the hostname command or the gethostname function call in <code>libc</code>.</li> <li>The Pod name and namespace are available as environment variables through the downward API.</li> <li>User-defined environment variables from the Pod definition are also available to the Container, as are any environment variables specified statically in the container image.</li> </ul>"},{"location":"containers/container-environment/#cluster-information","title":"Cluster Information","text":"<ul> <li>A list of all services running when a Container was created is available to that Container as environment variables.</li> <li>This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.</li> <li>For a service named <code>foo</code> that maps to a Container named <code>bar</code>, variables like <code>FOO_SERVICE_HOST</code> and <code>FOO_SERVICE_PORT</code> are defined.</li> <li>Services have dedicated IP addresses and are available to the Container via DNS if the DNS addon is enabled.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/","title":"Lifecycle Hooks","text":"<p>Container lifecycle hooks allow containers to be aware of events in their management lifecycle and run specific code when these events occur.</p>"},{"location":"containers/container-lifecycle-hooks/#container-hooks","title":"Container Hooks","text":"<ul> <li>PostStart: This hook is executed immediately after a container is created. However, there's no guarantee that it will execute before the container's <code>ENTRYPOINT</code>. No parameters are passed to the handler.</li> <li>PreStop: This hook is called right before a container is terminated due to various reasons like API request, liveness/startup probe failure, etc. The hook must complete before the <code>TERM</code> signal to stop the container is sent.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/#hook-handler-implementations","title":"Hook Handler Implementations","text":"<ul> <li>Containers can implement two types of hook handlers:<ul> <li><code>Exec</code>: Executes a specific command inside the container's cgroups and namespaces.</li> <li><code>HTTP</code>: Executes an HTTP request against a specific endpoint on the container.</li> </ul> </li> </ul>"},{"location":"containers/container-lifecycle-hooks/#hook-handler-execution","title":"Hook Handler Execution","text":"<ul> <li>Hook calls are synchronous within the context of the Pod containing the container.</li> <li>For <code>PostStart</code> hooks, the Container <code>ENTRYPOINT</code> and hook fire asynchronously.</li> <li><code>PreStop</code> hooks must complete before the <code>TERM</code> signal can be sent.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/#hook-delivery-guarantees","title":"Hook Delivery Guarantees","text":"<ul> <li>Generally, hook delivery is intended to be at least once, meaning a hook may be called multiple times for any given event.</li> </ul>"},{"location":"containers/container-lifecycle-hooks/#debugging-hook-handlers","title":"Debugging Hook Handlers","text":"<ul> <li>Logs for hook handlers are not exposed in Pod events. If a handler fails, it broadcasts an event like <code>FailedPostStartHook</code> or <code>FailedPreStopHook</code>.</li> </ul>"},{"location":"containers/images/","title":"Images","text":""},{"location":"containers/images/#image-pull-operations","title":"Image Pull Operations","text":"<ul> <li>Several methods to provide credentials, including node configuration and <code>imagePullSecrets</code>.</li> <li>Requires keys for access.</li> </ul>"},{"location":"containers/images/#private-registries","title":"Private Registries","text":"<ul> <li>Automatically set based on conditions like whether a tag or digest is specified.</li> </ul>"},{"location":"containers/images/#default-image-pull-policies","title":"Default Image Pull Policies","text":"<ul> <li><code>Never</code>: Never pulls image; uses local if available.</li> <li><code>Always</code>: Always pulls image.</li> <li><code>IfNotPresent</code>: Pulls image only if not present.</li> <li>By default, the pull policy is set to <code>IfNotPresent</code>, meaning the image is pulled only if not already present locally.</li> </ul>"},{"location":"containers/images/#updating-images","title":"Updating Images","text":"<ul> <li>Tags can be added to identify versions.</li> <li>They can include a registry hostname and port number.</li> </ul>"},{"location":"containers/images/#image-names","title":"Image Names","text":"<ul> <li>They are pushed to a registry and then referred to in a Pod.</li> <li>Container images encapsulate an application and its dependencies.</li> </ul>"},{"location":"containers/overview/","title":"Overview","text":""},{"location":"containers/overview/#what-are-containers","title":"What are Containers?","text":"<ul> <li>Technology for packaging an application along with its runtime dependencies.</li> <li>Containers are repeatable and standardized, ensuring the same behavior wherever you run them.</li> <li>They decouple applications from the underlying host infrastructure, making deployment easier across different cloud or OS environments.</li> <li>In a Kubernetes cluster, each node runs the containers that form the Pods assigned to that node.</li> </ul>"},{"location":"containers/overview/#container-images","title":"Container Images","text":"<ul> <li>A container image is a ready-to-run software package.</li> <li>It contains everything needed to run an application: the code, runtime, application and system libraries, and default settings.</li> <li>Containers are intended to be stateless and immutable. Changes should be made by building a new image and recreating the container.</li> </ul>"},{"location":"containers/overview/#container-runtimes","title":"Container Runtimes","text":"<ul> <li>A fundamental component in Kubernetes for running containers effectively.</li> <li>Manages the execution and lifecycle of containers within the Kubernetes environment.</li> <li>Kubernetes supports container runtimes like containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).</li> <li>You can allow your cluster to pick the default container runtime for a Pod or specify the RuntimeClass for different settings.</li> </ul> <p> A simple way to think about the relationship of containers and Kubernetes is that each node can run multiple pods, which in turn each run a single container (typically).</p> <pre><code>graph TD\n    Node[Node] --&gt; Pod1[Pod]\n    Node --&gt; Pod2[Pod]\n    Node --&gt; Pod3[Pod]\n    Node --&gt; Pod4[Pod]\n\n    Pod1 --&gt; Container1[Container]\n    Pod2 --&gt; Container2[Container]\n    Pod3 --&gt; Container3[Container]\n    Pod4 --&gt; Container4[Container]\n\n    style Node fill:#f9f,stroke:#333,stroke-width:4px\n    style Pod1 fill:#bbf,stroke:#333,stroke-width:2px\n    style Pod2 fill:#bbf,stroke:#333,stroke-width:2px\n    style Pod3 fill:#bbf,stroke:#333,stroke-width:2px\n    style Pod4 fill:#bbf,stroke:#333,stroke-width:2px\n    style Container1 fill:#88f,stroke:#333,stroke-width:1px\n    style Container2 fill:#88f,stroke:#333,stroke-width:1px\n    style Container3 fill:#88f,stroke:#333,stroke-width:1px\n    style Container4 fill:#88f,stroke:#333,stroke-width:1px\n</code></pre>"},{"location":"containers/runtime-class/","title":"Runtime Class","text":""},{"location":"containers/runtime-class/#motivation","title":"Motivation","text":"<ul> <li>You can set different RuntimeClasses for different Pods to balance performance and security.</li> <li>For example, you might use a runtime that employs hardware virtualization for Pods requiring high levels of information security.</li> </ul>"},{"location":"containers/runtime-class/#setup","title":"Setup","text":"<ol> <li>Configure the CRI implementation on nodes: This is runtime-dependent and involves setting up configurations that have a corresponding handler name.</li> <li>Create RuntimeClass resources: Each configuration set up in step 1 should have an associated handler name. For each handler, create a corresponding RuntimeClass object.</li> </ol>"},{"location":"containers/runtime-class/#usage","title":"Usage","text":"<ul> <li>You can specify a <code>runtimeClassName</code> in the Pod spec to use a particular RuntimeClass.</li> <li>If the specified RuntimeClass doesn't exist or the CRI can't run the corresponding handler, the Pod will enter a Failed state.</li> </ul>"},{"location":"containers/runtime-class/#scheduling","title":"Scheduling","text":"<ul> <li>You can set constraints to ensure that Pods running with this RuntimeClass are scheduled to nodes that support it.</li> <li>This is done through the scheduling field for a RuntimeClass.</li> </ul>"},{"location":"pods/disruptions/","title":"Disruptions","text":""},{"location":"pods/disruptions/#types-of-disruptions","title":"Types of Disruptions","text":"<ul> <li>Involuntary Disruptions: Unavoidable cases like hardware failure, cloud provider issues, etc.</li> <li>Voluntary Disruptions: Actions initiated by the application owner or cluster administrator, such as draining a node for repair or upgrade.</li> </ul>"},{"location":"pods/disruptions/#mitigating-disruptions","title":"Mitigating Disruptions","text":"<ul> <li>Request the resources your pod needs.</li> <li>Replicate your application for higher availability.</li> <li>Use anti-affinity to spread applications across racks or zones.</li> </ul>"},{"location":"pods/disruptions/#pod-disruption-budgets-pdb","title":"Pod Disruption Budgets (PDB)","text":"<ul> <li>Allows you to specify how many pods of a replicated application can be down simultaneously.</li> <li>Cluster managers should respect PDBs by calling the Eviction API.</li> </ul>"},{"location":"pods/disruptions/#pod-disruption-conditions","title":"Pod Disruption Conditions","text":"<ul> <li>A beta feature that adds a dedicated condition to indicate that the Pod is about to be deleted due to a disruption.</li> </ul>"},{"location":"pods/disruptions/#separation-of-roles","title":"Separation of Roles","text":"<ul> <li>Discusses the benefits of separating the roles of Cluster Manager and Application Owner.</li> </ul>"},{"location":"pods/disruptions/#options-for-cluster-administrators","title":"Options for Cluster Administrators","text":"<ul> <li>Accept downtime, failover to another cluster, or write disruption-tolerant applications and use PDBs.</li> </ul>"},{"location":"pods/downward-api/","title":"Downward API","text":""},{"location":"pods/downward-api/#understanding-the-downward-api","title":"Understanding the Downward API","text":"<p>The Downward API is a feature in Kubernetes that allows pods to retrieve information about themselves or the cluster, which can be exposed to containers within the pod. This mechanism enables containers to consume details about the pod or the cluster without direct interaction with the Kubernetes API server.</p>"},{"location":"pods/downward-api/#two-methods-of-exposure","title":"Two Methods of Exposure","text":"<ul> <li> <p>Environment Variables: Specific pod and container fields can be exposed to running containers as environment variables. This is defined in the pod's configuration file and allows a container to access information like its own name, namespace, or node details.</p> </li> <li> <p>Volume Files: Kubernetes can also expose the same information through files in a volume. This special volume type is called the \"downward API volume,\" and it presents information in a filesystem that the container can read, providing a more dynamic approach to accessing the data.</p> </li> </ul>"},{"location":"pods/downward-api/#benefits-of-low-coupling","title":"Benefits of Low Coupling","text":"<p>The downward API is particularly useful for legacy applications or third-party tools that expect certain information to be available in the environment but are not designed to interact with Kubernetes directly. It simplifies the process of adapting non-native Kubernetes applications to the platform.</p>"},{"location":"pods/downward-api/#available-fields-and-resources","title":"Available Fields and Resources","text":"<ul> <li>Containers can access a variety of information via the Downward API, including:</li> <li>Pod Metadata: Such as the pod's name, namespace, annotations, labels, and unique UID.</li> <li>Resource Requests and Limits: Information about the CPU and memory limits and requests that are set for the container.</li> </ul>"},{"location":"pods/downward-api/#fallback-for-resource-limits","title":"Fallback for Resource Limits","text":"<p>When a container's resource limits are not explicitly defined in the pod specification, the <code>kubelet</code> can expose the default limits as the maximum allocatable resources available on the node. This ensures that the container has some information about the resources it can use, which is critical for managing application performance and resource usage.</p>"},{"location":"pods/downward-api/#use-cases","title":"Use Cases","text":"<ul> <li>Configuration Files: Applications that configure themselves through external files can use the Downward API volume to generate those files.</li> <li>Self-Awareness: Containers that need to be aware of their metadata (for logging, monitoring, or other operational purposes) can use the Downward API to get that information.</li> <li>Resource Management: Containers can adjust their behavior based on the resources available to them, which is particularly useful in high-density multi-tenant environments where resource constraints are common.  The Downward API provides a powerful way to maintain the abstraction that Kubernetes offers while still giving containers the necessary information to operate correctly in a dynamic and distributed system. </li> </ul> <pre><code>graph TD\n    Pod[Pod]\n    EnvVars[Environment Variables]\n    DownwardAPIVolume[Downward API Volume]\n    Container[Container]\n\n    Pod --&gt;|Exposes info via| EnvVars\n    Pod --&gt;|Exposes info via| DownwardAPIVolume\n    EnvVars --&gt;|Accessed by| Container\n    DownwardAPIVolume --&gt;|Accessed by| Container\n\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:2px;\n    class Pod,Container,EnvVars,DownwardAPIVolume k8s;\n</code></pre>"},{"location":"pods/ephemeral-containers/","title":"Ephemeral Containers","text":""},{"location":"pods/ephemeral-containers/#purpose","title":"Purpose","text":"<p>Ephemeral containers are a special type of container designed for temporary tasks like troubleshooting. They are not meant for building applications.</p>"},{"location":"pods/ephemeral-containers/#immutability","title":"Immutability","text":"<p>Once a Pod is created, you can't add a container to it. Ephemeral containers offer a way to inspect the state of an existing Pod without altering it.</p>"},{"location":"pods/ephemeral-containers/#resource-allocation","title":"Resource Allocation","text":"<p>Ephemeral containers don't have guarantees for resources or execution. They will never be automatically restarted.</p>"},{"location":"pods/ephemeral-containers/#limitations","title":"Limitations","text":"<p>Many fields that are available for regular containers are disallowed for ephemeral containers, such as ports and resource allocations.</p>"},{"location":"pods/ephemeral-containers/#creation-method","title":"Creation Method","text":"<p>These containers are created using a special <code>ephemeralcontainers</code> handler in the API, not directly through <code>pod.spec</code>.</p>"},{"location":"pods/ephemeral-containers/#use-cases","title":"Use-Cases","text":"<p>Useful for interactive troubleshooting when <code>kubectl exec</code> is insufficient, especially with distroless images that lack debugging utilities.</p>"},{"location":"pods/ephemeral-containers/#process-namespace-sharing","title":"Process Namespace Sharing","text":"<p>Enabling this feature is helpful for viewing processes in other containers within the same Pod.</p>"},{"location":"pods/init-containers/","title":"Init Containers","text":""},{"location":"pods/init-containers/#what-are-init-containers","title":"What are Init Containers?","text":"<ul> <li>Specialized containers that run before the application containers in a Pod. They can contain utilities or setup scripts not present in the application image.</li> <li>Init containers always run to completion, and each must complete successfully before the next one starts. If an init container fails, it is restarted until it succeeds, depending on the Pod's <code>restartPolicy</code>.</li> </ul>"},{"location":"pods/init-containers/#configuration","title":"Configuration","text":"<ul> <li>Init containers are specified in the Pod specification under the <code>initContainers</code> field, similar to how application containers are defined.</li> </ul>"},{"location":"pods/init-containers/#differences-from-regular-containers","title":"Differences from Regular Containers","text":"<ul> <li>Init containers support all the features of application containers but do not support lifecycle hooks or probes like <code>livenessProbe</code>, <code>readinessProbe</code>, and <code>startupProbe</code>.</li> </ul>"},{"location":"pods/init-containers/#esource-handling","title":"esource Handling","text":"<ul> <li>Resource requests and limits for init containers are managed differently than for application containers.</li> </ul>"},{"location":"pods/init-containers/#sequential-execution","title":"Sequential Execution","text":"<ul> <li>If multiple init containers are specified, they are run sequentially, and each must succeed before the next can run.</li> </ul>"},{"location":"pods/init-containers/#use-cases","title":"Use Cases:","text":"<ul> <li>Running utilities or custom code for setup that are not present in the application image.</li> <li>Blocking or delaying application container startup until certain preconditions are met.</li> <li>Limiting the attack surface by keeping unnecessary tools separate.</li> <li>Examples: The documentation provides YAML examples to demonstrate how to define a Pod with init containers.</li> <li>Advanced features like <code>activeDeadlineSeconds</code> can be used to prevent init containers from failing forever.</li> <li>Starting from Kubernetes v1.28, a feature gate named <code>SidecarContainers</code> allows specifying a <code>restartPolicy</code> for init containers independent of the Pod and other init containers.</li> <li>Resource Sharing: The highest of any particular resource request or limit defined on all init containers is the effective init request/limit for the Pod.</li> <li>Pod Restart Reasons: A Pod can restart, causing re-execution of init containers, for various reasons like Pod infrastructure container restart or all containers in a Pod being terminated.</li> </ul>"},{"location":"pods/overview/","title":"Overview","text":"<p>Pods are the smallest deployable units of computing in Kubernetes. A Pod is a group of one or more containers with shared storage and network resources. They are always co-located and co-scheduled, running in a shared context. Pods model an application-specific \"logical host\" and can contain one or more application containers that are tightly coupled.</p>"},{"location":"pods/overview/#key-concepts","title":"Key Concepts:","text":"<ul> <li> <p>What is a Pod?: A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.</p> </li> <li> <p>Using Pods: Pods are generally not created directly but are created using workload resources like Deployment or Job.</p> </li> <li> <p>Workload Resources: These are resources that manage one or more Pods for you. Examples include Deployment, StatefulSet, and DaemonSet.</p> </li> <li> <p>Pod Templates: These are specifications for creating Pods and are included in workload resources.</p> </li> <li> <p>Pod Update and Replacement: When the Pod template for a workload resource is changed, new Pods are created based on the updated template.</p> </li> <li> <p>Resource Sharing and Communication: Pods enable data sharing and communication among their constituent containers.</p> </li> <li> <p>Storage in Pods: A Pod can specify a set of shared storage volumes that all containers in the Pod can access.</p> </li> <li> <p>Pod Networking: Each Pod is assigned a unique IP address. Containers in a Pod share the network namespace, including the IP address and network ports.</p> </li> <li> <p>Privileged Mode for Containers: Any container in a Pod can run in privileged mode to use operating system administrative capabilities.</p> </li> <li> <p>Static Pods: These are managed directly by the <code>kubelet</code> daemon on a specific node, without the API server observing them.</p> </li> <li> <p>Container Probes: These are diagnostics performed periodically by the <code>kubelet</code> on a container.</p> </li> </ul>"},{"location":"pods/pod-lifecycle/","title":"Pod Lifecycle","text":""},{"location":"pods/pod-lifecycle/#phases","title":"Phases","text":"<p>A Pod's life begins in <code>Pending</code> when it's accepted by the Kubernetes system, but the container images are not yet running. It moves to <code>Running</code> when its containers start, but may enter <code>Succeeded</code> or <code>Failed</code> if it completes its task or encounters an error, respectively. <code>Unknown</code> indicates that the cluster cannot determine the Pod's state, often due to communication problems.</p>"},{"location":"pods/pod-lifecycle/#container-states","title":"Container States","text":"<ul> <li>Containers within a Pod can be in different states:</li> <li><code>Waiting</code>: The container is not yet running its workload, typically because it's pulling its image or waiting for its command to start.</li> <li><code>Running</code>: The container is executing without issues.</li> <li><code>Terminated</code>: The container has stopped, either because it completed its task or due to an error. This state is often accompanied by exit codes and status messages that can be checked using <code>kubectl</code>.</li> </ul>"},{"location":"pods/pod-lifecycle/#container-restart-policy","title":"Container Restart Policy","text":"<ul> <li>The <code>restartPolicy</code> field within a Pod specification dictates the Kubelet's behavior when handling container terminations:</li> <li><code>Always</code>: Automatically restart the container if it stops.</li> <li><code>OnFailure</code>: Restart only if the container exits with a non-zero exit status (indicative of failure).</li> <li><code>Never</code>: Do not automatically restart the container.</li> </ul>"},{"location":"pods/pod-lifecycle/#pod-conditions","title":"Pod Conditions","text":"<ul> <li>These are flags set by the Kubelet to provide more granular status than the phase:<ul> <li><code>PodScheduled</code>: Indicates if the pod has been scheduled to a node.</li> <li><code>ContainersReady</code>: All containers in the Pod are ready.</li> <li><code>Initialized</code>: All init containers have started successfully.</li> <li><code>Ready</code>: The Pod is able to serve requests and should be added to the load balancing pools of all matching services.</li> </ul> </li> </ul>"},{"location":"pods/pod-lifecycle/#custom-readiness-checks","title":"Custom readiness checks","text":"<ul> <li>Can be configured via <code>readinessGates</code> in a Pod's specification, allowing you to define additional conditions to be evaluated before considering a Pod as ready.</li> <li><code>PodReadyToStartContainers</code> is a hypothetical condition that could be used to signify network readiness, implying the Pod's network setup is complete and it's ready to start containers.</li> </ul>"},{"location":"pods/pod-lifecycle/#container-probes","title":"Container Probes","text":"<ul> <li>These are diagnostic tools used by the Kubelet to assess the health and readiness of a container:</li> <li><code>livenessProbe</code>: Determines if a container is running. If this probe fails, the Kubelet kills the container which may be restarted depending on the pod's <code>restartPolicy</code>.</li> <li><code>readinessProbe</code>: Determines if a container is ready to respond to requests. Failing this probe means the container gets removed from service endpoints.</li> <li><code>startupProbe</code>: Used for containers that take a long time to start. If this probe fails, the Kubelet will not start the liveness or readiness probes, giving the container more time to initialize.</li> </ul>"},{"location":"pods/pod-lifecycle/#using-probes","title":"Using Probes","text":"<ul> <li>Liveness Probes: Implement if you need to handle the container's inability to recover from a deadlock or other runtime issues internally.</li> <li>Readiness Probes: Utilize when your container needs to perform certain actions such as warming a cache or migrating a database before it can serve requests.</li> <li>Startup Probes: Employ for slow-starting containers to ensure that Kubernetes doesn't kill them before they're up and running.</li> </ul>"},{"location":"pods/pod-lifecycle/#termination-of-pods","title":"Termination of Pods","text":"<ul> <li>Pods are terminated gracefully, allowing for cleanup and shutdown procedures to complete. The Kubelet sends a <code>SIGTERM</code> signal to the containers, indicating that they should shut down. You can specify the grace period during which a container should complete its shutdown before being forcibly killed.</li> </ul>"},{"location":"pods/qos-classes/","title":"QoS Classes","text":""},{"location":"pods/qos-classes/#scheduler-behavior","title":"Scheduler Behavior","text":"<p>The <code>kube-scheduler</code> does not consider QoS class when selecting which Pods to preempt.</p>"},{"location":"pods/qos-classes/#resource-management","title":"Resource Management","text":"<p>The resource request of a Pod is the sum of the resource requests of its containers, and similarly for the resource limit.</p>"},{"location":"pods/qos-classes/#behavior-independent-of-qos-class","title":"Behavior Independent of QoS Class","text":"<p>Any container exceeding a resource limit will be killed and restarted, and Pods exceeding resource requests become candidates for eviction.</p>"},{"location":"pods/qos-classes/#memory-qos-with-cgroup-v2","title":"Memory QoS with cgroup v2","text":"<p>This feature, in alpha stage as of Kubernetes v1.22, uses the memory controller of cgroup v2 to guarantee memory resources.</p>"},{"location":"pods/qos-classes/#types-of-qos-classes","title":"Types of QoS Classes","text":"<ul> <li>Guaranteed: These Pods have strict resource limits and are least likely to be evicted.</li> <li>Burstable: These Pods have some lower-bound resource guarantees but do not require a specific limit.</li> <li>BestEffort: These Pods can use any available node resources and are the first to be evicted under resource pressure.</li> </ul>"},{"location":"pods/user-namespaces/","title":"User Namespaces","text":""},{"location":"pods/user-namespaces/#feature-state","title":"Feature State","text":"<p>This is an alpha feature introduced in Kubernetes verison 1.25</p>"},{"location":"pods/user-namespaces/#purpose","title":"Purpose","text":"<p>User namespaces isolate the user running inside the container from the one in the host. This enhances security by limiting the damage a compromised container can do to the host or other pods.</p>"},{"location":"pods/user-namespaces/#linux-only-feature","title":"Linux-only Feature","text":"<p>This feature is specific to Linux and requires support for <code>idmap</code> mounts on the filesystems used.</p>"},{"location":"pods/user-namespaces/#container-runtime-support","title":"Container Runtime Support","text":"<p>CRI-O version 1.25 and later support this feature. <code>Containerd</code> v1.7 is not compatible with certain Kubernetes versions in terms of user namespace support.</p>"},{"location":"pods/user-namespaces/#uidgid-mapping","title":"UID/GID Mapping","text":"<p>The <code>kubelet</code> will assign unique host UIDs/GIDs to each pod to ensure no overlap.</p>"},{"location":"pods/user-namespaces/#capabilities","title":"Capabilities","text":"<p>Capabilities granted to a pod are limited to the pod's user namespace and are mostly invalid outside of it.</p>"},{"location":"pods/user-namespaces/#limitations","title":"Limitations","text":"<p>When using a user namespace, you cannot use other host namespaces like network, IPC, or PID.</p>"},{"location":"pods/user-namespaces/#security","title":"Security","text":"<p>The feature mitigates the impact of certain CVEs by ensuring that UIDs/GIDs used by the host's files and host's processes are in a specific range.</p>"},{"location":"policies/limit-ranges/","title":"Limit Ranges","text":""},{"location":"policies/limit-ranges/#what-is-a-limitrange","title":"What is a LimitRange?","text":"<p>A LimitRange is a policy that sets constraints on the resource allocations for various object kinds like Pods or PersistentVolumeClaims within a namespace. It can enforce minimum and maximum compute resource usage per Pod or Container, set storage request limits for PersistentVolumeClaims, and even enforce a ratio between resource requests and limits.</p>"},{"location":"policies/limit-ranges/#how-does-it-work","title":"How Does it Work?","text":"<ol> <li>Default Values: If a Pod or Container doesn't specify compute resource requirements, the LimitRange admission controller will apply default request and limit values.</li> <li>Tracking Usage: The LimitRange ensures that resource usage does not exceed the minimum, maximum, and ratio defined in any LimitRange present in the namespace.</li> <li>Validation: If you try to create or update an object that violates a LimitRange constraint, the API server will reject the request with an <code>HTTP 403 Forbidden</code> status.</li> </ol>"},{"location":"policies/limit-ranges/#special-considerations","title":"Special Considerations","text":"<ul> <li>LimitRange validations occur only at the Pod admission stage, not on running Pods.</li> <li>If two or more LimitRange objects exist in a namespace, the default value applied is not deterministic.</li> <li>The LimitRange does not check the consistency of the default values it applies. For example, a default limit value could be less than the request value specified for the container, making the Pod unschedulable.</li> </ul>"},{"location":"policies/node-resource-managers/","title":"Node Resource Managers","text":""},{"location":"policies/node-resource-managers/#topology-manager","title":"Topology Manager","text":"<p>The Topology Manager is the central component that coordinates the resource allocation process. It works in conjunction with other managers like the CPU Manager, Device Manager, and Memory Manager to ensure that resources are allocated in a way that maximizes performance and minimizes latency.</p>"},{"location":"policies/node-resource-managers/#policies","title":"Policies","text":"<p>The Topology Manager uses policies to determine how to allocate resources. These policies can be configured to suit different types of workloads. For example, you might have a policy that prioritizes the allocation of CPUs that are physically close to each other for a latency-sensitive application.</p>"},{"location":"policies/node-resource-managers/#cpu-manager","title":"CPU Manager","text":"<p>This manager is responsible for allocating CPU resources to pods. It ensures that CPU-intensive workloads are scheduled on the appropriate CPUs to maximize performance.</p>"},{"location":"policies/node-resource-managers/#cpu-manager-policies","title":"CPU Manager Policies","text":"<p>Different policies can be applied to manage how CPUs are allocated. For example, you might use a \"static\" policy to pin pods to specific CPUs, ensuring that they always have the resources they need.</p>"},{"location":"policies/node-resource-managers/#device-manager","title":"Device Manager","text":"<p>This manager handles the allocation of hardware devices like GPUs or TPUs. It works in tandem with the Topology Manager to ensure that devices are allocated in a way that is most beneficial for the running pods.</p>"},{"location":"policies/node-resource-managers/#memory-manager","title":"Memory Manager","text":"<p>This manager is responsible for allocating memory resources, particularly hugepages, to pods. Hugepages are larger-than-normal pages that can be used to improve memory performance for certain types of workloads.</p>"},{"location":"policies/node-resource-managers/#memory-manager-policies","title":"Memory Manager Policies","text":"<p>Just like with CPUs, different policies can be applied to manage how memory is allocated. This can be particularly useful for workloads that require a large amount of memory or have specific latency requirements.</p>"},{"location":"policies/node-resource-managers/#configuration","title":"Configuration","text":"<p>Each of these managers has its own set of configurable parameters, allowing you to fine-tune how resources are allocated on a per-node basis. Understanding and configuring Node Resource Managers can be crucial for getting the most out of your Kubernetes clusters, especially for high-performance or latency-sensitive applications.</p>"},{"location":"policies/policies-overview/","title":"Overview","text":""},{"location":"policies/policies-overview/#api-objects-as-policies","title":"API Objects as Policies","text":"<ul> <li>NetworkPolicies: Restrict ingress and egress traffic for workloads.</li> <li>LimitRanges: Manage resource allocation constraints across different object kinds.</li> <li>ResourceQuotas: Limit resource consumption for a namespace.</li> </ul>"},{"location":"policies/policies-overview/#admission-controllers","title":"Admission Controllers","text":"<ul> <li>These run in the API server and can validate or mutate API requests.</li> <li>Example: The <code>AlwaysPullImages</code> admission controller modifies a new Pod to set the image pull policy to <code>Always</code>.</li> <li>Kubernetes has several built-in admission controllers that are configurable via the API server <code>-enable-admission-plugins</code> flag.</li> </ul>"},{"location":"policies/policies-overview/#validatingadmissionpolicy","title":"ValidatingAdmissionPolicy","text":"<ul> <li>Allows configurable validation checks to be executed in the API server using the Common Expression Language (CEL).</li> <li>Can be used to block, audit, and warn users about non-compliant configurations.</li> </ul>"},{"location":"policies/policies-overview/#dynamic-admission-control","title":"Dynamic Admission Control","text":"<ul> <li>These controllers run outside the API server as separate applications.</li> <li>They can perform complex checks, including those that require retrieval of other cluster resources and external data.</li> <li>Example: An image verification check can look up data from OCI registries to validate container image signatures and attestations.</li> </ul>"},{"location":"policies/policies-overview/#kubelet-configurations","title":"Kubelet Configurations","text":"<ul> <li>Some Kubelet configurations act as policies, such as Process ID limits and reservations, and Node Resource Managers.</li> </ul>"},{"location":"policies/policies-overview/#implementation","title":"Implementation","text":"<ul> <li>Dynamic Admission Controllers that act as flexible policy engines are being developed in the Kubernetes ecosystem. Some examples are Kubewarden, Kyverno, OPA Gatekeeper, and Polaris.</li> </ul>"},{"location":"policies/process-id-limits/","title":"Process ID Limits","text":""},{"location":"policies/process-id-limits/#why-pid-limiting","title":"Why PID Limiting?","text":"<ul> <li>PIDs are a fundamental resource on nodes.</li> <li>It's easy to hit the task limit without hitting other resource limits, causing instability.</li> <li>Cluster administrators need mechanisms to prevent PID exhaustion that could affect host daemons like <code>kubelet</code> or <code>kube-proxy</code>.</li> </ul>"},{"location":"policies/process-id-limits/#how-to-configure-pid-limiting","title":"How to Configure PID Limiting","text":"<ul> <li>Node-Level PID Limits: You can reserve a number of PIDs for system use and Kubernetes system daemons using the -system-reserved and <code>-kube-reserved</code> command-line options for the <code>kubelet</code>.</li> <li>Pod-Level PID Limits: The number of PIDs can be limited at the node level for each Pod. This is configured using the <code>-pod-max-pids</code> command-line parameter to the <code>kubelet</code> or by setting <code>PodPidsLimit</code> in the <code>kubelet</code> configuration file.</li> </ul>"},{"location":"policies/process-id-limits/#pid-based-eviction","title":"PID-Based Eviction","text":"<ul> <li><code>Kubelet</code> can terminate a Pod if it's consuming an abnormal number of PIDs.</li> <li>You can configure soft and hard eviction policies.</li> <li>The eviction signal value is calculated periodically and does not enforce the limit.</li> </ul>"},{"location":"policies/process-id-limits/#limitations","title":"Limitations","text":"<ul> <li>Per-Pod PID limiting protects one Pod from another but doesn't protect node agents from PID exhaustion.</li> <li>Pod-defined PID limits are not currently supported.</li> </ul>"},{"location":"policies/resource-quotas/","title":"Resource Quotas","text":""},{"location":"policies/resource-quotas/#what-is-a-resource-quota","title":"What is a Resource Quota?","text":"<p>A Resource Quota is an object in Kubernetes that sets limits on resource usage for each namespace. This is particularly useful when multiple teams or services share a Kubernetes cluster and you want to prevent any single team or service from consuming all the resources.</p>"},{"location":"policies/resource-quotas/#how-does-it-work","title":"How Does It Work?","text":"<ul> <li>Namespaces: Different teams usually work in different namespaces. This isolation is often enforced using Role-Based Access Control (RBAC).</li> <li>ResourceQuota Object: For each namespace, an administrator creates a ResourceQuota object that defines the resource limits.</li> <li>Resource Tracking: As users create Kubernetes resources like pods and services in a namespace, the quota system keeps track of the usage to ensure it doesn't exceed the limits defined in the ResourceQuota object.</li> <li>Violation &amp; Enforcement: If a resource request violates a quota, the request will fail with an HTTP 403 FORBIDDEN status, along with a message explaining the violated constraint.</li> </ul>"},{"location":"policies/resource-quotas/#types-of-resources-that-can-be-quota-ed","title":"Types of Resources That Can Be Quota-ed","text":"<ul> <li>Compute Resources: CPU and memory can be limited. You can set both \"requests\" and \"limits\" for these resources.</li> <li>Storage Resources: You can limit the storage requested in Persistent Volume Claims (PVCs).</li> <li>Object Count: You can also limit the number of specific types of objects, like Pods, Services, etc.</li> <li>Extended Resources: From Kubernetes 1.10 onwards, you can also quota extended resources like GPUs.</li> <li>Priority Class: You can set quotas based on the priority class of the pods.</li> </ul>"},{"location":"policies/resource-quotas/#special-scopes","title":"Special Scopes","text":"<p>Resource quotas can also have scopes that further restrict what is measured by the quota. For example, you can set a quota that only counts resources for Pods that are not in a terminal state (i.e., their <code>.status.phase</code> is neither <code>Failed</code> nor <code>Succeeded</code>).</p>"},{"location":"policies/resource-quotas/#enabling-resource-quotas","title":"Enabling Resource Quotas","text":"<p>Resource quotas are usually enabled by default in many Kubernetes distributions. They are activated when the API server has <code>ResourceQuota</code> listed in its <code>--enable-admission-plugins</code> flag.</p>"},{"location":"policies/resource-quotas/#practical-examples","title":"Practical Examples","text":"<ul> <li>Team Quotas: In a cluster with 32 GiB RAM and 16 cores, you could let Team A use 20 GiB and 10 cores, Team B use 10 GiB and 4 cores, and keep the rest in reserve.</li> <li>Environment Quotas: Limit the \"testing\" namespace to 1 core and 1 GiB RAM, while letting the \"production\" namespace use any amount.</li> </ul>"},{"location":"scheduling-and-eviction/api-initiated-eviction/","title":"API Initiated Eviction","text":""},{"location":"scheduling-and-eviction/api-initiated-eviction/#api-initiated-evictions-vs-regular-deletion","title":"API-Initiated Evictions vs Regular Deletion","text":"<ul> <li>Regular Deletion: When you delete a pod using <code>kubectl delete pod</code>, the pod is terminated immediately without any checks for ongoing processes or disruption budgets.</li> <li>API-Initiated Evictions: When you use the Eviction API, Kubernetes respects the <code>PodDisruptionBudget</code> and <code>terminationGracePeriodSeconds</code> settings, ensuring that the pod is terminated in a way that minimizes disruption to your application.</li> </ul>"},{"location":"scheduling-and-eviction/api-initiated-eviction/#creating-an-eviction-object","title":"Creating an Eviction Object","text":"<ul> <li>To initiate an eviction, you create an Eviction object. This is similar to sending a <code>DELETE</code> request to the API server but with more control.</li> <li>The Eviction object specifies the pod to be evicted and can also include additional parameters like <code>deleteOptions</code>.</li> </ul>"},{"location":"scheduling-and-eviction/api-initiated-eviction/#admission-checks","title":"Admission Checks","text":"<ul> <li>When you request an eviction, the API server performs several checks to ensure that the eviction can proceed safely.</li> <li>PodDisruptionBudget: Checks if the eviction would violate any configured <code>PodDisruptionBudgets</code>.</li> <li>Node Conditions: Checks if the node on which the pod is running is in a condition to handle the eviction.  Based on these checks, the API server may respond with:  </li> <li><code>200 OK</code>: Eviction can proceed.</li> <li><code>429 Too Many Requests</code>: Eviction cannot proceed due to <code>PodDisruptionBudget</code> violation.</li> <li><code>500 Internal Server Error</code>: Eviction cannot proceed due to an internal error.</li> </ul>"},{"location":"scheduling-and-eviction/api-initiated-eviction/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Stuck Evictions: Sometimes, you may find that evictions are not proceeding and are stuck returning 429 or 500 responses.</li> <li>Abort or Pause: You may need to abort or pause any automated operations that are causing the API server to be overwhelmed.</li> <li>Direct Deletion: As a last resort, you can directly delete the pod, but this is not recommended as it bypasses all safety checks.</li> </ul>"},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/","title":"Assigning Pods to Nodes","text":""},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/#assigning-pods-to-nodes","title":"Assigning Pods to Nodes","text":"<p>Automatic Scheduling: By default, Kubernetes automatically schedules Pods to Nodes based on available resources. However, you can override this behavior for specific use-cases.</p>"},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/#methods-for-manual-scheduling","title":"Methods for Manual Scheduling","text":"<ul> <li><code>nodeSelector</code>: Simplest form, matches node labels.</li> <li><code>Affinity and Anti-affinity</code>: More expressive than nodeSelector.</li> <li><code>nodeName</code>: Directly specify the node.</li> <li><code>Pod Topology Spread Constraints</code>: Control how Pods are spread across your cluster.</li> </ul>"},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/#node-labels","title":"Node Labels","text":"<ul> <li>Nodes can have labels that you manually attach or that Kubernetes automatically populates.</li> <li>Labels can be used for node isolation and security.</li> </ul>"},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/#nodeselector","title":"NodeSelector","text":"<p>A field in the Pod spec that specifies node labels to match.</p>"},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/#affinity-and-anti-affinity","title":"Affinity and Anti-affinity","text":"<ul> <li><code>Node Affinity</code>: Similar to nodeSelector but more expressive.</li> <li><code>requiredDuringSchedulingIgnoredDuringExecution</code>: Hard requirement.</li> <li><code>preferredDuringSchedulingIgnoredDuringExecution</code>: Soft requirement.</li> <li><code>Inter-pod Affinity/Anti-affinity</code>: Allows you to specify rules based on labels of other Pods, not just node labels.</li> </ul>"},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/#node-affinity-weight","title":"Node Affinity Weight","text":"<p>You can assign weights to preferred rules. The scheduler uses these to score nodes.</p>"},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/#inter-pod-affinity-and-anti-affinity","title":"Inter-pod Affinity and Anti-affinity","text":"<p>Allows you to specify rules based on labels of other Pods that are already running on the node.</p>"},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/#practical-use-cases","title":"Practical Use-cases","text":"<p>Useful when used with higher-level objects like ReplicaSets, StatefulSets, and Deployments.</p>"},{"location":"scheduling-and-eviction/assigning-pods-to-nodes/#example-redis-cache-deployment","title":"Example: Redis Cache Deployment","text":"<p>An example is given for a Redis cache where <code>podAntiAffinity</code> is used to ensure that no two replicas with the label <code>app=store</code> are scheduled on the same node.</p>"},{"location":"scheduling-and-eviction/dynamic-resource-allocation/","title":"Dynamic Resource Allocation","text":""},{"location":"scheduling-and-eviction/dynamic-resource-allocation/#key-concepts","title":"Key Concepts","text":"<ul> <li><code>ResourceClass</code>: Defines the resource driver and common parameters for a certain kind of resource.</li> <li><code>ResourceClaim</code>: Specifies a particular resource instance required by a workload.</li> <li><code>ResourceClaimTemplate</code>: Defines the spec for creating ResourceClaims.</li> <li><code>PodSchedulingContext</code>: Used internally for coordinating pod scheduling when ResourceClaims need to be allocated.</li> </ul>"},{"location":"scheduling-and-eviction/dynamic-resource-allocation/#example-configuration","title":"Example Configuration","text":"<p>Here's an example configuration for a fictional resource driver:</p> <pre><code>apiVersion: resource.k8s.io/v1alpha2\nkind: ResourceClass\nname: resource.example.com\ndriverName: resource-driver.example.com\n---\napiVersion: cats.resource.example.com/v1\nkind: ClaimParameters\nname: large-black-cat-claim-parameters\nspec:\n  color: black\n  size: large\n---\napiVersion: resource.k8s.io/v1alpha2\nkind: ResourceClaimTemplate\nmetadata:\n  name: large-black-cat-claim-template\nspec:\n  spec:\n    resourceClassName: resource.example.com\n    parametersRef:\n      apiGroup: cats.resource.example.com\n      kind: ClaimParameters\n      name: large-black-cat-claim-parameters\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-cats\nspec:\n  containers:\n  - name: container0\n    image: ubuntu:20.04\n    command: [\"sleep\", \"9999\"]\n    resources:\n      claims:\n      - name: cat-0\n  - name: container1\n    image: ubuntu:20.04\n    command: [\"sleep\", \"9999\"]\n    resources:\n      claims:\n      - name: cat-1\n  resourceClaims:\n  - name: cat-0\n    source:\n      resourceClaimTemplateName: \\\n      large-black-cat-claim-template\n  - name: cat-1\n    source:\n      resourceClaimTemplateName: \\\n      large-black-cat-claim-template\n</code></pre>"},{"location":"scheduling-and-eviction/dynamic-resource-allocation/#scheduling","title":"Scheduling","text":"<p>Resource drivers are responsible for marking ResourceClaims as \"allocated\" once resources for them are reserved. This informs the scheduler where in the cluster a ResourceClaim is available.</p>"},{"location":"scheduling-and-eviction/dynamic-resource-allocation/#monitoring","title":"Monitoring","text":"<p>The <code>kubelet</code> provides a gRPC service for the discovery of dynamic resources of running Pods.</p>"},{"location":"scheduling-and-eviction/dynamic-resource-allocation/#enabling-the-feature","title":"Enabling the Feature","text":"<p>This is an alpha feature and needs to be enabled explicitly. You also need to install a resource driver for specific resources that are meant to be managed using this API.</p>"},{"location":"scheduling-and-eviction/kubernetes-scheduler/","title":"Kubernetes Scheduler","text":""},{"location":"scheduling-and-eviction/kubernetes-scheduler/#scheduling-overview","title":"Scheduling Overview","text":"<ul> <li>Role of Scheduler: The scheduler watches for newly created Pods that have no Node assigned and is responsible for finding the best Node for each Pod.</li> <li>Scheduling Principles: The scheduler takes into account various factors like resource requirements, hardware/software constraints, affinity and anti-affinity rules, etc., to make the scheduling decision.</li> </ul>"},{"location":"scheduling-and-eviction/kubernetes-scheduler/#kube-scheduler","title":"kube-scheduler","text":"<ul> <li>Default Scheduler: <code>kube-scheduler</code> is the default scheduler in Kubernetes and is part of the control plane.</li> <li>Customization: It's designed to allow you to write your own scheduling component if needed.</li> <li>Feasible Nodes: Nodes that meet the scheduling requirements for a Pod are called feasible nodes. If no nodes are suitable, the Pod remains unscheduled.</li> <li>Scoring: After filtering feasible Nodes, the scheduler scores them based on a set of functions and picks the Node with the highest score to run the Pod.</li> </ul>"},{"location":"scheduling-and-eviction/kubernetes-scheduler/#node-selection","title":"Node Selection","text":"<p>Two-Step Operation: Node selection is done in two steps: Filtering and Scoring:  </p> <ol> <li>Filtering: This step finds the set of Nodes where it's feasible to schedule the Pod. For example, it checks if a Node has enough resources to meet the Pod's requirements.</li> <li>Scoring: In this step, the scheduler ranks the remaining Nodes based on active scoring rules.</li> </ol>"},{"location":"scheduling-and-eviction/kubernetes-scheduler/#configuring-the-scheduler","title":"Configuring the Scheduler","text":"<ul> <li>Scheduling Policies: These allow you to configure predicates for filtering and priorities for scoring.</li> <li>Scheduling Profiles: These allow you to configure plugins that implement different scheduling stages like <code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code>, etc.</li> </ul>"},{"location":"scheduling-and-eviction/node-pressure-eviction/","title":"Node Pressure Eviction","text":""},{"location":"scheduling-and-eviction/node-pressure-eviction/#node-pressure-eviction","title":"Node-Pressure Eviction","text":"<ul> <li>Purpose: To proactively terminate pods to reclaim resources on nodes.</li> <li>Monitored Resources: Memory, disk space, filesystem inodes.</li> <li>Eviction Phases: Sets the phase for selected pods to <code>Failed</code> and terminates them.</li> <li>Constraints: Doesn't respect <code>PodDisruptionBudget</code> or <code>terminationGracePeriodSeconds</code>.</li> </ul>"},{"location":"scheduling-and-eviction/node-pressure-eviction/#self-healing-behavior","title":"Self-Healing Behavior","text":"<ul> <li>Workload Management: If pods are managed by objects like StatefulSet or Deployment, new pods replace the evicted ones.</li> <li>Static Pods: <code>Kubelet</code> tries to create a replacement for evicted static pods.</li> </ul>"},{"location":"scheduling-and-eviction/node-pressure-eviction/#eviction-signals-and-thresholds","title":"Eviction Signals and Thresholds","text":"<ul> <li>Eviction Signals: Current state of a particular resource.</li> <li>Eviction Thresholds: Minimum amount of the resource that should be available.</li> <li>Monitoring Intervals: Evaluated based on housekeeping-interval, default is 10s.</li> </ul>"},{"location":"scheduling-and-eviction/node-pressure-eviction/#types-of-eviction-thresholds","title":"Types of Eviction Thresholds","text":"<ul> <li>Soft Eviction Thresholds: Have a grace period.</li> <li>Hard Eviction Thresholds: No grace period, immediate termination.</li> </ul>"},{"location":"scheduling-and-eviction/node-pressure-eviction/#pod-selection-for-eviction","title":"Pod Selection for Eviction","text":"<ul> <li>Criteria: Resource usage, Pod Priority, and resource usage relative to requests.</li> <li>Eviction Order: BestEffort or Burstable pods where usage exceeds requests are evicted first.</li> </ul>"},{"location":"scheduling-and-eviction/node-pressure-eviction/#node-conditions","title":"Node Conditions","text":"<p><code>MemoryPressure</code>, <code>DiskPressure</code>, <code>PIDPressure</code>: Reflect that the node is under pressure.</p>"},{"location":"scheduling-and-eviction/node-pressure-eviction/#known-issues","title":"Known Issues","text":"<ul> <li>Memory Pressure Observation: <code>Kubelet</code> may not observe memory pressure immediately.</li> <li>Active File Memory: Not considered as available memory.</li> </ul>"},{"location":"scheduling-and-eviction/node-pressure-eviction/#good-practices","title":"Good Practices","text":"<ul> <li>Schedulable Resources: Make sure the scheduler doesn't schedule pods that will trigger eviction.</li> <li>DaemonSets: Give high enough priority to avoid eviction.</li> </ul>"},{"location":"scheduling-and-eviction/node-pressure-eviction/#flags-and-configuration","title":"Flags and Configuration","text":"<p>Various flags like <code>-eviction-hard</code>, <code>-system-reserved</code>, <code>-eviction-minimum-reclaim</code> etc. are used for fine-tuning.</p>"},{"location":"scheduling-and-eviction/pod-overhead/","title":"Pod Overhead","text":""},{"location":"scheduling-and-eviction/pod-overhead/#what-is-pod-overhead","title":"What is Pod Overhead?","text":"<p>Pod Overhead is a feature in Kubernetes that accounts for the resources consumed by the Pod infrastructure on top of the container requests and limits. This is particularly useful when you're running Pods in a runtime that has additional overhead, such as Kata Containers or gVisor.</p>"},{"location":"scheduling-and-eviction/pod-overhead/#how-it-works","title":"How It Works","text":"<p>PodOverhead Field: The <code>PodOverhead</code> field is added to the Pod specification under the <code>spec</code> section. This field specifies the additional overhead in terms of CPU and memory.</p> <p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: overhead-demo\nspec:\n  overhead:\n    cpu: \"200m\"\n    memory: \"120Mi\"\n</code></pre> </p> <ul> <li> <p>Scheduler's Role: The Kubernetes scheduler takes into account this overhead when scheduling the Pod. This ensures that the Node has enough resources to accommodate not just the containers but also the Pod overhead.</p> </li> <li> <p>Resource Accounting: The <code>kubelet</code> also includes the overhead when calculating the Pod's resource usage. This is reflected in metrics and used in eviction decisions. </p> </li> </ul>"},{"location":"scheduling-and-eviction/pod-overhead/#admission-controller","title":"Admission Controller","text":"<ul> <li>RuntimeClass: You can define the overhead in the <code>RuntimeClass</code> if you're using multiple runtimes in your cluster. This makes it easier to manage overhead settings across different types of Pods.</li> </ul> <p><pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: kata-fc\nhandler: kata-fc\noverhead:\n  podFixed:\n    cpu: \"250m\"\n    memory: \"160Mi\"\n</code></pre> </p> <ul> <li>Automatic Injection: When you create a Pod that specifies a RuntimeClass with Pod overhead defined, the overhead settings are automatically injected into the Pod spec.</li> </ul>"},{"location":"scheduling-and-eviction/pod-overhead/#monitoring-and-metrics","title":"Monitoring and Metrics","text":"<ul> <li> <p>Resource Metrics: Metrics related to CPU and memory usage will include the overhead, providing a more accurate representation of actual resource utilization.</p> </li> <li> <p>Eviction: The <code>kubelet</code> considers the overhead when making eviction decisions, ensuring that it doesn't evict Pods based on inaccurate resource usage data.</p> </li> </ul>"},{"location":"scheduling-and-eviction/pod-priority-and-preemption/","title":"Pod Priority and Preemption","text":""},{"location":"scheduling-and-eviction/pod-priority-and-preemption/#how-to-use-priority-and-preemption","title":"How to Use Priority and Preemption","text":"<ul> <li>Add one or more <code>PriorityClasses</code>.</li> <li>Create Pods with <code>priorityClassName</code> set to one of the added <code>PriorityClasses</code>.</li> </ul>"},{"location":"scheduling-and-eviction/pod-priority-and-preemption/#priorityclass","title":"PriorityClass","text":"<ul> <li>Defines a mapping from a priority class name to the integer value of the priority.</li> <li>Optional fields: <code>globalDefault</code> and <code>description</code>.</li> </ul>"},{"location":"scheduling-and-eviction/pod-priority-and-preemption/#non-preempting-priorityclass","title":"Non-preempting PriorityClass","text":"<p>Pods with preemptionPolicy: Never cannot preempt other pods but may still be preempted by higher-priority pods.</p>"},{"location":"scheduling-and-eviction/pod-priority-and-preemption/#pod-priority","title":"Pod Priority","text":"<p>After you have one or more PriorityClasses, you can create Pods that specify one of those PriorityClass names in their specifications.</p>"},{"location":"scheduling-and-eviction/pod-priority-and-preemption/#effect-of-pod-priority-on-scheduling-order","title":"Effect of Pod Priority on Scheduling Order","text":"<p>The scheduler orders pending Pods by their priority.</p>"},{"location":"scheduling-and-eviction/pod-priority-and-preemption/#preemption","title":"Preemption","text":"<p>If no Node is found that satisfies all the specified requirements of the Pod, preemption logic is triggered.</p>"},{"location":"scheduling-and-eviction/pod-priority-and-preemption/#limitations-of-preemption","title":"Limitations of Preemption","text":"<ul> <li>Graceful termination of preemption victims.</li> <li><code>PodDisruptionBudget</code> is supported, but not guaranteed.</li> <li>Inter-Pod affinity on lower-priority Pods.</li> <li>Cross node preemption is not supported.</li> </ul>"},{"location":"scheduling-and-eviction/pod-scheduling-readiness/","title":"Pod Scheduling Readiness","text":""},{"location":"scheduling-and-eviction/pod-scheduling-readiness/#schedulinggates","title":"SchedulingGates","text":"<ul> <li> <p>What It Is: <code>SchedulingGates</code> is a new field in the Pod specification. It's an array of strings, where each string represents a condition that the Pod must meet before it can be scheduled.</p> </li> <li> <p>Immutability: Once a Pod is created with <code>SchedulingGates</code>, you can't add new gates to it. However, you can remove existing ones to make the Pod schedulable.</p> </li> <li> <p>Use Case: Imagine you have a multi-stage deployment process where a Pod needs to pass some pre-conditions before it can be scheduled. You can use <code>SchedulingGates</code> to ensure that the Pod only gets scheduled after these conditions are met.</p> </li> </ul>"},{"location":"scheduling-and-eviction/pod-scheduling-readiness/#observability","title":"Observability","text":"<ul> <li> <p>New Metrics Label: The <code>scheduler_pending_pods</code> metric now includes a \"gated\" label. This helps in monitoring Pods that are intentionally not being scheduled due to <code>SchedulingGates</code>.</p> </li> <li> <p>Monitoring: This feature allows for better observability into why certain Pods are not being scheduled, making it easier to debug issues related to Pod scheduling.</p> </li> </ul>"},{"location":"scheduling-and-eviction/pod-scheduling-readiness/#mutable-pod-scheduling-directives","title":"Mutable Pod Scheduling Directives","text":"<ul> <li> <p>What It Is: Starting from Kubernetes v1.27, you can change the scheduling directives of a Pod even if it has <code>SchedulingGates</code>.</p> </li> <li> <p>Constraints: The catch is that you can only \"tighten\" the scheduling directives. This means you can make them more restrictive, but not more permissive. For example, if a Pod was initially set to be scheduled on nodes with a certain label, you can change it to only be scheduled on nodes with that label and an additional condition.</p> </li> <li> <p>Use Case: This is useful in scenarios where you might need to adjust the scheduling conditions of a Pod due to changes in cluster state or workload requirements, without having to delete and recreate the Pod.</p> </li> </ul>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/","title":"Pod Topology Spread Constraints","text":""},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#motivation","title":"Motivation","text":"<p>The primary motivation is to distribute Pods in a way that they are not all scheduled on the same node or same zone, which could be a single point of failure. For example, if you have a service that scales its Pods automatically, you don't want all Pods to be on the same node. As you scale up, you might also want to consider network latency and costs, aiming to distribute Pods across different data centers or zones.</p>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#topologyspreadconstraints-field","title":"topologySpreadConstraints Field","text":"<p>This is the field you add to your Pod spec to define the constraints. It has several sub-fields: - <code>maxSkew</code>: Defines how unevenly Pods may be distributed. A lower number means a more even distribution. - <code>topologyKey</code>: The key for node labels to define a topology (e.g., zone, node, etc.) - <code>whenUnsatisfiable</code>: What to do if the constraint can't be satisfied (DoNotSchedule or ScheduleAnyway). - <code>labelSelector</code>: Used to find Pods that these constraints apply to. - <code>minDomains</code>: (Optional) Minimum number of domains (like zones or nodes) that must be eligible for Pod placement.</p>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#node-labels","title":"Node Labels","text":"<p>Nodes should be labeled with the topology keys you intend to use, like <code>zone</code> or <code>region</code>. These labels are used by the scheduler to make decisions.</p>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#multiple-constraints","title":"Multiple Constraints","text":"<p>You can define multiple <code>topologySpreadConstraints</code>. In such cases, all constraints must be satisfied for a Pod to be scheduled.</p>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#interaction-with-node-affinity","title":"Interaction with Node Affinity","text":"<p>If a Pod also has node affinity rules, then the scheduler will consider those rules in conjunction with the topology spread constraints.</p>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#example-1-basic-spread-constraints","title":"Example 1: Basic Spread Constraints","text":"<p>This example ensures that the Pods are spread across different zones.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  topologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: zone\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        app: myapp\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#example-2-multiple-constraints","title":"Example 2: Multiple Constraints","text":"<p>This example ensures that the Pods are spread both across zones and nodes.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  topologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: zone\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        app: myapp\n  - maxSkew: 2\n    topologyKey: node\n    whenUnsatisfiable: ScheduleAnyway\n    labelSelector:\n      matchLabels:\n        app: myapp\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#example-3-using-mindomains","title":"Example 3: Using minDomains","text":"<p>This example ensures that Pods are spread across at least 2 zones.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  topologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: zone\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        app: myapp\n    minDomains: 2\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#example-4-interaction-with-node-affinity","title":"Example 4: Interaction with Node Affinity","text":"<p>This example shows how node affinity can be combined with spread constraints.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringScheduling \\\n      IgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: hardware\n            operator: In\n            values:\n            - fast-ssd\n  topologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: zone\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        app: myapp\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"scheduling-and-eviction/pod-topology-spread-constraints/#implicit-conventions","title":"Implicit Conventions","text":"<ul> <li>Only Pods in the same namespace as the incoming Pod are considered as matching candidates.</li> <li>Also, any nodes that don't have the <code>topologyKey</code> specified in <code>topologySpreadConstraints</code> are bypassed.</li> <li>This feature is particularly useful for large, distributed, and dynamic clusters where you want to control the Pod distribution for reasons like high availability, data locality, and load balancing.</li> </ul>"},{"location":"scheduling-and-eviction/resource-bin-packing/","title":"Resource Bin Packing","text":""},{"location":"scheduling-and-eviction/resource-bin-packing/#mostallocated-strategy","title":"MostAllocated Strategy","text":"<p>This strategy scores nodes based on resource utilization, favoring nodes with higher allocation. You can set weights for each resource type to influence the node score. Here's an example configuration:</p> <pre><code>apiVersion: kubescheduler.config\\\n.k8s.io/v1beta3\nkind: KubeSchedulerConfiguration\nprofiles:\n- pluginConfig:\n  - args:\n      scoringStrategy:\n        resources:\n        - name: cpu\n          weight: 1\n        - name: memory\n          weight: 1\n        - name: intel.com/foo\n          weight: 3\n        - name: intel.com/bar\n          weight: 3\n        type: MostAllocated\n    name: NodeResourcesFit\n</code></pre>"},{"location":"scheduling-and-eviction/resource-bin-packing/#requestedtocapacityratio-strategy","title":"RequestedToCapacityRatio Strategy","text":"<p>This strategy allows users to specify resources and their weights to score nodes based on the request-to-capacity ratio. It's particularly useful for bin packing extended resources. Here's an example configuration:  </p> <pre><code>apiVersion: kubescheduler.config\\\n.k8s.io/v1beta3\nkind: KubeSchedulerConfiguration\nprofiles:\n- pluginConfig:\n  - args:\n      scoringStrategy:\n        resources:\n        - name: intel.com/foo\n          weight: 3\n        - name: intel.com/bar\n          weight: 3\n        requestedToCapacityRatio:\n          shape:\n          - utilization: 0\n            score: 0\n          - utilization: 100\n            score: 10\n        type: RequestedToCapacityRatio\n    name: NodeResourcesFit\n</code></pre>"},{"location":"scheduling-and-eviction/resource-bin-packing/#tuning-the-score-function","title":"Tuning the Score Function","text":"<pre><code>shape:\n  - utilization: 0\n    score: 0\n  - utilization: 100\n    score: 10\n</code></pre>"},{"location":"scheduling-and-eviction/scheduler-performance-tuning/","title":"Scheduler Performance Tuning","text":""},{"location":"scheduling-and-eviction/scheduler-performance-tuning/#key-points","title":"Key Points","text":"<ul> <li> <p>Balancing Latency and Accuracy: In large clusters, you can balance the scheduler's behavior between latency (quick placement of new Pods) and accuracy (making fewer poor placement decisions).</p> </li> <li> <p>Setting the Threshold: The <code>percentageOfNodesToScore</code> option in the <code>KubeSchedulerConfiguration</code> setting determines a threshold for scheduling nodes. It accepts values between 0 and 100. A value of 0 indicates that the scheduler should use its default setting.</p> </li> <li> <p>Node Scoring Threshold: To improve performance, the scheduler can stop looking for feasible nodes once it has found enough. You specify a threshold as a percentage of all nodes in your cluster.</p> </li> <li> <p>Default Threshold: If you don't specify a threshold, Kubernetes calculates a figure using a linear formula. The lower bound for the automatic value is 5%.</p> </li> </ul>"},{"location":"scheduling-and-eviction/scheduler-performance-tuning/#example-configuration","title":"Example Configuration","text":"<pre><code>apiVersion: kubescheduler.config.\\\nk8s.io/v1alpha1\nkind: KubeSchedulerConfiguration\nalgorithmSource:\n  provider: DefaultProvider\n...\npercentageOfNodesToScore: 50\n</code></pre>"},{"location":"scheduling-and-eviction/scheduler-performance-tuning/#internal-details","title":"Internal Details","text":"<p>The scheduler iterates over the nodes in a round-robin fashion and also considers nodes from different zones to ensure fairness.</p>"},{"location":"scheduling-and-eviction/scheduling-framework/","title":"Scheduling Framework","text":""},{"location":"scheduling-and-eviction/scheduling-framework/#framework-workflow","title":"Framework Workflow","text":"<p>Scheduling Cycle &amp; Binding Cycle: The scheduling cycle selects a node for the Pod, and the binding cycle applies that decision to the cluster. These cycles can run serially or concurrently.</p>"},{"location":"scheduling-and-eviction/scheduling-framework/#extension-points","title":"Extension Points","text":"<p>The framework offers various extension points where scheduler plugins can register. These include: - <code>PreEnqueue</code>: Called before adding Pods to the internal active queue. - <code>QueueSort</code>: Used to sort Pods in the scheduling queue. - <code>PreFilter</code>: Pre-processes info about the Pod or checks conditions. - <code>Filter</code>: Filters out nodes that cannot run the Pod. - <code>PostFilter</code>: Called when no feasible nodes were found. - <code>PreScore</code>: Performs \"pre-scoring\" work. - <code>Score</code>: Ranks nodes that have passed the filtering phase. - <code>NormalizeScore</code>: Modifies scores before final ranking. - <code>Reserve</code>: Notifies plugins when resources are being reserved and unreserved. - <code>Permit</code>: Prevents or delays the binding to the candidate node. - <code>PreBind</code>: Performs work required before a Pod is bound. - <code>Bind</code>: Binds a Pod to a Node. - <code>PostBind</code>: Called after a Pod is successfully bound.</p>"},{"location":"scheduling-and-eviction/scheduling-framework/#plugin-api","title":"Plugin API","text":"<ul> <li>Plugins must first register and get configured.</li> <li>They then use the extension point interfaces, which have a specific form.</li> </ul>"},{"location":"scheduling-and-eviction/scheduling-framework/#plugin-configuration","title":"Plugin Configuration","text":"<ul> <li>You can enable or disable plugins in the scheduler configuration.</li> <li>Most scheduling plugins are enabled by default in Kubernetes v1.18 or later.</li> <li>You can also implement your own scheduling plugins.</li> </ul>"},{"location":"scheduling-and-eviction/taints-and-tolerations/","title":"Taints and Tolerations","text":""},{"location":"scheduling-and-eviction/taints-and-tolerations/#concepts","title":"Concepts","text":"<ul> <li> <p>Taints: These are applied to nodes to mark them as unsuitable for certain pods. For example, <code>kubectl taint nodes node1 key1=value1:NoSchedule</code> adds a taint to <code>node1</code>.</p> </li> <li> <p>Tolerations: These are applied to pods and allow them to be scheduled on nodes with matching taints.</p> </li> </ul> <p>For example, a toleration could look like this:</p> <pre><code>tolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoSchedule\"\n</code></pre>"},{"location":"scheduling-and-eviction/taints-and-tolerations/#effects","title":"Effects","text":"<ul> <li> <p><code>NoExecute</code>: Affects already running pods. Pods that do not tolerate the taint are evicted immediately.</p> </li> <li> <p><code>NoSchedule</code>: No new pods will be scheduled on the tainted node unless they have a matching toleration.</p> </li> <li> <p><code>PreferNoSchedule</code>: A softer version of <code>NoSchedule</code>. Kubernetes will try to avoid placing a pod that does not tolerate the taint on the node, but it's not guaranteed.</p> </li> </ul>"},{"location":"scheduling-and-eviction/taints-and-tolerations/#use-cases","title":"Use Cases","text":"<ul> <li> <p>Dedicated Nodes: For exclusive use by a particular set of users.</p> </li> <li> <p>Nodes with Special Hardware: For example, nodes with GPUs.</p> </li> <li> <p>Taint-based Evictions: Automatically taints a node when certain conditions are true, like <code>node.kubernetes.io/not-ready</code>.</p> </li> </ul>"},{"location":"scheduling-and-eviction/taints-and-tolerations/#example","title":"Example","text":"<p>Here's an example of a pod that uses tolerations:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  tolerations:\n  - key: \"example-key\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n</code></pre>"},{"location":"security/cloud-native-security/","title":"Cloud Native Security","text":"<p>A layered security model known as the \"4C's of Cloud Native Security,\" which consists of Cloud, Clusters, Containers, and Code.  </p>"},{"location":"security/cloud-native-security/#cloud","title":"Cloud","text":"<p>The cloud layer serves as the trusted computing base for a Kubernetes cluster. If this layer is vulnerable, the components built on top of it are also at risk. The document provides links to security documentation for popular cloud providers like AWS, Google Cloud, and Azure. It also offers suggestions for securing your infrastructure, such as limiting network access to the API Server and nodes, and encrypting etcd storage.</p>"},{"location":"security/cloud-native-security/#clusters","title":"Clusters","text":"<p>This section focuses on securing both the cluster components and the applications running within the cluster. It emphasizes the importance of Role-Based Access Control (RBAC) for API access, and recommends encrypting application secrets in etcd. It also discusses Pod Security Standards and Network Policies for additional layers of security.</p>"},{"location":"security/cloud-native-security/#containers","title":"Containers","text":"<p>While container security is considered outside the scope of this guide, it does offer general recommendations. These include scanning containers for vulnerabilities, signing container images, and using container runtimes that offer strong isolation.</p>"},{"location":"security/cloud-native-security/#code","title":"Code","text":"<p>The code layer is where you have the most control over security. Recommendations here include using TLS for all TCP communications, limiting exposed port ranges, and regularly scanning third-party libraries for vulnerabilities. It also suggests using static code analysis tools to identify unsafe coding practices.</p>"},{"location":"security/controlling-access-to-kubernetes-api/","title":"Controlling Access to K8s API","text":""},{"location":"security/controlling-access-to-kubernetes-api/#transport-security","title":"Transport Security","text":"<ul> <li>The Kubernetes API server listens on port 6443 by default, protected by TLS.</li> <li>In production, it usually serves on port 443.</li> <li>The API server presents a certificate, which can be signed by a private or public CA.</li> <li>If using a private CA, the client needs to have a copy of the CA certificate.</li> </ul>"},{"location":"security/controlling-access-to-kubernetes-api/#authentication","title":"Authentication","text":"<ul> <li>Once TLS is established, the HTTP request moves to the Authentication step.</li> <li>The API server can run one or more Authenticator modules.</li> <li>Authenticators can include client certificates, passwords, plain tokens, bootstrap tokens, and JSON Web Tokens (for service accounts).</li> <li>If authentication fails, the request is rejected with HTTP status code 401.</li> </ul>"},{"location":"security/controlling-access-to-kubernetes-api/#authorization","title":"Authorization","text":"<ul> <li>After authentication, the request must be authorized.</li> <li>Authorization is based on policies that specify what actions a user can perform.</li> <li>For example, a user \"Bob\" might have read-only access to \"pods\" in the \"projectCaribou\" namespace.</li> <li>Multiple authorization modules like ABAC, RBAC, and Webhook can be used.</li> </ul>"},{"location":"security/controlling-access-to-kubernetes-api/#admission-control","title":"Admission Control","text":"<ul> <li>Admission Control modules can modify or reject requests.</li> <li>They act on requests that create, modify, delete, or connect to an object but not on read-only requests.</li> <li>If any admission controller rejects the request, it is immediately denied.</li> <li>Admission controllers can also set complex defaults for fields.</li> </ul>"},{"location":"security/controlling-access-to-kubernetes-api/#auditing","title":"Auditing","text":"<ul> <li>Kubernetes auditing provides a set of records documenting actions in a cluster.</li> <li>It audits activities generated by users, applications using the API, and the control plane.</li> </ul>"},{"location":"security/hardening-guide-auth/","title":"Hardening Guide","text":""},{"location":"security/hardening-guide-auth/#x509-client-certificate-authentication","title":"X.509 Client Certificate Authentication","text":"<ul> <li>Used for system components like Kubelet to API Server.</li> <li>Not suitable for user authentication due to non-revocable certificates and other limitations.</li> </ul>"},{"location":"security/hardening-guide-auth/#static-token-file","title":"Static Token File","text":"<ul> <li>Credentials are stored in clear text on control plane node disks.</li> <li>Not recommended for production due to security risks and lack of lockout mechanisms.</li> </ul>"},{"location":"security/hardening-guide-auth/#bootstrap-tokens","title":"Bootstrap Tokens","text":"<ul> <li>Used for joining nodes to clusters.</li> <li>Not suitable for user authentication due to hard-coded group memberships and lack of lockout mechanisms.</li> </ul>"},{"location":"security/hardening-guide-auth/#serviceaccount-secret-tokens","title":"ServiceAccount Secret Tokens","text":"<ul> <li>Used for workloads in the cluster to authenticate to the API server.</li> <li>Being replaced with TokenRequest API tokens. Unsuitable for user authentication for various reasons, including lack of expiry.</li> </ul>"},{"location":"security/hardening-guide-auth/#tokenrequest-api-tokens","title":"TokenRequest API Tokens","text":"<ul> <li>Useful for short-lived service authentication.</li> <li>Not recommended for user authentication due to lack of revocation methods.</li> </ul>"},{"location":"security/hardening-guide-auth/#openid-connect-oidc-token-authentication","title":"OpenID Connect (OIDC) Token Authentication","text":"<ul> <li>Allows integration with external identity providers.</li> <li>Requires careful setup and short token lifespan for security.</li> </ul>"},{"location":"security/hardening-guide-auth/#webhook-token-authentication","title":"Webhook Token Authentication","text":"<ul> <li>Allows an external service to make authentication decisions via a webhook.</li> <li>Suitability depends on the software used for the authentication service.</li> </ul>"},{"location":"security/hardening-guide-auth/#authenticating-proxy","title":"Authenticating Proxy","text":"<ul> <li>Uses a proxy to set specific header values for username and group memberships.</li> <li>Requires securely configured TLS and proper header security.</li> </ul>"},{"location":"security/kubernetes-api-server-bypass-risks/","title":"API Server Bypass","text":""},{"location":"security/kubernetes-api-server-bypass-risks/#static-pods","title":"Static Pods","text":"<p>Static Pods are managed directly by the kubelet on each node and are not controlled by the API server. An attacker with write access to the manifest directory could modify or introduce new static Pods.</p>"},{"location":"security/kubernetes-api-server-bypass-risks/#mitigations","title":"Mitigations","text":"<ul> <li>Enable kubelet static Pod manifest functionality only if required.</li> <li>Restrict filesystem access to the static Pod manifest directory.</li> <li>Regularly audit and report all access to directories hosting static Pod manifests.</li> </ul>"},{"location":"security/kubernetes-api-server-bypass-risks/#the-kubelet-api","title":"The kubelet API","text":"<p>The kubelet API is exposed on TCP port 10250 and allows for information disclosure and command execution in containers.</p>"},{"location":"security/kubernetes-api-server-bypass-risks/#mitigations_1","title":"Mitigations","text":"<ul> <li>Restrict access to sub-resources of the nodes API object using mechanisms like RBAC.</li> <li>Restrict access to the kubelet port to trusted IP ranges.</li> <li>Ensure kubelet authentication is set to webhook or certificate mode.</li> </ul>"},{"location":"security/kubernetes-api-server-bypass-risks/#the-etcd-api","title":"The etcd API","text":"<p>Kubernetes uses etcd as a datastore, and direct access to this API can lead to data disclosure or modification.</p>"},{"location":"security/kubernetes-api-server-bypass-risks/#mitigations_2","title":"Mitigations","text":"<ul> <li>Control access to the private key for the etcd server certificate.</li> <li>Restrict access to the etcd port at a network level.</li> </ul>"},{"location":"security/kubernetes-api-server-bypass-risks/#container-runtime-socket","title":"Container runtime socket","text":"<p>The container runtime exposes a Unix socket for interaction with containers. An attacker with access to this socket can launch or interact with containers.</p>"},{"location":"security/kubernetes-api-server-bypass-risks/#mitigations_3","title":"Mitigations","text":"<ul> <li>Control filesystem access to container runtime sockets.</li> <li>Isolate the kubelet from other components using mechanisms like Linux kernel namespaces.</li> <li>Restrict or forbid the use of hostPath mounts that include the container runtime socket.</li> </ul>"},{"location":"security/kubernetes-secrets-best-practices/","title":"Secrets Best Practices","text":""},{"location":"security/kubernetes-secrets-best-practices/#principles-and-practices","title":"Principles and Practices","text":"<p>In Kubernetes, a Secret is an object that stores sensitive information like passwords, OAuth tokens, and SSH keys. Secrets offer better control over sensitive information and reduce the risk of accidental exposure. By default, Secret values are base64 encoded and stored unencrypted, but you can configure them to be encrypted at rest. Pods can reference Secrets in various ways, such as in a volume mount or as an environment variable.</p>"},{"location":"security/kubernetes-secrets-best-practices/#configure-encryption-at-rest","title":"Configure Encryption at Rest","text":"<p>Secrets are stored unencrypted in etcd by default. It's recommended to configure encryption for Secret data in etcd.</p>"},{"location":"security/kubernetes-secrets-best-practices/#configure-least-privilege-access-to-secrets","title":"Configure Least-Privilege Access to Secrets","text":"<p>When planning your access control mechanism, like Kubernetes Role-based Access Control (RBAC), restrict access to Secret objects. Limit watch or list access to only the most privileged, system-level components. Only grant 'get' access if the component's behavior requires it.</p>"},{"location":"security/kubernetes-secrets-best-practices/#additional-recommendations","title":"Additional Recommendations","text":"<ul> <li>Use short-lived Secrets.</li> <li>Implement audit rules that alert on specific events, like concurrent reading of multiple Secrets by a single user.</li> <li>Improve etcd management policies, such as wiping or shredding the durable storage used by etcd once it's no longer in use.</li> <li>If there are multiple etcd instances, configure encrypted SSL/TLS communication between them.</li> </ul>"},{"location":"security/kubernetes-secrets-best-practices/#configure-access-to-external-secrets","title":"Configure Access to External Secrets","text":"<p>You can use third-party Secrets store providers to keep your confidential data outside your cluster. The Kubernetes Secrets Store CSI Driver is a DaemonSet that allows the kubelet to retrieve Secrets from external stores and mount them into specific Pods. ## For Developers</p>"},{"location":"security/kubernetes-secrets-best-practices/#restrict-secret-access-to-specific-containers","title":"Restrict Secret Access to Specific Containers","text":"<p>If a Pod has multiple containers and only one needs access to a Secret, configure the volume mount or environment variable so that the other containers don't have access.</p>"},{"location":"security/kubernetes-secrets-best-practices/#protect-secret-data-after-reading","title":"Protect Secret Data After Reading","text":"<p>After reading the Secret from an environment variable or volume, your application should still protect the value. For example, avoid logging the Secret or transmitting it to an untrusted party.</p>"},{"location":"security/kubernetes-secrets-best-practices/#avoid-sharing-secret-manifests","title":"Avoid Sharing Secret Manifests","text":"<p>If you configure a Secret through a manifest with the Secret data encoded as base64, avoid sharing this file or checking it into a source repository.</p>"},{"location":"security/multi-tenancy/","title":"Multi-Tenancy","text":""},{"location":"security/multi-tenancy/#introduction","title":"Introduction","text":"<p>Multi-tenancy in Kubernetes refers to the ability to divide a single Kubernetes cluster into isolated partitions, allowing multiple teams or customers to share the cluster's resources without interfering with each other.</p>"},{"location":"security/multi-tenancy/#challenges-in-multi-tenancy","title":"Challenges in Multi-tenancy","text":"<p>The main challenges include ensuring resource isolation, data security, and equitable resource allocation among tenants. The document outlines that these challenges require both architectural and policy-based solutions.</p>"},{"location":"security/multi-tenancy/#namespace-tenancy","title":"Namespace Tenancy","text":"<p>Namespaces act as a primary isolation mechanism. Each tenant can be allocated a namespace, within which they can create and manage resources. This provides a level of isolation and helps in resource tracking.</p>"},{"location":"security/multi-tenancy/#hierarchical-namespaces","title":"Hierarchical Namespaces","text":"<p>This is an advanced feature that allows namespaces to be organized in a hierarchical fashion, inheriting policies and roles from parent namespaces. This is useful for large organizations where multiple departments or teams share a cluster but have different sub-teams requiring varying levels of access.</p>"},{"location":"security/multi-tenancy/#policy-based-resource-isolation","title":"Policy-based Resource Isolation","text":"<p>Policies like Role-Based Access Control (RBAC), Network Policies, and PodSecurityPolicies can be applied at the namespace level to provide additional layers of security and isolation. For example, Network Policies can restrict communication between pods in different namespaces.</p>"},{"location":"security/multi-tenancy/#control-plane-isolation","title":"Control Plane Isolation","text":"<p>The control plane is the set of components that manage the overall state of the cluster. Isolating the control plane ensures that tenants cannot interfere with these critical components, thereby maintaining cluster stability.</p>"},{"location":"security/multi-tenancy/#soft-multi-tenancy-vs-hard-multi-tenancy","title":"Soft Multi-tenancy vs Hard Multi-tenancy","text":"<ul> <li>Soft Multi-tenancy: Assumes that all users are trusted but still provides certain levels of isolation and security. Suitable for scenarios where all users belong to the same organization.</li> <li>Hard Multi-tenancy: Assumes that not all users are trusted and provides strict isolation measures. Suitable for public or shared clusters where users are from different organizations.</li> </ul>"},{"location":"security/pod-security-admission/","title":"Pod Security Admission","text":""},{"location":"security/pod-security-admission/#pod-security-levels","title":"Pod Security Levels","text":"<p>The Pod Security Admission Controller places requirements on a Pod's Security Context and other related fields according to three levels defined by the Pod Security Standards: - Privileged - Baseline - Restricted</p>"},{"location":"security/pod-security-admission/#labels-for-namespaces","title":"Labels for Namespaces","text":"<p>Once the feature is enabled or the webhook is installed, you can configure namespaces with labels to define the admission control mode for pod security. The modes are: - Enforce: Policy violations will cause the pod to be rejected. - Audit: Policy violations will trigger an audit annotation but are otherwise allowed. - Warn: Policy violations will trigger a user-facing warning but are otherwise allowed.</p>"},{"location":"security/pod-security-admission/#workload-resources-and-pod-templates","title":"Workload Resources and Pod Templates","text":"<p>Pods are often created indirectly via workload objects like Deployments or Jobs. Both the audit and warning modes are applied to these workload resources, but the enforce mode is only applied to the resulting pod objects.</p>"},{"location":"security/pod-security-admission/#exemptions","title":"Exemptions","text":"<p>You can define exemptions to bypass pod security enforcement. These exemptions can be based on: - Usernames - RuntimeClassNames - Namespaces  - Certain pod field updates are also exempt from policy checks.</p>"},{"location":"security/pod-security-standards/","title":"Pod Security Standards","text":""},{"location":"security/pod-security-standards/#privileged-policy","title":"Privileged Policy","text":"<p>Purpose: For system and infrastructure-level workloads managed by privileged, trusted users. Characteristics: No restrictions, allows for known privilege escalations.</p>"},{"location":"security/pod-security-standards/#baseline-policy","title":"Baseline Policy","text":"<p>Purpose: For common containerized workloads, prevents known privilege escalations. Characteristics: - Disallows privileged access to the Windows node. - Sharing host namespaces is not allowed. - Privileged Pods are disallowed. - Adding additional capabilities beyond a specified list is disallowed. - HostPath volumes are forbidden. - HostPorts are disallowed or restricted to a known list. - Overrides to the default AppArmor profile are restricted. - Setting custom SELinux user or role options is forbidden. - The default /proc masks are required. - Seccomp profile must not be set to Unconfined. - Only a \"safe\" subset of sysctls is allowed.</p>"},{"location":"security/pod-security-standards/#restricted-policy","title":"Restricted Policy","text":"<p>Purpose: For security-critical applications and lower-trust users. Characteristics: - Enforces everything from the baseline profile. - Only permits specific volume types. - Privilege escalation is not allowed. - Containers must run as non-root users. - Seccomp profile must be explicitly set to one of the allowed values. - Containers must drop ALL capabilities except <code>NET_BIND_SERVICE</code>.</p>"},{"location":"security/rbac-best-practices/","title":"RBAC Best Practices","text":""},{"location":"security/rbac-best-practices/#general-good-practice","title":"General Good Practice","text":"<p>Least Privilege: Assign minimal RBAC rights to users and service accounts. Use RoleBindings instead of ClusterRoleBindings, avoid wildcard permissions, and don't use cluster-admin accounts for daily tasks.</p>"},{"location":"security/rbac-best-practices/#minimize-distribution-of-privileged-tokens","title":"Minimize Distribution of Privileged Tokens","text":"<p>Limit the number of nodes running powerful pods and avoid running them alongside untrusted or publicly-exposed ones. Use Taints, NodeAffinity, or PodAntiAffinity to ensure separation.</p>"},{"location":"security/rbac-best-practices/#hardening","title":"Hardening","text":"<p>Review default RBAC rights and make changes to harden security. For instance, review bindings for the <code>system:unauthenticated</code> group and set <code>automountServiceAccountToken: false</code> to avoid auto-mounting of service account tokens.</p>"},{"location":"security/rbac-best-practices/#periodic-review","title":"Periodic Review","text":"<p>Regularly review RBAC settings to remove redundant entries and check for privilege escalations.</p>"},{"location":"security/rbac-best-practices/#privilege-escalation-risks","title":"Privilege Escalation Risks","text":"<p>Be cautious with privileges like listing secrets, workload creation, and persistent volume creation as they can lead to privilege escalation.</p>"},{"location":"security/rbac-best-practices/#denial-of-service-risks","title":"Denial of Service Risks","text":"<p>Users with object creation rights can potentially create a denial-of-service condition. Resource quotas can be used to mitigate this issue.</p>"},{"location":"security/security-checklist/","title":"Security Checklist","text":""},{"location":"security/security-checklist/#system-authentication-and-authorization","title":"System Authentication and Authorization","text":"<p>Recommendations include not using the <code>system:masters</code> group for user or component authentication after bootstrapping and following Role-Based Access Control (RBAC) good practices.</p>"},{"location":"security/security-checklist/#network-security","title":"Network Security","text":"<p>Suggestions include using CNI plugins that support network policies, applying ingress and egress network policies to all workloads, and not exposing the Kubernetes API, kubelet API, and etcd publicly on the Internet.</p>"},{"location":"security/security-checklist/#pod-security","title":"Pod Security","text":"<p>The document advises setting RBAC rights only when necessary, applying appropriate Pod Security Standards policies, and setting memory and CPU limits for workloads.</p>"},{"location":"security/security-checklist/#logs-and-auditing","title":"Logs and Auditing","text":"<p>It recommends protecting audit logs from general access and disabling the <code>/logs</code> API.</p>"},{"location":"security/security-checklist/#pod-placement","title":"Pod Placement","text":"<p>Suggestions include isolating sensitive applications on specific nodes or using sandboxed runtimes.</p>"},{"location":"security/security-checklist/#secrets","title":"Secrets","text":"<p>The checklist advises against using ConfigMaps for confidential data and recommends using encryption at rest for the Secret API.</p>"},{"location":"security/security-checklist/#images","title":"Images","text":"<p>Recommendations include minimizing unnecessary content in container images, running images as an unprivileged user, and regularly scanning images for vulnerabilities.</p>"},{"location":"security/security-checklist/#admission-controllers","title":"Admission Controllers","text":"<p>The document suggests enabling an appropriate selection of admission controllers and securely configuring the admission chain plugins and webhooks.</p>"},{"location":"security/service-accounts/","title":"Service Accounts","text":""},{"location":"security/service-accounts/#what-are-service-accounts","title":"What are Service Accounts?","text":"<p>A service account is a non-human account in Kubernetes that provides a distinct identity within a cluster. These accounts are useful for various purposes, such as authenticating to the API server or implementing identity-based security policies. Service accounts have specific properties: - Namespaced: Bound to a Kubernetes namespace. - Lightweight: Defined in the Kubernetes API. - Portable: Can be easily included in configuration bundles for containerized workloads.</p>"},{"location":"security/service-accounts/#default-service-accounts","title":"Default Service Accounts","text":"<p>When you create a cluster, Kubernetes automatically creates a default ServiceAccount for every namespace. These default accounts have limited permissions, primarily for API discovery. If you delete the default ServiceAccount, the control plane replaces it.</p>"},{"location":"security/service-accounts/#use-cases","title":"Use Cases","text":"<p>Service accounts can be used in scenarios like: - Pods needing to communicate with the Kubernetes API server. - Pods requiring an identity for an external service. - External services needing to communicate with the Kubernetes API server. - Third-party security software relying on ServiceAccount identities.</p>"},{"location":"security/service-accounts/#how-to-use-service-accounts","title":"How to Use Service Accounts","text":"<ul> <li>Create a ServiceAccount: Use <code>kubectl</code> or a manifest to define the object.</li> <li>Grant Permissions: Use mechanisms like RBAC to grant the necessary permissions.</li> <li>Assign to Pods: During Pod creation, assign the ServiceAccount.</li> </ul>"},{"location":"security/service-accounts/#granting-permissions","title":"Granting Permissions","text":"<p>You can use Kubernetes RBAC to grant minimal permissions to each service account, adhering to the principle of least privilege.</p>"},{"location":"security/service-accounts/#cross-namespace-access","title":"Cross-Namespace Access","text":"<p>RBAC can also be used to allow service accounts in one namespace to perform actions in another namespace.</p>"},{"location":"security/service-accounts/#assigning-a-serviceaccount-to-a-pod","title":"Assigning a ServiceAccount to a Pod","text":"<p>To assign a ServiceAccount to a Pod, set the <code>spec.serviceAccountName</code> field in the Pod specification. Kubernetes will automatically provide the credentials for that ServiceAccount to the Pod.</p>"},{"location":"security/service-accounts/#authenticating-service-account-credentials","title":"Authenticating Service Account Credentials","text":"<p>ServiceAccounts use signed JSON Web Tokens (JWTs) to authenticate. The API server validates these tokens based on various parameters like signature, expiry, and audience.</p>"},{"location":"security/service-accounts/#authenticating-in-your-own-code","title":"Authenticating in Your Own Code","text":"<p>If you have services that need to validate Kubernetes service account credentials, you can use methods like the TokenReview API or OIDC discovery.</p>"},{"location":"security/service-accounts/#alternatives","title":"Alternatives","text":"<ul> <li>Issue your own tokens and use Webhook Token Authentication for validation.</li> <li>Use service meshes like Istio for providing certificates to Pods.</li> </ul>"},{"location":"services-and-networking/dns/","title":"DNS","text":""},{"location":"services-and-networking/dns/#terminology","title":"Terminology","text":"<ul> <li>DNS: Domain Name System</li> <li>FQDN: Fully Qualified Domain Name</li> <li>SRV Records: Service records in DNS</li> </ul>"},{"location":"services-and-networking/dns/#what-is-dns-for-services-and-pods","title":"What is DNS for Services and Pods?","text":"<ul> <li>Kubernetes creates DNS records for Services and Pods.</li> <li>Allows for name-based service discovery within the cluster.</li> </ul>"},{"location":"services-and-networking/dns/#dns-records","title":"DNS Records","text":"<ul> <li>Services and Pods get DNS records.</li> <li>\"Normal\" Services get A/AAAA records.</li> <li>Headless Services also get A/AAAA records but resolve to the set of IPs of all Pods selected by the Service.</li> <li>SRV Records are created for named ports.</li> </ul>"},{"location":"services-and-networking/dns/#pods","title":"Pods","text":"<ul> <li>Pods have A/AAAA records.</li> <li>The DNS resolution is <code>pod-ip-address.my-namespace.pod.cluster-domain.example</code>.</li> <li>Pods exposed by a Service have additional DNS resolution.</li> </ul>"},{"location":"services-and-networking/dns/#pods-hostname-and-subdomain-fields","title":"Pod's hostname and subdomain fields","text":"<ul> <li>The hostname is by default the Pod's <code>metadata.name</code>.</li> <li>The hostname can be overridden by <code>spec.hostname</code>.</li> <li>The fully qualified domain name (FQDN) can be set using <code>spec.subdomain</code>.</li> </ul>"},{"location":"services-and-networking/dns/#pods-dns-policy","title":"Pod's DNS Policy","text":"<ul> <li>DNS policies can be set per-Pod.</li> <li>Options include <code>Default</code>, <code>ClusterFirst</code>, <code>ClusterFirstWithHostNet</code>, and <code>None</code>.</li> </ul>"},{"location":"services-and-networking/dns/#pods-dns-config","title":"Pod's DNS Config","text":"<ul> <li>Allows more control over DNS settings for a Pod.</li> <li>Can specify nameservers, searches, and options.</li> </ul>"},{"location":"services-and-networking/dns/#dns-search-domain-list-limits","title":"DNS search domain list limits","text":"<ul> <li>Kubernetes does not limit the DNS Config until the length of the search domain list exceeds 32 or the total length of all search domains exceeds 2048.</li> </ul>"},{"location":"services-and-networking/dns/#dns-resolution-on-windows-nodes","title":"DNS resolution on Windows nodes","text":"<ul> <li><code>ClusterFirstWithHostNet</code> is not supported on Windows nodes.</li> <li>Windows treats all names with a <code>.</code> as a FQDN and skips FQDN resolution.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/","title":"Endpoint Slices","text":""},{"location":"services-and-networking/endpoint-slices/#endpointslice-api","title":"EndpointSlice API","text":"<ul> <li>Provides a scalable and extensible way to track network endpoints in a Kubernetes cluster.</li> <li>Automatically created for any Kubernetes Service with a selector.</li> <li>Groups network endpoints by protocol, port number, and Service name.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#address-types","title":"Address Types","text":"<ul> <li>Supports IPv4, IPv6, and FQDN (Fully Qualified Domain Name).</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#conditions","title":"Conditions","text":"<ul> <li>Three conditions: <code>ready</code>, <code>serving</code>, and <code>terminating</code>:</li> <li><code>ready</code> maps to a Pod's Ready condition.</li> <li><code>serving</code> is for pod readiness during termination.</li> <li><code>terminating</code> indicates whether an endpoint is terminating.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#topology-information","title":"Topology Information","text":"<ul> <li>Includes the location of the endpoint, Node name, and zone.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#management","title":"Management","text":"<ul> <li>Mostly managed by the control plane's endpoint slice controller.</li> <li>Label <code>endpointslice.kubernetes.io/managed-by</code> indicates the entity managing an EndpointSlice.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#ownership","title":"Ownership","text":"<ul> <li>Owned by the Service they track endpoints for.</li> <li>Ownership indicated by an owner reference and a <code>kubernetes.io/service-name</code> label.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#endpointslice-mirroring","title":"EndpointSlice Mirroring","text":"<ul> <li>Mirrors custom Endpoints resources to EndpointSlices unless certain conditions are met.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#distribution-of-endpointslices","title":"Distribution of EndpointSlices","text":"<ul> <li>Tries to fill EndpointSlices as full as possible but does not actively rebalance them.</li> <li>Prioritizes limiting EndpointSlice updates over a perfectly full distribution.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#duplicate-endpoints","title":"Duplicate Endpoints","text":"<ul> <li>Endpoints may be represented in more than one EndpointSlice due to the nature of EndpointSlice changes.</li> </ul>"},{"location":"services-and-networking/endpoint-slices/#comparison-with-endpoints","title":"Comparison with Endpoints","text":"<ul> <li>EndpointSlices offer better scalability and extensibility compared to the original Endpoints API.</li> </ul>"},{"location":"services-and-networking/ingress/","title":"Ingress","text":""},{"location":"services-and-networking/ingress/#terminology","title":"Terminology","text":"<ul> <li>Edge Router: A router that enforces the firewall policy for your cluster.</li> <li>Cluster Network: A set of links, logical or physical, that facilitate communication within a cluster.</li> </ul>"},{"location":"services-and-networking/ingress/#what-is-ingress","title":"What is Ingress?","text":"<ul> <li>Manages external access to services within a cluster.</li> <li>Typically provides HTTP and HTTPS routes.</li> <li>Can provide load balancing, SSL termination, and name-based virtual hosting.</li> </ul>"},{"location":"services-and-networking/ingress/#prerequisites","title":"Prerequisites","text":"<ul> <li>Must have an Ingress controller to satisfy an Ingress.</li> <li>Basic workflow: Create an Ingress object -&gt; Ingress controller configures the load balancer.</li> </ul>"},{"location":"services-and-networking/ingress/#the-ingress-resource","title":"The Ingress Resource","text":"<ul> <li>Mainly composed of a set of rules based on hostnames and paths.</li> <li>API object that manages external access to services.</li> </ul>"},{"location":"services-and-networking/ingress/#ingress-rules","title":"Ingress Rules","text":"<ul> <li>Define how to route traffic by hostnames and paths.</li> <li>Each rule has one or more HTTP paths, each forwarding to a defined backend.</li> </ul>"},{"location":"services-and-networking/ingress/#defaultbackend","title":"DefaultBackend","text":"<ul> <li>An addressable Kubernetes Service to handle all requests not matching any path in the Ingress rules.</li> <li>Serves as a catch-all for undefined routes.</li> </ul>"},{"location":"services-and-networking/ingress/#resource-backends","title":"Resource Backends","text":"<ul> <li>A feature to forward traffic to resources other than Kubernetes Services.</li> <li>Can be used to route traffic to a custom resource.</li> </ul>"},{"location":"services-and-networking/ingress/#path-types","title":"Path Types","text":"<ul> <li>Defines how to match requests based on their paths.</li> <li>Types: <code>Exact</code>, <code>Prefix</code>, and <code>ImplementationSpecific</code>.</li> </ul>"},{"location":"services-and-networking/ingress/#hostname-wildcards","title":"Hostname Wildcards","text":"<ul> <li>Allows for the routing of HTTP traffic based on wildcards in hostnames.</li> <li>E.g., <code>.foo.com</code> routes to a specific service.</li> </ul>"},{"location":"services-and-networking/ingress/#ingress-class","title":"Ingress Class","text":"<ul> <li>Allows you to configure multiple Ingress controllers.</li> <li>Each controller is identified by a unique class.</li> </ul>"},{"location":"services-and-networking/ingress/#ingressclass-scope","title":"IngressClass Scope","text":"<ul> <li>Defines the scope of a particular Ingress class.</li> <li>Can be either cluster-wide or namespaced.</li> </ul>"},{"location":"services-and-networking/ingress/#deprecated-annotation","title":"Deprecated Annotation","text":"<ul> <li>Annotations for specifying ingress class are deprecated.</li> <li>Replaced by the ingressClassName field in the Ingress spec.</li> </ul>"},{"location":"services-and-networking/ingress/#default-ingressclass","title":"Default IngressClass","text":"<ul> <li>Specifies the ingress class to use when none is defined.</li> <li>Configured through a cluster-wide setting.</li> </ul>"},{"location":"services-and-networking/ingress/#types-of-ingress","title":"Types of Ingress","text":"<ul> <li>Single Service Ingress: Simplest kind, routes everything to one Service.</li> <li>Simple fanout: Routes traffic from a single IP address to more than one Service.</li> <li>Name-based virtual hosting: Routes traffic on multiple hostnames to different services.</li> </ul>"},{"location":"services-and-networking/ingress/#ingress-controllers","title":"Ingress Controllers","text":""},{"location":"services-and-networking/ingress/#introduction","title":"Introduction","text":"<ul> <li>Ingress controllers are essential for the functioning of an Ingress resource in a Kubernetes cluster.</li> <li>Unlike other controllers, Ingress controllers are not started automatically and must be set up manually.</li> </ul>"},{"location":"services-and-networking/ingress/#supported-controllers","title":"Supported Controllers","text":"<ul> <li>Kubernetes officially supports and maintains AWS, GCE, and nginx ingress controllers.</li> </ul>"},{"location":"services-and-networking/ingress/#additional-controllers","title":"Additional Controllers","text":"<ul> <li>Various third-party ingress controllers like AKS Application Gateway, Apache APISIX, Avi Kubernetes Operator, and many others are available.</li> </ul>"},{"location":"services-and-networking/ingress/#using-multiple-ingress-controllers","title":"Using Multiple Ingress Controllers","text":"<ul> <li>You can deploy multiple ingress controllers in a cluster using ingress class.</li> <li>The <code>.metadata.name</code> of the ingress class resource is important when creating an Ingress object.</li> </ul>"},{"location":"services-and-networking/ingress/#default-ingressclass_1","title":"Default IngressClass","text":"<ul> <li>If an Ingress object doesn't specify an IngressClass and there's exactly one IngressClass marked as default, Kubernetes applies the default IngressClass.</li> <li>An IngressClass is marked as default by setting the i<code>ngressclass.kubernetes.io/is-default-class</code> annotation to <code>true</code>.</li> </ul>"},{"location":"services-and-networking/ingress/#controller-specifications","title":"Controller Specifications","text":"<ul> <li>While all ingress controllers should ideally fulfill the Kubernetes specification, they may operate differently.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/","title":"IPv4 - IPv6","text":""},{"location":"services-and-networking/ipv4-ipv6/#ipv4ipv6-dual-stack-networking","title":"IPv4/IPv6 dual-stack networking","text":"<ul> <li>Enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.</li> <li>Enabled by default for Kubernetes clusters starting in version 1.21.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#supported-features","title":"Supported Features","text":"<ul> <li>Dual-stack Pod networking</li> <li>IPv4 and IPv6 enabled Services</li> <li>Pod off-cluster egress routing via both IPv4 and IPv6 interfaces</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.20 or later</li> <li>Provider support for dual-stack networking</li> <li>A network plugin that supports dual-stack networking</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#configure-ipv4ipv6-dual-stack","title":"Configure IPv4/IPv6 dual-stack","text":"<ul> <li>Various flags for kube-apiserver, kube-controller-manager, kube-proxy, and kubelet to set dual-stack cluster network assignments.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#services","title":"Services","text":"<ul> <li>Can use IPv4, IPv6, or both.</li> <li><code>.spec.ipFamilyPolicy</code> can be set to <code>SingleStack</code>, <code>PreferDualStack</code>, or <code>RequireDualStack</code>.</li> <li><code>.spec.ipFamilies</code> can be set to define the order of IP families for dual-stack.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#dual-stack-service-configuration-scenarios","title":"Dual-stack Service configuration scenarios","text":"<ul> <li>Examples provided for various dual-stack Service configurations.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#dual-stack-defaults-on-existing-services","title":"Dual-stack defaults on existing Services","text":"<ul> <li>Existing Services are configured by the control plane to set <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code>.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#service-type-loadbalancer","title":"Service type LoadBalancer","text":"<ul> <li>To provision a dual-stack load balancer, set <code>.spec.type</code> to <code>LoadBalancer</code> and <code>.spec.ipFamilyPolicy</code> to <code>PreferDualStack</code> or <code>RequireDualSactk</code>.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#egress-traffic","title":"Egress traffic","text":"<ul> <li>Enable egress traffic for Pods using non-publicly routable IPv6 addresses through mechanisms like transparent proxying or IP masquerading.</li> </ul>"},{"location":"services-and-networking/ipv4-ipv6/#windows-support","title":"Windows support","text":"<ul> <li>Dual-stack IPv4/IPv6 networking is supported for pods and nodes with single-family services on Windows.</li> </ul>"},{"location":"services-and-networking/network-overview/","title":"Overview","text":""},{"location":"services-and-networking/network-overview/#the-kubernetes-network-model","title":"The Kubernetes Network Model","text":"<ul> <li>Every Pod gets a unique cluster-wide IP address.</li> <li>Pods can communicate with all other pods on any node without NAT.</li> <li>Agents on a node can communicate with all pods on that node.</li> <li>\"IP-per-pod\" model: Containers within a Pod share their network namespaces, including their IP and MAC addresses.</li> </ul>"},{"location":"services-and-networking/network-overview/#kubernetes-networking-concerns","title":"Kubernetes Networking Concerns","text":"<ul> <li>Containers within a Pod communicate via loopback.</li> <li>Cluster networking enables communication between different Pods.</li> <li>The Service API exposes applications running in Pods to be reachable from outside the cluster.</li> <li>Ingress provides extra functionality for exposing HTTP applications, websites, and APIs.</li> </ul>"},{"location":"services-and-networking/network-overview/#service","title":"Service","text":"<ul> <li>Exposes an application running in the cluster behind a single outward-facing endpoint.</li> </ul>"},{"location":"services-and-networking/network-overview/#ingress","title":"Ingress","text":"<ul> <li>Makes HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism.</li> </ul>"},{"location":"services-and-networking/network-overview/#ingress-controllers","title":"Ingress Controllers","text":"<ul> <li>Required for an Ingress to work in the cluster.</li> </ul>"},{"location":"services-and-networking/network-overview/#endpointslices","title":"EndpointSlices","text":"<ul> <li>Allows the Service to scale to handle large numbers of backends.</li> </ul>"},{"location":"services-and-networking/network-overview/#network-policies","title":"Network Policies","text":"<ul> <li>Controls traffic flow at the IP address or port level.</li> </ul>"},{"location":"services-and-networking/network-overview/#dns-for-services-and-pods","title":"DNS for Services and Pods","text":"<ul> <li>Workloads can discover Services within the cluster using DNS.</li> </ul>"},{"location":"services-and-networking/network-overview/#ipv4ipv6-dual-stack","title":"IPv4/IPv6 dual-stack","text":"<ul> <li>Supports single-stack IPv4, single-stack IPv6, or dual-stack networking.</li> </ul>"},{"location":"services-and-networking/network-overview/#topology-aware-routing","title":"Topology Aware Routing","text":"<ul> <li>Helps keep network traffic within the zone where it originated.</li> </ul>"},{"location":"services-and-networking/network-overview/#service-internal-traffic-policy","title":"Service Internal Traffic Policy","text":"<ul> <li>Keeps network traffic within the node if two Pods in the cluster are running on the same node.</li> </ul>"},{"location":"services-and-networking/network-policies/","title":"Network Policies","text":""},{"location":"services-and-networking/network-policies/#the-two-sorts-of-pod-isolation","title":"The Two Sorts of Pod Isolation","text":"<ol> <li>Namespace isolation: Isolating whole namespaces from one another.</li> <li>Pod isolation: More granular control, isolating individual pods.</li> </ol>"},{"location":"services-and-networking/network-policies/#the-networkpolicy-resource","title":"The NetworkPolicy Resource","text":"<ul> <li>Defines how pods are allowed to communicate with each other and other network endpoints.</li> <li>PodSelector targets Pods to apply the policy.</li> <li>PolicyTypes specifies what types of traffic are affected.</li> </ul>"},{"location":"services-and-networking/network-policies/#behavior-of-to-and-from-selectors","title":"Behavior of to and from Selectors","text":"<ul> <li>ingress and egress rules can be set.</li> <li>Rules can be as specific as \"allow traffic from these IPs\" or \"allow traffic from Pods with these labels.\"</li> </ul>"},{"location":"services-and-networking/network-policies/#default-policies","title":"Default Policies","text":"<ul> <li>Policies that apply when no other policies do.</li> </ul>"},{"location":"services-and-networking/network-policies/#default-deny-all-ingress-traffic","title":"Default Deny All Ingress Traffic","text":"<ul> <li>Blocks all incoming traffic to Pods unless it matches a NetworkPolicy.</li> </ul>"},{"location":"services-and-networking/network-policies/#allow-all-ingress-traffic","title":"Allow All Ingress Traffic","text":"<ul> <li>Allows all incoming traffic to Pods.</li> </ul>"},{"location":"services-and-networking/network-policies/#default-deny-all-egress-traffic","title":"Default Deny All Egress Traffic","text":"<ul> <li>Blocks all outgoing traffic from Pods unless it matches a NetworkPolicy.</li> </ul>"},{"location":"services-and-networking/network-policies/#allow-all-egress-traffic","title":"Allow All Egress Traffic","text":"<ul> <li>Allows all outgoing traffic from Pods.</li> </ul>"},{"location":"services-and-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic","title":"Default Deny All Ingress and All Egress Traffic","text":"<ul> <li>Blocks both incoming and outgoing traffic unless they match a NetworkPolicy.</li> </ul>"},{"location":"services-and-networking/network-policies/#sctp-support","title":"SCTP Support","text":"<ul> <li>SCTP (Stream Control Transmission Protocol) is supported as a protocol alongside TCP and UDP.</li> </ul>"},{"location":"services-and-networking/network-policies/#targeting-a-range-of-ports","title":"Targeting a Range of Ports","text":"<ul> <li>NetworkPolicy can target a range of ports instead of a single port.</li> </ul>"},{"location":"services-and-networking/network-policies/#targeting-multiple-namespaces-by-label","title":"Targeting Multiple Namespaces by Label","text":"<ul> <li>NetworkPolicy can target multiple namespaces using namespace labels.</li> </ul>"},{"location":"services-and-networking/network-policies/#targeting-a-namespace-by-its-name","title":"Targeting a Namespace by its Name","text":"<ul> <li>Specific namespaces can be targeted by their name.</li> </ul>"},{"location":"services-and-networking/network-policies/#what-you-cant-do-with-network-policies-at-least-not-yet","title":"What You Can't Do with Network Policies (at least, not yet)","text":"<ul> <li>Limitations like not being able to enforce egress based on DNS names, or not being able to limit access based on the protocol's fields.</li> </ul>"},{"location":"services-and-networking/service-networking/","title":"Service Networking","text":""},{"location":"services-and-networking/service-networking/#introduction","title":"Introduction","text":"<ul> <li>Services expose applications running on a set of Pods.</li> <li>Services can have a cluster-scoped virtual IP address (ClusterIP).</li> <li>Clients connect using the virtual IP, and Kubernetes load-balances the traffic.</li> </ul>"},{"location":"services-and-networking/service-networking/#how-clusterips-are-allocated","title":"How ClusterIPs are Allocated","text":"<ul> <li>Dynamically: The control plane picks a free IP address from the configured IP range.</li> <li>Statically: You can specify an IP address within the configured IP range.</li> </ul>"},{"location":"services-and-networking/service-networking/#uniqueness-of-clusterip","title":"Uniqueness of ClusterIP","text":"<ul> <li>Every Service ClusterIP must be unique across the cluster.</li> <li>Creating a Service with an already allocated ClusterIP will result in an error.</li> </ul>"},{"location":"services-and-networking/service-networking/#why-reserve-clusterips","title":"Why Reserve ClusterIPs","text":"<ul> <li>For well-known IP addresses that other components and users in the cluster can use.</li> <li>Example: DNS Service in the cluster may use a well-known IP.</li> </ul>"},{"location":"services-and-networking/service-networking/#avoiding-clusterip-conflicts","title":"Avoiding ClusterIP Conflicts","text":"<ul> <li>Kubernetes has an allocation strategy to reduce the risk of collision.</li> <li>The ClusterIP range is divided based on a formula.</li> <li>Dynamic IP assignment uses the upper band by default.  </li> </ul> <p>For the KCAD exam, understanding how ClusterIPs are allocated, both dynamically and statically, is crucial. Also, knowing how to avoid conflicts and the uniqueness constraint can be vital.</p>"},{"location":"services-and-networking/service-networking/#service-internal-traffic-policy","title":"Service Internal Traffic Policy","text":""},{"location":"services-and-networking/service-networking/#key-points","title":"Key Points:","text":"<ul> <li>Feature State: Available in Kubernetes v1.26 as stable.</li> <li>Purpose: Allows internal traffic restrictions to only route internal traffic to endpoints within the originating node. This is useful for reducing costs and improving performance.</li> <li>Configuration: You can enable this feature by setting .spec.internalTrafficPolicy to Local in the Service specification.</li> <li>This instructs kube-proxy to only use node-local endpoints for cluster-internal traffic.</li> </ul> <pre><code>    apiVersion: v1\n    kind: Service\n    metadata:\n      name: my-service\n    spec:\n      selector:\n        app.kubernetes.io/name: MyApp\n      ports:\n        protocol: TCP\n          port: 80\n          targetPort: 9376\n      internalTrafficPolicy: Local\n</code></pre>"},{"location":"services-and-networking/service-networking/#how-it-works","title":"How it Works","text":"<ul> <li>The kube-proxy filters the endpoints based on the <code>.spec.internalTrafficPolicy</code> setting. When set to <code>Local</code>, only node-local endpoints - are considered. When set to <code>Cluster</code> (the default), or not set, all endpoints are considered.</li> </ul>"},{"location":"services-and-networking/service/","title":"Services","text":"<p>What is a Service? A Kubernetes Service is an abstraction layer that defines a logical set of Pods and enables external traffic exposure, load balancing, and service discovery.</p>"},{"location":"services-and-networking/service/#types-of-services","title":"Types of Services","text":"<ol> <li> <p>ClusterIP:</p> <ul> <li>Default type.</li> <li>Exposes the service on an internal IP in the cluster.</li> <li>Accessible only within the cluster.</li> </ul> </li> <li> <p>NodePort:</p> <ul> <li>Exposes the service on a static port on each Node\u2019s IP.</li> <li>External entities can access it by <code>:</code>.</li> </ul> </li> <li> <p>LoadBalancer:</p> <ul> <li>Provisions an external load balancer and assigns a fixed, external IP to the service.</li> <li>Typically used in cloud environments.</li> </ul> </li> <li> <p>ExternalName:</p> <ul> <li>Maps the service to the contents of the externalName field (e.g., <code>foo.bar.example.com</code>).</li> </ul> </li> </ol>"},{"location":"services-and-networking/service/#service-discovery","title":"Service Discovery","text":"<ul> <li>Services are discoverable through environment variables or DNS.</li> <li>The kube-dns component handles DNS-based service discovery.</li> </ul>"},{"location":"services-and-networking/service/#selector-and-labels","title":"Selector and Labels","text":"<ul> <li>Services route traffic to Pods based on label selectors.</li> </ul>"},{"location":"services-and-networking/service/#ports","title":"Ports","text":"<ul> <li>You can specify <code>port</code> (port exposed by the service), <code>targetPort</code> (port on the Pod), and <code>nodePort</code> (port on the node).</li> </ul>"},{"location":"services-and-networking/service/#endpoints","title":"Endpoints","text":"<ul> <li>Services have associated Endpoints that contain the IP addresses of the Pods the traffic should be routed to.</li> </ul>"},{"location":"services-and-networking/service/#session-affinity","title":"Session Affinity","text":"<ul> <li>Services support <code>None</code> and <code>ClientIP</code> session affinity to maintain session state.</li> </ul>"},{"location":"services-and-networking/service/#service-account","title":"Service Account","text":"<ul> <li>You can associate a service account to control the level of access a service has.</li> </ul>"},{"location":"services-and-networking/service/#headless-services","title":"Headless Services","text":"<ul> <li>Services without a ClusterIP for direct Pod-to-Pod communication.</li> </ul>"},{"location":"services-and-networking/service/#service-topology","title":"Service Topology","text":"<ul> <li>Allows routing of traffic based on Node labels.</li> </ul>"},{"location":"services-and-networking/service/#dual-stack-services","title":"Dual-Stack Services","text":"<ul> <li>Services can be IPv4/IPv6 dual-stack enabled for hybrid communication.</li> </ul>"},{"location":"services-and-networking/service/#quality-of-service-qos","title":"Quality of Service (QoS)","text":"<ul> <li>Services don't have QoS guarantees but the Pods backing them can have QoS classes like <code>Guaranteed</code>, <code>Burstable</code>, and <code>BestEffort</code>.</li> </ul>"},{"location":"services-and-networking/service/#service-mesh","title":"Service Mesh","text":"<ul> <li>Istio or Linkerd can be used for advanced service-to-service communication features like canary deployments, circuit breakers, etc.</li> </ul>"},{"location":"services-and-networking/topology-aware-routing/","title":"Topology Aware Routing","text":""},{"location":"services-and-networking/topology-aware-routing/#motivation","title":"Motivation","text":"<ul> <li>Designed for multi-zone environments.</li> <li>Aims to keep network traffic within the originating zone for reliability, performance, and cost.</li> </ul>"},{"location":"services-and-networking/topology-aware-routing/#enabling-topology-aware-routing","title":"Enabling Topology Aware Routing","text":"<ul> <li>Enabled by setting the <code>service.kubernetes.io/topology-mode</code> annotation to <code>Auto</code>.</li> <li>EndpointSlices will have Topology Hints populated to allocate endpoints to specific zones.</li> </ul>"},{"location":"services-and-networking/topology-aware-routing/#when-it-works-best","title":"When it works best","text":"<ul> <li>Even distribution of incoming traffic.</li> <li>Service has 3 or more endpoints per zone.</li> </ul>"},{"location":"services-and-networking/topology-aware-routing/#how-it-works","title":"How It Works","text":"<ul> <li>\"Auto\" heuristic proportionally allocates endpoints to each zone.</li> <li>EndpointSlice controller sets hints based on allocatable CPU cores in each zone.</li> </ul>"},{"location":"services-and-networking/topology-aware-routing/#endpointslice-controller","title":"EndpointSlice controller","text":"<ul> <li>Responsible for setting hints on EndpointSlices.</li> <li>Allocates endpoints based on the allocatable CPU cores for nodes in each zone.</li> </ul>"},{"location":"services-and-networking/topology-aware-routing/#kube-proxy","title":"kube-proxy","text":"<ul> <li>Filters endpoints based on hints set by the EndpointSlice controller.</li> <li>Sometimes allocates endpoints from different zones for even distribution.</li> </ul>"},{"location":"services-and-networking/topology-aware-routing/#safeguards","title":"Safeguards","text":"<ul> <li>Several rules to ensure safe use of Topology Aware Hints.</li> <li>If rules aren't met, <code>kube-proxy</code> selects endpoints from anywhere in the cluster.</li> </ul>"},{"location":"services-and-networking/topology-aware-routing/#constraints","title":"Constraints","text":"<ul> <li>Not used when <code>internalTrafficPolicy</code> is set to <code>Local</code>.</li> <li>Does not work well for Services with traffic originating from a subset of zones.</li> <li>Does not account for unready nodes or nodes with specific labels.</li> </ul>"},{"location":"services-and-networking/topology-aware-routing/#custom-heuristics","title":"Custom heuristics","text":"<ul> <li>Allows for custom heuristics if the built-in ones don't meet specific use cases.</li> </ul>"},{"location":"storage/csi-volume-cloning/","title":"CSI Volume Cloning","text":""},{"location":"storage/csi-volume-cloning/#introduction","title":"Introduction","text":"<p>The document introduces the concept of cloning existing Container Storage Interface (CSI) Volumes in Kubernetes. The feature allows you to specify existing Persistent Volume Claims (PVCs) in the <code>dataSource</code> field to indicate that you want to clone a volume. A clone is essentially a duplicate of an existing Kubernetes volume that behaves like any standard volume. The key difference is that upon provisioning, the backend device creates an exact duplicate of the specified volume instead of a new empty one.</p>"},{"location":"storage/csi-volume-cloning/#implementation","title":"Implementation","text":"<p>From the Kubernetes API perspective, cloning is implemented by allowing you to specify an existing PVC as a <code>dataSource</code> during new PVC creation. The source PVC must be bound and available, meaning it should not be in use.</p>"},{"location":"storage/csi-volume-cloning/#user-considerations","title":"User Considerations","text":"<ul> <li>Cloning support is only available for CSI drivers.</li> <li>Only dynamic provisioners support cloning.</li> <li>CSI drivers may or may not have implemented volume cloning.</li> <li>Cloning can only be done within the same namespace for both source and destination PVCs.</li> <li>Cloning is supported with different Storage Classes.</li> <li>The destination volume can have the same or a different storage class as the source.</li> <li>The default storage class can be used, and storageClassName can be omitted in the spec.</li> <li>Cloning can only be done between two volumes that use the same <code>VolumeMode</code> setting.</li> </ul>"},{"location":"storage/csi-volume-cloning/#provisioning","title":"Provisioning","text":"<p>Clones are provisioned like any other PVC, except that a <code>dataSource</code> is added that references an existing PVC in the same namespace. The document provides a YAML example for creating a new PVC that is a clone of an existing one.</p>"},{"location":"storage/csi-volume-cloning/#usage","title":"Usage","text":"<p>Once the new PVC is available, it can be consumed like any other PVC. It becomes an independent object that can be consumed, cloned, snapshotted, or deleted independently. The source is not linked to the newly created clone in any way, allowing for modifications or deletions without affecting the clone.</p>"},{"location":"storage/dynamic-volume-provisioning/","title":"Dynamic Volume Provisioning","text":""},{"location":"storage/dynamic-volume-provisioning/#overview","title":"Overview","text":"<ul> <li>Dynamic volume provisioning allows for the on-demand creation of storage volumes. This eliminates the need for cluster administrators to manually create storage volumes and their corresponding PersistentVolume objects in Kubernetes. The feature is based on the API object <code>StorageClass</code> from the API group <code>storage.k8s.io</code>.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#background","title":"Background","text":"<ul> <li>A cluster administrator can define multiple <code>StorageClass</code> objects, each specifying a volume plugin (provisioner) and the parameters to pass to that provisioner. This allows for the exposure of multiple types of storage within a cluster, each with custom parameters. This design abstracts the complexity of storage provisioning from end-users, allowing them to choose from multiple storage options.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#enabling-dynamic-provisioning","title":"Enabling Dynamic Provisioning","text":"<ul> <li>To enable this feature, a cluster administrator must pre-create one or more <code>StorageClass</code> objects. These objects define which provisioner should be used and what parameters should be passed when dynamic provisioning is invoked. For example, a storage class named \"slow\" might provision standard disk-like persistent disks, while a storage class named \"fast\" might provision SSD-like persistent disks.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#using-dynamic-provisioning","title":"Using Dynamic Provisioning","text":"<ul> <li>Users can request dynamically provisioned storage by including a storage class in their <code>PersistentVolumeClaim</code>. The <code>storageClassName</code> field of the <code>PersistentVolumeClaim</code> object must match the name of a <code>StorageClass</code> configured by the administrator. For instance, to select the \"fast\" storage class, a user would specify <code>storageClassName: fast</code> in their claim.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#defaulting-behavior","title":"Defaulting Behavior","text":"<ul> <li>Dynamic provisioning can be enabled such that all claims are dynamically provisioned if no storage class is specified. This is achieved by marking one <code>StorageClass</code> object as the default and ensuring that the <code>DefaultStorageClass</code> admission controller is enabled on the API server.</li> </ul>"},{"location":"storage/dynamic-volume-provisioning/#topology-awareness","title":"Topology Awareness","text":"<ul> <li>In Multi-Zone clusters, it's important that storage backends are provisioned in the Zones where Pods are scheduled. This ensures that Pods can be spread across Zones in a Region while still having access to the appropriate storage.</li> </ul>"},{"location":"storage/ephemeral-volumes/","title":"Ephemeral Volumes","text":""},{"location":"storage/ephemeral-volumes/#introduction","title":"Introduction","text":"<ul> <li>Ephemeral volumes are designed for temporary storage needs.</li> <li>They follow the Pod's lifetime and are created and deleted along with the Pod.</li> <li>Useful for caching services and read-only input data like configuration or secret keys.</li> </ul>"},{"location":"storage/ephemeral-volumes/#types-of-ephemeral-volumes","title":"Types of Ephemeral Volumes","text":"<ul> <li><code>emptyDir</code>: Empty at Pod startup, storage from kubelet base directory or RAM.</li> <li><code>configMap</code>, <code>downwardAPI</code>, <code>secret</code>: Inject Kubernetes data into a Pod.</li> <li><code>CSI ephemeral</code> volumes: Provided by special CSI drivers.</li> <li>Generic ephemeral volumes: Can be provided by any storage driver that supports dynamic provisioning.</li> </ul>"},{"location":"storage/ephemeral-volumes/#csi-ephemeral-volumes","title":"CSI Ephemeral Volumes","text":"<ul> <li>Managed locally on each node.</li> <li>Created after a Pod has been scheduled onto a node.</li> <li>No concept of rescheduling Pods.</li> <li>Not covered by storage resource usage limits of a Pod.</li> </ul>"},{"location":"storage/ephemeral-volumes/#generic-ephemeral-volumes","title":"Generic Ephemeral Volumes","text":"<ul> <li>Similar to emptyDir but may have additional features like fixed size, initial data, etc.</li> <li>Supports typical volume operations like snapshotting, cloning, resizing, and storage capacity tracking.</li> </ul>"},{"location":"storage/ephemeral-volumes/#lifecycle-and-persistentvolumeclaim","title":"Lifecycle and PersistentVolumeClaim","text":"<ul> <li>PVC parameters are allowed inside a volume source of the Pod.</li> <li>When the Pod is created, an actual PVC object is created and deleted along with the Pod.</li> <li>PVCs can be used like any other PVCs, including as data sources in volume cloning or snapshotting.</li> </ul>"},{"location":"storage/ephemeral-volumes/#security-considerations","title":"Security Considerations","text":"<ul> <li>Allows users to create PVCs indirectly.</li> <li>Normal namespace quota for PVCs still applies.</li> </ul>"},{"location":"storage/node-specific-volume-limits/","title":"Node Specific Volume Limits","text":""},{"location":"storage/node-specific-volume-limits/#kubernetes-default-limits","title":"Kubernetes Default Limits","text":"<ul> <li>Amazon Elastic Block Store (EBS): 39 volumes per Node</li> <li>Google Persistent Disk: 16 volumes per Node</li> <li>Microsoft Azure Disk Storage: 16 volumes per Node</li> </ul>"},{"location":"storage/node-specific-volume-limits/#custom-limits","title":"Custom Limits","text":"<p>You can customize these limits by setting the value of the <code>KUBE_MAX_PD_VOLS</code> environment variable and then restarting the scheduler. For CSI drivers, you may need to consult their specific documentation for customization procedures.</p>"},{"location":"storage/node-specific-volume-limits/#dynamic-volume-limits","title":"Dynamic Volume Limits","text":"<p>As of Kubernetes v1.17, dynamic volume limits are supported for Amazon EBS, Google Persistent Disk, Azure Disk, and CSI. Kubernetes automatically determines the Node type and enforces the appropriate maximum number of volumes for that node. For example: - On Google Compute Engine, up to 127 volumes can be attached, depending on the node type. - For Amazon EBS disks on M5, C5, R5, T3, and Z1D instance types, only 25 volumes can be attached. - On Azure, up to 64 disks can be attached, depending on the node type.</p>"},{"location":"storage/node-specific-volume-limits/#csi-driver-limits","title":"CSI Driver Limits","text":"<p>If a CSI storage driver advertises a maximum number of volumes for a Node, the kube-scheduler will honor that limit.</p>"},{"location":"storage/persistent-volumes/","title":"Persistent Volumes","text":""},{"location":"storage/persistent-volumes/#introduction","title":"Introduction","text":"<ul> <li>PVs are abstraction layers for storage in Kubernetes.</li> <li>PVCs are requests for PV resources by pods.</li> <li>PVs are cluster-wide and can be used by multiple pods.</li> </ul>"},{"location":"storage/persistent-volumes/#lifecycle-of-a-volume-and-claim","title":"Lifecycle of a Volume and Claim","text":""},{"location":"storage/persistent-volumes/#provisioning","title":"Provisioning","text":"<ul> <li>PVs can be provisioned statically or dynamically.</li> <li>Dynamic provisioning relies on StorageClasses.</li> <li>StorageClasses define provisioning mechanisms (e.g., AWS EBS, GCE PD).</li> </ul>"},{"location":"storage/persistent-volumes/#binding","title":"Binding","text":"<ul> <li>PVCs are bound to suitable PVs based on labels, storage capacity, and access modes.</li> </ul>"},{"location":"storage/persistent-volumes/#using","title":"Using","text":"<ul> <li>Pods specify PVCs in their volume specifications.</li> <li>Multiple pods can use the same PVC, but only one pod can mount it in <code>ReadWrite</code> mode at a time.</li> </ul>"},{"location":"storage/persistent-volumes/#reclaiming","title":"Reclaiming","text":"<ul> <li>PVs can be retained, recycled, or deleted after PVC release.</li> <li>Retain: PV data is preserved.</li> <li>Recycle: Data is deleted and PV can be reused.</li> <li>Delete: PV is deleted along with data.</li> </ul>"},{"location":"storage/persistent-volumes/#storage-object-in-use-protection","title":"Storage Object in Use Protection","text":"<ul> <li>PVs with bound PVCs have a finalizer to prevent accidental deletion.</li> <li>Ensures data safety while PVCs are in use.</li> </ul>"},{"location":"storage/persistent-volumes/#reclaiming_1","title":"Reclaiming","text":"<ul> <li>Defines PV's behavior after PVC release.</li> <li>Options include Retain, Recycle, and Delete.</li> <li>Appropriate setting depends on use case.</li> </ul>"},{"location":"storage/persistent-volumes/#persistentvolume-claims","title":"PersistentVolume Claims","text":"<ul> <li>PVCs request storage resources.</li> <li>They specify access modes (<code>ReadWriteOnce</code>, <code>ReadOnlyMany</code>, <code>ReadWriteMany</code>), resource requests, and StorageClass.</li> <li>Reference a StorageClass to dynamically provision PVs.</li> </ul>"},{"location":"storage/persistent-volumes/#access-modes","title":"Access Modes","text":"<ul> <li><code>ReadWriteOnce</code>: Can be mounted as read-write by a single node.</li> <li><code>ReadOnlyMany</code>: Can be mounted read-only by many nodes.</li> <li><code>ReadWriteMany</code>: Can be mounted as read-write by many nodes.</li> </ul>"},{"location":"storage/persistent-volumes/#volume-modes","title":"Volume Modes","text":"<ul> <li>PVCs can specify volume modes:</li> <li>Filesystem: Usual file-based volumes.</li> <li>Block: Raw block devices.</li> </ul>"},{"location":"storage/persistent-volumes/#resources","title":"Resources","text":"<ul> <li>PVCs request storage capacity (e.g., 1Gi) and StorageClass.</li> <li>Helps in selecting an appropriate PV.</li> </ul>"},{"location":"storage/persistent-volumes/#selector","title":"Selector","text":"<ul> <li>PVCs can use selectors to filter PVs based on labels and annotations.</li> <li>Useful for matching specific criteria.</li> </ul>"},{"location":"storage/persistent-volumes/#class","title":"Class","text":"<ul> <li>StorageClass defines storage type (e.g., SSD, HDD) and provisioning.</li> <li>PVCs reference a StorageClass to request storage.</li> </ul>"},{"location":"storage/persistent-volumes/#claims-as-volumes","title":"Claims As Volumes","text":"<ul> <li>Pods can consume PVCs as volumes.</li> <li>Allows dynamic provisioning based on pod requirements.</li> </ul>"},{"location":"storage/persistent-volumes/#raw-block-volume-support","title":"Raw Block Volume Support","text":"<ul> <li>Kubernetes supports raw block volumes for high-performance workloads.</li> <li>PVCs request raw block volumes.</li> <li>Useful for databases and applications needing low-level access.</li> </ul>"},{"location":"storage/persistent-volumes/#volume-snapshot-and-restore","title":"Volume Snapshot and Restore","text":"<ul> <li>Kubernetes supports volume snapshots and restoration.</li> <li>Users can create, clone, and restore volumes from snapshots.</li> </ul>"},{"location":"storage/persistent-volumes/#volume-cloning","title":"Volume Cloning","text":"<ul> <li>Enables creating PVCs from existing PV data.</li> <li>Useful for scaling applications or creating replicas.</li> </ul>"},{"location":"storage/persistent-volumes/#volume-populators-and-data-sources","title":"Volume Populators and Data Sources","text":"<ul> <li>Populators enable dynamic provisioning from data sources.</li> <li>Data sources can be external data or other PVCs.</li> </ul>"},{"location":"storage/persistent-volumes/#cross-namespace-data-sources","title":"Cross-Namespace Data Sources","text":"<ul> <li>Data sources can be referenced across namespaces.</li> <li>Enhances flexibility in PVC usage.</li> </ul>"},{"location":"storage/persistent-volumes/#data-source-references","title":"Data Source References","text":"<ul> <li>PVCs can reference data sources to create volumes.</li> <li>Supports various volume types and scenarios.</li> </ul>"},{"location":"storage/persistent-volumes/#using-volume-populators","title":"Using Volume Populators","text":"<ul> <li>Populators facilitate on-demand provisioning based on data sources.</li> <li>Ideal for dynamic storage allocation.</li> </ul>"},{"location":"storage/persistent-volumes/#using-a-cross-namespace-volume-data-source","title":"Using a Cross-Namespace Volume Data Source","text":"<ul> <li>Data sources can be referenced from different namespaces.</li> <li>Enables sharing data sources across projects.</li> </ul>"},{"location":"storage/projected-volumes/","title":"Projected Volumes","text":""},{"location":"storage/projected-volumes/#introduction","title":"Introduction","text":"<ul> <li>Projected volumes map multiple existing volume sources into a single directory.</li> <li>Supported volume sources: <code>secret</code>, <code>downwardAPI</code>, <code>configMap</code>, <code>serviceAccountToken</code>.</li> <li>All sources must be in the same namespace as the Pod.</li> </ul>"},{"location":"storage/projected-volumes/#example-configuration-secret-downwardapi-configmap","title":"Example Configuration: Secret, DownwardAPI, ConfigMap","text":"<ul> <li>Demonstrates how to combine <code>secret</code>, <code>downwardAPI</code>, and <code>configMap</code> in a single Pod.</li> <li>Uses <code>apiVersion: v1, kind: Pod</code>, and specifies volume sources under projected.sources.</li> </ul>"},{"location":"storage/projected-volumes/#example-configuration-non-default-permission-mode","title":"Example Configuration: Non-Default Permission Mode","text":"<ul> <li>Shows how to set a non-default permission mode for secrets.</li> <li>Uses mode: 511 to set specific permissions for the secret.</li> </ul>"},{"location":"storage/projected-volumes/#serviceaccounttoken-projected-volumes","title":"ServiceAccountToken Projected Volumes","text":"<ul> <li>Allows injecting the token for the current service account into a Pod.</li> <li>Fields:<ul> <li>audience: Intended audience of the token (optional).</li> <li>expirationSeconds: Token validity duration, at least 10 minutes.</li> <li>path: Relative path to the mount point.</li> </ul> </li> </ul>"},{"location":"storage/projected-volumes/#securitycontext-interactions","title":"SecurityContext Interactions","text":""},{"location":"storage/projected-volumes/#linux","title":"Linux","text":"<ul> <li>Projected files have correct ownership, including container user ownership.</li> </ul>"},{"location":"storage/projected-volumes/#windows","title":"Windows","text":"<ul> <li>Ownership is not enforced due to virtual SAM database in each container.</li> <li>Recommended to place shared files in their own volume mount outside of <code>C:\\\\</code>.</li> </ul>"},{"location":"storage/storage-capacity/","title":"Storage Capacity","text":""},{"location":"storage/storage-capacity/#feature-state","title":"Feature State","text":"<ul> <li>The feature is stable as of Kubernetes v1.24. It helps Kubernetes keep track of storage capacity and aids the scheduler in placing Pods on nodes with sufficient storage.</li> </ul>"},{"location":"storage/storage-capacity/#before-you-begin","title":"Before You Begin","text":"<ul> <li>To utilize storage capacity tracking, you must be running Kubernetes v1.28 or above and use a CSI driver that supports this feature.</li> </ul>"},{"location":"storage/storage-capacity/#api-extensions","title":"API Extensions","text":"<ul> <li><code>CSIStorageCapacity</code> objects: Created by a CSI driver in its namespace, each object contains capacity information for one storage class and specifies which nodes can access that storage.</li> <li><code>CSIDriverSpec.StorageCapacity</code> field: When set to true, the Kubernetes scheduler considers storage capacity for volumes using the CSI driver.</li> </ul>"},{"location":"storage/storage-capacity/#scheduling","title":"Scheduling","text":"<ul> <li>The scheduler uses storage capacity information if:</li> <li>A Pod uses a yet-to-be-created volume.</li> <li>The volume uses a StorageClass that references a CSI driver and uses <code>WaitForFirstConsumer</code> volume binding mode.</li> <li>The CSIDriver object for the driver has StorageCapacity set to true.</li> <li>In this case, the scheduler only considers nodes with enough storage. The check is basic and compares the volume size against the capacity listed in CSIStorageCapacity objects that include the node.</li> </ul>"},{"location":"storage/storage-capacity/#rescheduling","title":"Rescheduling","text":"<ul> <li>Node selection is tentative until the CSI driver confirms the volume creation. If the volume can't be created due to outdated capacity information, the scheduler retries.</li> </ul>"},{"location":"storage/storage-capacity/#limitations","title":"Limitations","text":"<ul> <li>Scheduling can fail permanently if a Pod uses multiple volumes and one volume consumes all the available capacity in a topology segment.</li> <li>The feature increases the chance of successful scheduling but doesn't guarantee it due to potentially outdated information.</li> </ul>"},{"location":"storage/storage-classes/","title":"Storage Classes","text":""},{"location":"storage/storage-classes/#introduction","title":"Introduction","text":"<ul> <li>StorageClass in Kubernetes allows administrators to define different \"classes\" of storage. These classes can represent various quality-of-service levels, backup policies, or any arbitrary policies set by the administrators. Kubernetes doesn't enforce what these classes should represent.</li> </ul>"},{"location":"storage/storage-classes/#the-storageclass-resource","title":"The StorageClass Resource","text":"<ul> <li>A StorageClass contains fields like provisioner, parameters, and reclaimPolicy. These are used when dynamically provisioning a</li> <li>PersistentVolume (PV) that belongs to the class. The name of the StorageClass is significant and is used by users to request a specific class. Administrators can also set a default StorageClass for PVCs that don't specify any class.</li> </ul>"},{"location":"storage/storage-classes/#default-storageclass","title":"Default StorageClass","text":"<ul> <li>If a PersistentVolumeClaim (PVC) doesn't specify a storageClassName, the cluster's default StorageClass is used. Only one default StorageClass can exist in a cluster.</li> </ul>"},{"location":"storage/storage-classes/#provisioner","title":"Provisioner","text":"<ul> <li>Specifies what volume plugin is used for provisioning PVs. Both internal and external provisioners can be used. For example, NFS doesn't have an internal provisioner but can use an external one.</li> </ul>"},{"location":"storage/storage-classes/#reclaim-policy","title":"Reclaim Policy","text":"<ul> <li>Specifies what happens to a dynamically created PV when it is released. The options are <code>Delete</code> or <code>Retain</code>.</li> </ul>"},{"location":"storage/storage-classes/#allow-volume-expansion","title":"Allow Volume Expansion","text":"<ul> <li>Indicates whether a PV can be expanded. This is controlled by the allowVolumeExpansion field in the StorageClass.</li> </ul>"},{"location":"storage/storage-classes/#mount-options","title":"Mount Options","text":"<ul> <li>Specifies mount options for dynamically created PVs. If an invalid mount option is given, the PV mount will fail.</li> </ul>"},{"location":"storage/storage-classes/#volume-binding-mode","title":"Volume Binding Mode","text":"<ul> <li>Controls when volume binding and provisioning occur. The default is <code>Immediate</code> mode, but <code>WaitForFirstConsumer</code> mode can be used for topology-constrained storage backends.</li> </ul>"},{"location":"storage/storage-classes/#allowed-topologies","title":"Allowed Topologies","text":"<ul> <li>Used to restrict the topology of provisioned volumes to specific zones.</li> </ul>"},{"location":"storage/storage-classes/#parameters","title":"Parameters","text":"<ul> <li>Describes additional provisioning parameters that are specific to the provisioner. For example, AWS EBS-specific parameters like <code>type</code>, <code>iopsPerGB</code>, etc.</li> </ul>"},{"location":"storage/storage-classes/#aws-ebs","title":"AWS EBS","text":"<ul> <li>Provides an example of how to define a StorageClass for AWS EBS, including various parameters like <code>type</code>, <code>iopsPerGB</code>, and <code>fsType</code>.</li> </ul>"},{"location":"storage/storage-classes/#gce-pd","title":"GCE PD","text":"<ul> <li>Similar to AWS but for Google Cloud's Persistent Disk. Includes parameters like <code>type</code>, <code>fstype</code>, and <code>replication-type</code>.</li> </ul>"},{"location":"storage/storage-classes/#nfs","title":"NFS","text":"<ul> <li>Explains that Kubernetes doesn't have an internal NFS provisioner and provides examples of how to use an external NFS provisioner.</li> </ul>"},{"location":"storage/storage-classes/#vsphere","title":"vSphere","text":"<ul> <li>Discusses two types of provisioners for vSphere and provides examples of how to define a StorageClass for vSphere.</li> </ul>"},{"location":"storage/storage-classes/#ceph-rbd","title":"Ceph RBD","text":"<ul> <li>Notes that the internal provisioner for Ceph RBD is deprecated and provides an example of a StorageClass for Ceph.</li> <li>Azure Disk: Provides an example of a StorageClass for Azure Disk but the content is truncated.</li> </ul>"},{"location":"storage/volume-health-monitoring/","title":"Volume Health Monitoring","text":""},{"location":"storage/volume-health-monitoring/#feature-state","title":"Feature State","text":"<p>The feature is in alpha state as of Kubernetes v1.21.</p>"},{"location":"storage/volume-health-monitoring/#overview","title":"Overview","text":"<p>Volume Health Monitoring in Kubernetes is part of the Container Storage Interface (CSI). It allows CSI Drivers to detect abnormal conditions in the underlying storage systems and report them as events on Persistent Volume Claims (PVCs) or Pods.</p>"},{"location":"storage/volume-health-monitoring/#components","title":"Components","text":"<p>The feature is implemented in two main components: 1. External Health Monitor Controller: This controller watches for abnormal volume conditions and reports them on the related PVC. 2. Kubelet: It also plays a role in volume health monitoring.</p>"},{"location":"storage/volume-health-monitoring/#controller-side-monitoring","title":"Controller-Side Monitoring","text":"<p>If a CSI Driver supports this feature from the controller side, an event will be reported on the related PVC when an abnormal volume condition is detected. The External Health Monitor controller also watches for node failure events. You can enable node failure monitoring by setting the <code>enable-node-watcher</code> flag to true. When a node failure is detected, an event is reported on the PVC to indicate that pods using this PVC are on a failed node.</p>"},{"location":"storage/volume-health-monitoring/#node-side-monitoring","title":"Node-Side Monitoring","text":"<p>If a CSI Driver supports this feature from the node side, an event will be reported on every Pod using the PVC when an abnormal volume condition is detected.</p>"},{"location":"storage/volume-health-monitoring/#metrics","title":"Metrics","text":"<p>Volume Health information is also exposed as Kubelet VolumeStats metrics. A new metric <code>kubelet_volume_stats_health_status_abnormal</code> is added, which includes two labels: <code>namespace</code> and <code>persistentvolumeclaim</code>. The count is either 1 or 0, where 1 indicates the volume is unhealthy and 0 indicates the volume is healthy.</p>"},{"location":"storage/volume-snapshot-classes/","title":"Volume Snapshot Classes","text":""},{"location":"storage/volume-snapshot-classes/#introduction","title":"Introduction","text":"<ul> <li>The concept of VolumeSnapshotClass in Kubernetes is similar to that of StorageClass. While StorageClass allows administrators to define \"classes\" of storage for provisioning volumes, VolumeSnapshotClass serves the same purpose but for provisioning volume snapshots.</li> </ul>"},{"location":"storage/volume-snapshot-classes/#the-volumesnapshotclass-resource","title":"The VolumeSnapshotClass Resource","text":"<ul> <li>A VolumeSnapshotClass contains three main fields:</li> <li>Driver: Specifies the CSI volume plugin used for provisioning VolumeSnapshots.</li> <li>DeletionPolicy: Configures what happens to a VolumeSnapshotContent when the associated VolumeSnapshot is deleted. It can be either <code>Retain</code> or <code>Delete</code>.</li> <li>Parameters: Describes additional configurations for volume snapshots belonging to this class.</li> </ul> <p>The name of the VolumeSnapshotClass object is significant as it is used by users to request a particular class. Once created, these objects cannot be updated. Here's an example YAML configuration:</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: csi-hostpath-snapclass\ndriver: hostpath.csi.k8s.io\ndeletionPolicy: Delete\nparameters:\n</code></pre>"},{"location":"storage/volume-snapshot-classes/#default-volumesnapshotclass","title":"Default VolumeSnapshotClass","text":"<ul> <li>Administrators can specify a default VolumeSnapshotClass for those VolumeSnapshots that don't request any particular class. This is done by adding an annotation <code>snapshot.storage.kubernetes.io/is-default-class: \"true\"</code>.</li> </ul> <p>Example: <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: csi-hostpath-snapclass\n  annotations:\n    snapshot.storage.kubernetes.io/is-default-class: \"true\"\ndriver: hostpath.csi.k8s.io\ndeletionPolicy: Delete\nparameters:\n</code></pre></p>"},{"location":"storage/volume-snapshot-classes/#deletion-policy","title":"Deletion Policy","text":"<ul> <li>The <code>deletionPolicy</code> can be either <code>Retain</code> or <code>Delete</code>:</li> <li><code>Retain</code>: The underlying snapshot and VolumeSnapshotContent remain even if the VolumeSnapshot is deleted.</li> <li><code>Delete</code>: Both the underlying storage snapshot and the VolumeSnapshotContent object are deleted when the VolumeSnapshot is deleted.</li> </ul>"},{"location":"storage/volume-snapshots/","title":"Volume Snapshots","text":""},{"location":"storage/volume-snapshots/#introduction","title":"Introduction","text":"<p>In Kubernetes, a VolumeSnapshot represents a snapshot of a volume on a storage system. The document assumes familiarity with Kubernetes persistent volumes. VolumeSnapshotContent and VolumeSnapshot API resources are provided to create volume snapshots. A VolumeSnapshotContent is a snapshot taken from a volume in the cluster, provisioned by an administrator. It is a resource in the cluster, similar to a PersistentVolume.</p>"},{"location":"storage/volume-snapshots/#api-objects-and-support","title":"API Objects and Support","text":"<p>VolumeSnapshot, VolumeSnapshotContent, and VolumeSnapshotClass are Custom Resource Definitions (CRDs), not part of the core API. VolumeSnapshot support is only available for CSI drivers. A snapshot controller and a sidecar helper container called csi-snapshotter are deployed as part of the VolumeSnapshot deployment process. The snapshot controller watches VolumeSnapshot and VolumeSnapshotContent objects and is responsible for their creation and deletion.</p>"},{"location":"storage/volume-snapshots/#lifecycle","title":"Lifecycle","text":"<p>VolumeSnapshotContents are resources in the cluster, while VolumeSnapshots are requests for those resources. Snapshots can be provisioned in two ways: pre-provisioned or dynamically provisioned. In pre-provisioned, a cluster administrator creates VolumeSnapshotContents with details of the real volume snapshot on the storage system. In dynamic provisioning, a snapshot is taken from a PersistentVolumeClaim.</p>"},{"location":"storage/volume-snapshots/#binding","title":"Binding","text":"<p>The snapshot controller handles the binding of a VolumeSnapshot object with an appropriate VolumeSnapshotContent object. The binding is a one-to-one mapping.</p>"},{"location":"storage/volume-snapshots/#protection","title":"Protection","text":"<p>While taking a snapshot of a PersistentVolumeClaim, that PersistentVolumeClaim is in-use. Deletion of the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.</p>"},{"location":"storage/volume-snapshots/#deletion","title":"Deletion","text":"<p>Deletion is triggered by deleting the VolumeSnapshot object, and the DeletionPolicy will be followed. If the DeletionPolicy is Delete, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object.</p>"},{"location":"storage/volume-snapshots/#volumesnapshots-and-volumesnapshotcontents-specs","title":"VolumeSnapshots and VolumeSnapshotContents Specs","text":"<p>Each VolumeSnapshot contains a spec and a status. For dynamically provisioning a snapshot, <code>volumeHandle</code> is the unique identifier of the volume created on the storage backend. For pre-provisioned snapshots, <code>snapshotHandle</code> is the unique identifier of the volume snapshot created on the storage backend.</p>"},{"location":"storage/volume-snapshots/#converting-volume-mode","title":"Converting Volume Mode","text":"<p>If the VolumeSnapshots API supports the <code>sourceVolumeMode</code> field, then it has the capability to prevent unauthorized users from converting the mode of a volume.</p>"},{"location":"storage/volume-snapshots/#provisioning-volumes-from-snapshots","title":"Provisioning Volumes from Snapshots","text":"<p>You can provision a new volume, pre-populated with data from a snapshot, by using the <code>dataSource</code> field in the PersistentVolumeClaim object.</p>"},{"location":"storage/volumes/","title":"Background","text":"<ul> <li>Kubernetes volumes are essential for managing data within pods.</li> <li>They are abstracted from the underlying storage, making it easier to handle storage in a containerized environment.</li> <li>Volumes allow data sharing between containers within a pod and enable persistence of data.  </li> </ul>"},{"location":"storage/volumes/#types-of-volumes","title":"Types of Volumes","text":""},{"location":"storage/volumes/#hostpath","title":"hostPath","text":"<ul> <li>Mounts files or directories from the node's file system into pods.</li> <li>Useful for accessing node-specific resources.</li> <li>Note that this can pose security and portability concerns.</li> </ul>"},{"location":"storage/volumes/#configmap","title":"configMap","text":"<ul> <li>ConfigMap volumes enable pods to access configuration data.</li> <li>Ideal for injecting configuration settings, environment variables, or configuration files into pods.</li> <li>Enhances pod flexibility by separating configuration from container images.</li> </ul>"},{"location":"storage/volumes/#downwardapi","title":"downwardAPI","text":"<ul> <li>Downward API volumes expose pod and container metadata as files.</li> <li>Pods can consume metadata like pod name, namespace, labels, and annotations.</li> <li>Allows for dynamic configuration based on pod context.</li> </ul>"},{"location":"storage/volumes/#emptydir","title":"emptyDir","text":"<ul> <li>An ephemeral volume created when a pod is assigned to a node.</li> <li>Useful for temporary storage needs within a pod.</li> <li>Data in emptyDir volumes is lost when the pod is removed.</li> </ul>"},{"location":"storage/volumes/#fc-fibre-channel","title":"fc (Fibre Channel)","text":"<ul> <li>Connects pods to Fibre Channel storage devices.</li> <li>Requires specialized hardware and drivers for Fibre Channel connectivity.</li> <li>Typically used in enterprise environments with Fibre Channel storage infrastructure.</li> </ul>"},{"location":"storage/volumes/#cephfs","title":"cephfs","text":"<ul> <li>Allows pods to mount the Ceph File System.</li> <li>Ceph is a distributed storage system that provides scalability and data redundancy.</li> <li>Useful for applications requiring shared file storage.</li> </ul>"},{"location":"storage/volumes/#awselasticblockstore-removed","title":"awsElasticBlockStore (Removed)","text":"<ul> <li>This volume type was used for managing AWS Elastic Block Store (EBS) volumes.</li> <li>EBS volumes provide block-level storage for AWS instances.</li> <li>Deprecated in favor of using the Container Storage Interface (CSI) or other storage options supported by AWS.</li> </ul>"},{"location":"storage/volumes/#azuredisk-removed","title":"azureDisk (Removed)","text":"<ul> <li>Used for attaching Azure Disk storage to pods.</li> <li>Azure Disks are durable and scalable storage options in Azure.</li> <li>Deprecated, and users are encouraged to use CSI drivers for Azure or other suitable options.</li> </ul>"},{"location":"storage/volumes/#cinder-removed","title":"cinder (Removed)","text":"<ul> <li>Cinder volumes were used for OpenStack Cinder block storage.</li> <li>Cinder provides block storage management for OpenStack.</li> <li>Deprecated; recommended to use CSI drivers or other OpenStack volume solutions.</li> </ul>"},{"location":"storage/volumes/#glusterfs-removed","title":"glusterfs (Removed)","text":"<ul> <li>Previously used for mounting GlusterFS distributed file systems.</li> <li>GlusterFS provides scalable and distributed storage.</li> <li>Deprecated; use CSI drivers or alternative GlusterFS options.</li> </ul>"},{"location":"storage/volumes/#azurefile-deprecated","title":"azureFile (Deprecated)","text":"<ul> <li>Previously used for mounting Azure File Shares in pods.</li> <li>Azure Files provide managed file shares in Azure.</li> <li>Deprecated; consider using CSI drivers for Azure or other alternatives.</li> </ul>"},{"location":"storage/volumes/#gcepersistentdisk-deprecated","title":"gcePersistentDisk (Deprecated)","text":"<ul> <li>Previously used for attaching Google Compute Engine (GCE) Persistent Disks.</li> <li>GCE Persistent Disks offer durable block storage in Google Cloud.</li> <li>Deprecated; consider using CSI drivers for GCE or other suitable GCE storage options.</li> </ul>"},{"location":"storage/volumes/#gitrepo-deprecated","title":"gitRepo (Deprecated)","text":"<ul> <li>Deprecated volume type that clones a Git repository into a volume.</li> <li>Rarely used in practice due to better alternatives like init containers or Git-based CI/CD workflows.</li> </ul>"},{"location":"workloads/daemonsets/","title":"DaemonSets","text":""},{"location":"workloads/daemonsets/#what-is-a-daemonset","title":"What is a DaemonSet?","text":"<ul> <li>A DaemonSet ensures that all (or some) nodes in a Kubernetes cluster run a copy of a specific Pod.</li> <li>Typical uses include running a cluster storage daemon, logs collection daemon, or node monitoring daemon on every node.</li> </ul>"},{"location":"workloads/daemonsets/#writing-a-daemonset-spec","title":"Writing a DaemonSet Spec","text":"<ul> <li>You can describe a DaemonSet in a YAML file.</li> <li>Required fields include <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code>.</li> </ul>"},{"location":"workloads/daemonsets/#pod-template","title":"Pod Template","text":"<ul> <li>The <code>.spec.template</code> is a Pod template with the same schema as a Pod.</li> <li>It must have a <code>RestartPolicy</code> equal to <code>Always</code>.</li> </ul>"},{"location":"workloads/daemonsets/#pod-selector","title":"Pod Selector","text":"<ul> <li>The <code>.spec.selector</code> field is a pod selector that must match the labels of the <code>.spec.template</code>.</li> </ul>"},{"location":"workloads/daemonsets/#running-pods-on-select-nodes","title":"Running Pods on Select Nodes","text":"<ul> <li>You can specify node selectors or affinities to control on which nodes the Pods will run.</li> </ul>"},{"location":"workloads/daemonsets/#how-daemon-pods-are-scheduled","title":"How Daemon Pods are Scheduled","text":"<ul> <li>The DaemonSet controller adds spec.affinity.nodeAffinity to match the target host.</li> <li>Different schedulers can be specified for the Pods.</li> </ul>"},{"location":"workloads/daemonsets/#taints-and-tolerations","title":"Taints and Tolerations","text":"<ul> <li>DaemonSet controller automatically adds a set of tolerations to DaemonSet Pods to ensure they can be scheduled even under various node conditions.</li> </ul>"},{"location":"workloads/daemonsets/#communicating-with-daemon-pods","title":"Communicating with Daemon Pods","text":"<ul> <li>Various patterns like <code>Push</code>, <code>NodeIP</code> and <code>Known Port</code>, <code>DNS</code>, and <code>Service</code> can be used for communication.</li> </ul>"},{"location":"workloads/daemonsets/#updating-a-daemonset","title":"Updating a DaemonSet","text":"<ul> <li>Node labels can be changed, and the DaemonSet will update accordingly.</li> <li>Rolling updates and rollbacks can be performed.</li> </ul>"},{"location":"workloads/daemonsets/#alternatives-to-daemonset","title":"Alternatives to DaemonSet","text":"<ul> <li>Daemon processes can also be run directly on nodes using init scripts, but DaemonSets offer several advantages like monitoring, logging, and resource isolation.</li> </ul>"},{"location":"workloads/daemonsets/#deployments-vs-daemonsets","title":"Deployments vs DaemonSets","text":"<ul> <li>Use Deployments for stateless services and DaemonSets for node-level functionalities.</li> </ul>"},{"location":"workloads/deployments/","title":"Deployments","text":"<p>A Kubernetes Deployment is a higher-level abstraction designed to manage the desired state of a set of replicated Pods. It allows you to describe an intended state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. Here are some key features and functionalities:  </p>"},{"location":"workloads/deployments/#key-features","title":"Key Features","text":"<ul> <li>Replica Management: You can specify the number of Pod replicas you want to run.</li> <li>Updates: Allows for rolling updates to Pods, ensuring zero downtime.</li> <li>Rollbacks: If something goes wrong, you can roll back to a previous stable version.</li> <li>Scaling: You can easily scale your application up or down.</li> <li>Self-healing: Automatically replaces failed or unhealthy Pods.</li> </ul>"},{"location":"workloads/deployments/#common-operations","title":"Common Operations","text":"<ul> <li>Create a Deployment: Usually done through a YAML file that describes the Deployment.</li> <li>Inspect a Deployment: Using commands like <code>kubectl get deployments</code> or <code>kubectl describe deployment &lt;deployment-name&gt;</code>.</li> <li>Update a Deployment: You can update the image or other fields in the YAML and apply it.</li> <li>Rollback a Deployment: Using <code>kubectl rollout undo deployment &lt;deployment-name&gt;</code>.</li> <li>Scale a Deployment: Using <code>kubectl scale deployment &lt;deployment-name&gt; --replicas=number</code>.</li> </ul>"},{"location":"workloads/deployments/#yaml-example","title":"YAML Example","text":"<p>Here's a simple example of a Deployment YAML file:</p> <p><code>yaml apiVersion: apps/v1 kind: Deployment metadata:   name: nginx-deployment spec:   replicas: 3   selector:     matchLabels:       app: nginx   template:     metadata:       labels:         app: nginx     spec:       containers:       - name: nginx         image: nginx:1.14.2</code></p>"},{"location":"workloads/jobs/","title":"Workloads","text":""},{"location":"workloads/jobs/#running-an-example-job","title":"Running an example Job","text":"<ul> <li>Use <code>kubectl create -f</code> to create a Job from a YAML file.</li> <li><code>kubectl get jobs</code> to list all Jobs and their statuses.</li> </ul>"},{"location":"workloads/jobs/#writing-a-job-spec","title":"Writing a Job spec","text":"<ul> <li><code>metadata.name</code> to specify the Job name.</li> <li><code>spec.template.spec.containers[].image</code> to specify the container image.</li> <li><code>spec.template.spec.restartPolicy</code> must be either <code>Never</code> or <code>OnFailure</code>.</li> </ul>"},{"location":"workloads/jobs/#job-labels","title":"Job Labels","text":"<ul> <li>Labels are key-value pairs attached to Jobs.</li> <li>Useful for organizing and querying Jobs.</li> </ul>"},{"location":"workloads/jobs/#pod-template","title":"Pod Template","text":"<ul> <li>Nested inside the Job spec under spec.template.</li> <li>Specifies the Pod's containers, volumes, and other configurations.</li> </ul>"},{"location":"workloads/jobs/#pod-selector","title":"Pod selector","text":"<ul> <li>Automatically generated based on Job's <code>metadata.labels</code>.</li> <li>Do not set <code>.spec.selector</code> field manually unless you know what you're doing.</li> </ul>"},{"location":"workloads/jobs/#parallel-execution-for-jobs","title":"Parallel execution for Jobs","text":"<ul> <li><code>spec.parallelism</code>: Number of Pods running simultaneously.</li> <li><code>spec.completions</code>: Number of Pods that must complete successfully for the Job to be marked as complete.</li> </ul>"},{"location":"workloads/jobs/#completion-mode","title":"Completion mode","text":"<ul> <li>Indexed Jobs: Each Pod gets a unique index between 0 and spec.completions-1. NonIndexed: No unique identifiers for Pods.</li> </ul>"},{"location":"workloads/jobs/#handling-pod-and-container-failures","title":"Handling Pod and container failures","text":"<ul> <li><code>spec.backoffLimit</code>: Number of allowed failures before Job is marked as failed.</li> <li><code>spec.activeDeadlineSeconds</code>: Time in seconds that a Job is allowed to run.</li> </ul>"},{"location":"workloads/jobs/#pod-backoff-failure-policy","title":"Pod backoff failure policy","text":"<ul> <li>Exponential backoff for restarting failed Pods. Controlled by spec.backoffLimit and spec.activeDeadlineSeconds.</li> </ul>"},{"location":"workloads/jobs/#backoff-limit-per-index","title":"Backoff limit per index","text":"<ul> <li>In Indexed Jobs, each index has its own backoff limit and retry mechanism.</li> </ul>"},{"location":"workloads/jobs/#pod-failure-policy","title":"Pod failure policy","text":"<ul> <li>No explicit failure policy, but can be managed using <code>spec.backoffLimit</code> and <code>spec.activeDeadlineSeconds</code>.</li> </ul>"},{"location":"workloads/jobs/#job-termination-and-cleanup","title":"Job termination and cleanup","text":"<ul> <li>Deleting a Job will also delete all its Pods.</li> <li>Use <code>kubectl delete job --selector=</code> to delete multiple Jobs.</li> </ul>"},{"location":"workloads/jobs/#clean-up-finished-jobs-automatically","title":"Clean up finished jobs automatically","text":"<ul> <li><code>.spec.ttlSecondsAfterFinished</code>: Time-to-live in seconds after Job completion, after which the Job and its Pods are deleted.</li> </ul>"},{"location":"workloads/jobs/#ttl-mechanism-for-finished-jobs","title":"TTL mechanism for finished Jobs","text":"<ul> <li>TTL controller in Kubernetes takes care of this.</li> <li>Only works if the feature gate <code>TTLAfterFinished</code> is enabled.</li> </ul>"},{"location":"workloads/jobs/#job-patterns","title":"Job patterns","text":"<ul> <li>One-off Jobs: Run once and terminate.</li> <li>CronJobs: Scheduled Jobs, defined using cron syntax.</li> </ul>"},{"location":"workloads/jobs/#ttl-after-finished-controller","title":"TTL-After-Finished Controller","text":"<ul> <li>Provides a TTL (time to live) mechanism to limit the lifetime of Job objects that have finished execution.</li> </ul>"},{"location":"workloads/jobs/#cleanup-for-finished-jobs","title":"Cleanup for Finished Jobs","text":"<ul> <li>Supported only for Jobs.</li> <li>You can specify <code>.spec.ttlSecondsAfterFinished</code> field to clean up finished Jobs automatically.</li> <li>The timer starts once the Job status changes to Complete or Failed.</li> <li>After TTL expires, the Job becomes eligible for cascading removal, including its dependent objects.</li> <li>Kubernetes honors object lifecycle guarantees, such as waiting for finalizers.</li> </ul>"},{"location":"workloads/jobs/#setting-ttl","title":"Setting TTL","text":"<ul> <li>You can set the TTL seconds at any time.</li> <li>Can be specified in the Job manifest.</li> <li>Can be manually set for existing, already finished Jobs.</li> <li>Can use a mutating admission webhook to set this field dynamically at Job creation time or after the Job has finished.</li> <li>You can write your own controller to manage the cleanup TTL for Jobs based on selectors.</li> </ul>"},{"location":"workloads/jobs/#caveats","title":"Caveats","text":"<ul> <li>Updating TTL for Finished Jobs: You can modify the TTL period even after the job is created or has finished. However, retention is not guaranteed if you extend the TTL after it has already expired.</li> <li>Time Skew: The feature is sensitive to time skew in your cluster, which may cause the control plane to clean up Job objects at the wrong time.</li> </ul>"},{"location":"workloads/jobs/#cronjobs","title":"Cronjobs","text":"<ul> <li>CronJobs are used for performing regular scheduled actions like backups, report generation, etc.</li> </ul>"},{"location":"workloads/jobs/#schedule-syntax","title":"Schedule Syntax","text":"<ul> <li>Uses Cron syntax for scheduling.</li> <li>Extended \"Vixie cron\" step values are supported.</li> <li>Macros like <code>@monthly</code>, <code>@weekly</code>, etc., can also be used.</li> </ul>"},{"location":"workloads/jobs/#job-template","title":"Job Template","text":"<ul> <li>Defines a template for the Jobs that the CronJob creates.</li> <li>Same schema as a Job but nested and without <code>apiVersion</code> or <code>kind</code>.</li> </ul>"},{"location":"workloads/jobs/#deadline-for-delayed-job-start","title":"Deadline for Delayed Job Start","text":"<ul> <li>Optional <code>.spec.startingDeadlineSeconds</code> field.</li> <li>Defines a deadline for starting the Job if it misses its scheduled time.</li> </ul>"},{"location":"workloads/jobs/#concurrency-policy","title":"Concurrency Policy","text":"<ul> <li>Optional <code>.spec.concurrencyPolicy</code> field.</li> <li>Options: <code>Allow</code> (default), <code>Forbid</code>, <code>Replace</code>.</li> </ul>"},{"location":"workloads/jobs/#schedule-suspension","title":"Schedule Suspension","text":"<ul> <li>Optional <code>.spec.suspend</code> field to suspend execution of Jobs.</li> </ul>"},{"location":"workloads/jobs/#jobs-history-limits","title":"Jobs History Limits","text":"<ul> <li><code>.spec.successfulJobsHistoryLimit</code> and <code>.spec.failedJobsHistoryLimit</code> fields are optional.</li> </ul>"},{"location":"workloads/jobs/#time-zones","title":"Time Zones","text":"<ul> <li>Time zones can be specified using <code>.spec.timeZone</code>.</li> </ul>"},{"location":"workloads/jobs/#job-creation","title":"Job Creation","text":"<ul> <li>A CronJob creates a Job object approximately once per execution time of its schedule.</li> </ul>"},{"location":"workloads/replicaset/","title":"ReplicaSets","text":"<p>A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.  </p>"},{"location":"workloads/replicaset/#what-is-a-replicaset","title":"What is a ReplicaSet?","text":"<p>A ReplicaSet is a Kubernetes controller that ensures a specified number of pod replicas are running at any given time.</p>"},{"location":"workloads/replicaset/#purpose","title":"Purpose","text":"<p>It is designed to maintain high availability and fault tolerance for pods.</p>"},{"location":"workloads/replicaset/#relationship-with-pods","title":"Relationship with Pods","text":"<p>A ReplicaSet creates and deletes pods as needed to meet the desired replica count.</p>"},{"location":"workloads/replicaset/#labels-and-selectors","title":"Labels and Selectors","text":"<p>ReplicaSets use labels and selectors to identify which pods to manage.</p>"},{"location":"workloads/replicaset/#scaling","title":"Scaling","text":"<p>You can manually scale a ReplicaSet or use it with an autoscaler.</p>"},{"location":"workloads/replicaset/#yaml-configuration","title":"YAML Configuration","text":"<p>A ReplicaSet is defined in a YAML file, specifying the desired number of replicas and the pod template.</p>"},{"location":"workloads/replicaset/#rolling-updates-and-rollbacks","title":"Rolling Updates and Rollbacks","text":"<p>While ReplicaSets themselves don't support rolling updates, they are often used with Deployments that do.</p>"},{"location":"workloads/replicaset/#ownership","title":"Ownership","text":"<p>A ReplicaSet is considered the \"owner\" of the pods it manages, and this ownership info is stored in the pod's metadata.</p>"},{"location":"workloads/replicaset/#manual-intervention","title":"Manual Intervention","text":"<p>It's possible to manually delete pods managed by a ReplicaSet, but it's generally not recommended unless you know what you're doing.</p>"},{"location":"workloads/replicaset/#limitations","title":"Limitations","text":"<p>ReplicaSets do not support pod versioning, unlike Deployments.</p>"},{"location":"workloads/replicaset/#use-cases","title":"Use Cases","text":"<p>Ideal for stateless applications where pods are interchangeable.</p>"},{"location":"workloads/replicaset/#best-practices","title":"Best Practices","text":"<p>It's generally better to use Deployments, which use ReplicaSets under the hood but offer more features like rolling updates.</p>"},{"location":"workloads/replicationcontroller/","title":"ReplicationController","text":""},{"location":"workloads/replicationcontroller/#purpose-and-functionality","title":"Purpose and Functionality","text":"<ul> <li>Ensures a specified number of pod replicas are running at any given time.</li> <li>Automatically replaces pods that fail, are deleted, or are terminated.</li> </ul>"},{"location":"workloads/replicationcontroller/#how-it-works","title":"How it Works","text":"<ul> <li>If there are too many pods, it terminates the extra ones.</li> <li>If there are too few, it starts more.</li> </ul>"},{"location":"workloads/replicationcontroller/#abbreviation","title":"Abbreviation","text":"<ul> <li>Often abbreviated to <code>rc</code> in discussions and kubectl commands.</li> </ul>"},{"location":"workloads/replicationcontroller/#use-cases","title":"Use Cases","text":"<ul> <li>Can run one instance of a Pod indefinitely.</li> <li>Can run several identical replicas of a replicated service, like web servers.</li> </ul>"},{"location":"workloads/replicationcontroller/#configuration-example","title":"Configuration Example","text":"<ul> <li>YAML configuration specifies the number of <code>replicas</code>, <code>selector</code>, and pod <code>template</code>.</li> </ul>"},{"location":"workloads/replicationcontroller/#commands","title":"Commands","text":"<ul> <li><code>kubectl apply -f &lt;config-file&gt;</code> to apply the configuration.</li> <li><code>kubectl describe replicationcontrollers/rc-name</code> to check the status.</li> </ul>"},{"location":"workloads/replicationcontroller/#pod-template","title":"Pod Template","text":"<ul> <li><code>.spec.template</code> is a pod template with the same schema as a Pod.</li> </ul>"},{"location":"workloads/replicationcontroller/#labels-and-selectors","title":"Labels and Selectors","text":"<ul> <li><code>.spec.selector</code> is a label selector that manages pods with matching labels.</li> </ul>"},{"location":"workloads/replicationcontroller/#scaling","title":"Scaling","text":"<ul> <li><code>.spec.replicas</code> specifies the number of pods that should run concurrently.</li> </ul>"},{"location":"workloads/replicationcontroller/#deletion","title":"Deletion","text":"<ul> <li>kubectl delete scales the ReplicationController to zero and waits for pod deletion.</li> </ul>"},{"location":"workloads/replicationcontroller/#alternatives","title":"Alternatives","text":"<ul> <li>ReplicaSet and Deployment are the next-generation alternatives.</li> </ul>"},{"location":"workloads/replicationcontroller/#daemonset-and-job","title":"DaemonSet and Job","text":"<ul> <li>For machine-level functions, use DaemonSet.</li> <li>For pods expected to terminate on their own, use Job.</li> </ul>"},{"location":"workloads/resources/","title":"Resources","text":""},{"location":"workloads/resources/#introduction","title":"Introduction","text":"<ul> <li>Kubernetes offers built-in APIs for declarative management of workloads.</li> <li>Workloads run as containers inside Pods.</li> <li>Kubernetes control plane manages Pods based on workload object specifications.</li> </ul>"},{"location":"workloads/resources/#deployment-and-replicaset","title":"Deployment and ReplicaSet","text":"<ul> <li>Most common way to run applications on the cluster.</li> <li>Good for managing stateless application workloads.</li> <li>Pods in a Deployment are interchangeable.</li> <li>Replaces the legacy <code>ReplicationController</code> API.</li> </ul>"},{"location":"workloads/resources/#statefulset","title":"StatefulSet","text":"<ul> <li>Manages one or more Pods running the same application code.</li> <li>Pods have a distinct identity, unlike Deployments.</li> <li>Commonly used to link Pods with their persistent storage via <code>PersistentVolume</code>.</li> <li>Replacement Pods connect to the same <code>PersistentVolume</code>.</li> </ul>"},{"location":"workloads/resources/#daemonset","title":"DaemonSet","text":"<ul> <li>Defines Pods that provide node-specific facilities.</li> <li>Useful for running drivers or other node-level services.</li> <li>Can run across every node or a subset of nodes in the cluster.</li> </ul>"},{"location":"workloads/resources/#job-and-cronjob","title":"Job and CronJob","text":"<ul> <li>Job represents a one-off task that runs to completion.</li> <li>CronJob represents tasks that repeat according to a schedule.</li> </ul>"}]}