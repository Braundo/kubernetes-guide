{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"","text":"<p>Welcome to the Kubernetes Guide, a quick and easy-to-digest summary of core Kubernetes concepts intended to help get you from zero to proficient!</p> <p>One thing to note about text formatting in this guide: you'll notice some terms start with a capital letter (i.e. Service, Pod, etc.). This is intentional and an attempt to adhere to standard formatting as laid out in the official Kubernetes documentation. Kubernetes API objects (like the ones just mentioned) should start with a capital letter.</p> <p>Legal disclaimer:  \"Kubernetes\", \"K8s\", and the Kubernetes logo are trademarks or registered trademarks of the Linux Foundation.  Neither myself nor this site are officially associated with the Linux Foundation.</p> <p></p> <p>k8s.guide is a free Kubernetes learning site I created and maintain in my spare time.</p> <p>If it\u2019s helped you, consider supporting it to help offset hosting and maintenance costs. Thanks for your support!</p> <p> </p> <p></p>"},{"location":"about/","title":"About","text":"<p>My name is Aaron Braundmeier and I've been working in the tech industry for over a decade at companies such as Mastercard, VMware, Broadcom and CVS. I've long been a Kubernetes fan and had the privilege of working hands-on in that space for many years. I am currently the Director of Kubernetes &amp; Container Platforms at Mastercard where I have the privilege of helping to shape the landscape of Kubernetes usage there. Please note that all opinions and content on this site belong to me and do not reflect the opinions, plans, or designs of any of my current or former employers.</p> <p></p> <p>I'm currently working on attaining Kubestronaut status:</p> <ul> <li> Certified Kubernetes Administrator (CKA)</li> <li> Kubernetes Certified Security Associate (KCSA)</li> <li> Kubernetes and Cloud Native Associate (KCNA)</li> <li> Certified Kubernetes Security Specialist (CKS)</li> <li> Certified Kubernetes Application Developer (CKAD)</li> </ul> <p> If you're interested in connecting, I can be reached in the following ways:</p> <p> aaron@braundmeier.com</p> <p> LinkedIn</p> <p> Signal</p> <p></p> Certified Kubernetes Administrator Issuer: The Linux Foundation Kubernetes and Cloud Security Associate Issuer: The Linux Foundation Kubernetes and Cloud Native Associate Issuer: The Linux Foundation AWS Certified Cloud Practitioner Issuer: The Linux Foundation Certified Argo Project Associate Issuer: The Linux Foundation Certified GitOps Associate Issuer: The Linux Foundation"},{"location":"audit-logging/","title":"Audit & Logging","text":"<p>In Kubernetes, there are two stories being told at the same time:</p> <ol> <li>The Detective Story (Audit Logs): \"Who changed the system configuration?\"</li> <li>The Operator Story (App Logs): \"Why is my application crashing?\"</li> </ol> <p>If you don't capture these stories immediately, they vanish. Kubernetes logs are ephemeral; when a Pod dies, its logs die with it.</p>"},{"location":"audit-logging/#1-audit-logs-the-black-box-recorder","title":"1. Audit Logs (The \"Black Box\" Recorder)","text":"<p>Audit logs record every single request sent to the Kubernetes API Server. They answer the questions: Who, What, Where, and When.</p> <ul> <li>Who: User <code>alice</code></li> <li>What: Tried to <code>delete</code></li> <li>Where: The <code>secret</code> named <code>db-pass</code></li> <li>When: At <code>12:05 PM</code></li> <li>Result: <code>403 Forbidden</code></li> </ul>"},{"location":"audit-logging/#the-4-audit-levels","title":"The 4 Audit Levels","text":"<p>You must configure how much data you want. This is a trade-off between \"Visibility\" and \"Disk Space.\"</p> Level Description Use Case <code>None</code> Don't log anything. Frequent, noisy events (like <code>kube-proxy</code> watching endpoints). <code>Metadata</code> Log the User, Timestamp, Resource, and Verb. No payloads. Standard Production (Low Cost, High Value). <code>Request</code> Log metadata + the request body sent by the user. Debugging \"Why did this object change?\" <code>RequestResponse</code> Log everything + the server's response body. High Security / Debugging. (Generates massive data). <p>Security Warning: Secrets in Logs</p> <p>Be extremely careful using <code>Request</code> or <code>RequestResponse</code> levels on <code>Secret</code> or <code>ConfigMap</code> resources. You might accidentally write your database passwords into your plain-text audit log files!</p>"},{"location":"audit-logging/#configuration-the-policy-file","title":"Configuration (The Policy File)","text":"<p>You pass a policy file to the API Server to define rules.</p> <pre><code># audit-policy.yaml\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n  # 1. Don't log noisy system calls\n  - level: None\n    users: [\"system:kube-proxy\"]\n    verbs: [\"watch\"]\n\n  # 2. Log full request body for critical changes (Pod modifications)\n  - level: Request\n    resources:\n    - group: \"\"\n      resources: [\"pods\"]\n\n  # 3. Default: Log metadata only for everything else\n  - level: Metadata\n</code></pre>"},{"location":"audit-logging/#2-application-logging-the-stream","title":"2. Application Logging (The \"Stream\")","text":"<p>Kubernetes does not provide a native storage solution for logs. It assumes your application writes to Standard Output (stdout) and Standard Error (stderr).</p> <p>The container runtime (containerd) captures these streams and writes them to a file on the Node (usually <code>/var/log/containers/*.log</code>).</p>"},{"location":"audit-logging/#the-logging-pipeline","title":"The Logging Pipeline","text":"<p>Since logs on the Node are deleted when the Pod is deleted, you need a Cluster-Level Logging Stack to ship them to safety.</p> <pre><code>graph LR\n    subgraph \"Worker Node\"\n        Pod[\"App Pod\"] --&gt;|stdout| File[\"/var/log/containers/...\"]\n        Agent[\"Log Agent&lt;br/&gt;(Fluent Bit / Promtail)\"] -.-&gt;|Reads| File\n    end\n\n    Agent --&gt;|Push| Backend[\"Log Storage&lt;br/&gt;(Loki / Elasticsearch)\"]\n    Backend --&gt;|Query| UI[\"Dashboard&lt;br/&gt;(Grafana / Kibana)\"]</code></pre>"},{"location":"audit-logging/#the-daemonset-pattern","title":"The \"DaemonSet\" Pattern","text":"<p>The most common architecture is running a Logging Agent (like Fluent Bit or Promtail) as a DaemonSet.</p> <ol> <li>One agent runs on every Node.</li> <li>It mounts <code>/var/log/containers</code> as a read-only volume.</li> <li>It tails every log file, adds metadata (Pod Name, Namespace), and pushes it to the backend.</li> </ol>"},{"location":"audit-logging/#3-best-practices","title":"3. Best Practices","text":""},{"location":"audit-logging/#security","title":"Security","text":"<ol> <li>Alert on <code>403 Forbidden</code>: If your Audit Logs show a user trying to read <code>secrets</code> and getting denied 10 times in a minute, you are under attack. Alert on this pattern.</li> <li>Separate Retention: Audit logs are legal documents. Keep them in a separate bucket (e.g., S3 Glacier) for 1 year, even if you only keep App logs for 7 days.</li> </ol>"},{"location":"audit-logging/#operations","title":"Operations","text":"<ol> <li>JSON Logging: Force your developers to log in JSON format.<ul> <li>Bad: <code>2023-10-01 Error: DB failed</code> (Hard to parse).</li> <li>Good: <code>{\"level\": \"error\", \"msg\": \"DB failed\", \"service\": \"payment\"}</code> (Easy to filter).</li> </ul> </li> <li>Don't Log to Files Inside Containers: If your legacy app writes to <code>/app/logs/server.log</code>, standard Kubernetes logging will not catch it. You must use a \"Sidecar\" container to <code>tail -f</code> that file to stdout, or configure the app to write to stdout directly.</li> </ol>"},{"location":"audit-logging/#summary","title":"Summary","text":"<ul> <li>Audit Logs track API access (\"Who did it?\"). They are configured via a Policy File on the control plane.</li> <li>App Logs track application health. They rely on the stdout/stderr stream.</li> <li>Persistence: Logs are ephemeral. You must use a collection agent (DaemonSet) to ship them to a central store (Loki/Elastic) or they will be lost on Pod restart.</li> <li>Golden Rule: Avoid <code>RequestResponse</code> logging for Secrets to prevent credential leaks.</li> </ul>"},{"location":"certification-preparation/","title":"Kubernetes Certification Preparation","text":"<p>The CNCF offers several certifications to validate your Kubernetes knowledge. This section helps you prepare for the three core exams:</p> <ul> <li>Certified Kubernetes Administrator (CKA)</li> <li>Certified Kubernetes Application Developer (CKAD)</li> <li>Certified Kubernetes Security Specialist (CKS)</li> </ul> <p>Each guide includes:</p> <ul> <li>Core topics you need to master</li> <li>Trusted resources and courses</li> <li>Practical exam tips and environment setup</li> </ul>"},{"location":"certification-preparation/#exam-overview","title":"Exam Overview","text":"Cert Duration Format Focus Area CKA 2 hours Hands-on lab Cluster operations, admin CKAD 2 hours Hands-on lab App design, deployment CKS 2 hours Hands-on lab Security and hardening"},{"location":"certification-preparation/#general-advice","title":"General Advice","text":"<ul> <li>Practice in a real cluster  -  don\u2019t rely only on theory</li> <li>Learn to navigate <code>kubectl</code> quickly  -  alias everything</li> <li>Master <code>vim</code>, <code>tmux</code>, and <code>kubectl explain</code></li> <li>Use tab-complete and <code>kubectl -h</code> constantly</li> <li>Use <code>--dry-run=client -o yaml</code> for rapid manifest generation</li> </ul>"},{"location":"certification-preparation/#resources","title":"Resources","text":""},{"location":"certification-preparation/#books","title":"Books","text":"Book Title Link Kubernetes Up &amp; Running Kubernetes Up &amp; Running The Kubernetes Book The Kubernetes Book Certified Kubernetes Administrator Study Guide Certified Kubernetes Administrator Study Guide Quick Start Kubernetes Quick Start Kubernetes Networking &amp; Kubernetes Networking &amp; Kubernetes Kubernetes Best Practices Kubernetes Best Practices The Book of Kubernetes The Book of Kubernetes"},{"location":"certification-preparation/#documentation","title":"Documentation","text":"Description Link Official Kubernetes documentation Kubernetes Documentation"},{"location":"certification-preparation/#online-courses","title":"Online Courses","text":"Course Link CKA Course on Udemy CKA Course on Udemy CKAD Design &amp; Build on Pluralsight CKAD Design &amp; Build on Pluralsight"},{"location":"certification-preparation/#practice-labs","title":"Practice Labs","text":"Description Link Katacoda Katacoda Play with Kubernetes Play with Kubernetes Killer Shell killer.sh <p>Note: You\u2019ll have access to kubernetes.io/docs and github.com/kubernetes during the exam.</p>"},{"location":"certification-preparation/#ready-to-dive-in","title":"Ready to Dive In?","text":"<p>Choose your path:</p> <ul> <li>CKA \u2013 Admin-focused</li> <li>CKAD \u2013 Developer-focused</li> <li>CKS \u2013 Security-focused</li> </ul>"},{"location":"cka/","title":"Certified Kubernetes Administrator (CKA)","text":"<p>The Certified Kubernetes Administrator (CKA) exam tests your ability to install, configure, and manage Kubernetes clusters in real-world scenarios. It focuses heavily on system-level operations, cluster components, and day-to-day administrator tasks.</p>"},{"location":"cka/#exam-overview","title":"Exam Overview","text":"<ul> <li>Format: Hands-on, performance-based lab</li> <li>Duration: 2 hours</li> <li>Passing score: 66%</li> <li>Price: $395 USD (includes one retake)</li> <li>Open book: Access to kubernetes.io/docs and GitHub</li> </ul>"},{"location":"cka/#domains-weights","title":"Domains &amp; Weights","text":"Domain Weight Cluster Architecture, Installation &amp; Configuration 25% Workloads &amp; Scheduling 15% Services &amp; Networking 20% Storage 10% Troubleshooting 30%"},{"location":"cka/#what-you-should-master","title":"What You Should Master","text":""},{"location":"cka/#1-cluster-architecture-setup-25","title":"1. Cluster Architecture &amp; Setup (25%)","text":"<ul> <li><code>kubeadm init</code>, <code>join</code>, <code>reset</code></li> <li>Control plane components: API server, scheduler, controller manager</li> <li>Node components: kubelet, kube-proxy, container runtime</li> <li><code>kubectl config</code> + kubeconfig structure</li> <li>Certificate management (CA, client certs)</li> <li><code>etcdctl</code> backup and restore</li> <li>Static Pods and manifests in <code>/etc/kubernetes/manifests</code></li> <li>Taints and tolerations</li> </ul>"},{"location":"cka/#2-workloads-scheduling-15","title":"2. Workloads &amp; Scheduling (15%)","text":"<ul> <li>Deployments, ReplicaSets, Jobs, CronJobs</li> <li>Labels, selectors, and affinity/anti-affinity rules</li> <li>Taints, tolerations, and node selectors</li> <li>DaemonSets</li> </ul>"},{"location":"cka/#3-services-networking-20","title":"3. Services &amp; Networking (20%)","text":"<ul> <li>ClusterIP, NodePort, LoadBalancer</li> <li>CoreDNS troubleshooting</li> <li>NetworkPolicies (basic understanding)</li> <li>Ingress (YAML-level familiarity)</li> <li>Pod-to-Pod communication</li> </ul>"},{"location":"cka/#4-storage-10","title":"4. Storage (10%)","text":"<ul> <li>Volumes and volumeMounts</li> <li>PersistentVolumes (PV) and PersistentVolumeClaims (PVC)</li> <li>StorageClasses</li> <li>AccessModes and reclaim policies</li> </ul>"},{"location":"cka/#5-troubleshooting-30","title":"5. Troubleshooting (30%)","text":"<ul> <li>Pod/container status (<code>kubectl describe</code>, logs, events)</li> <li><code>kubectl exec</code>, <code>port-forward</code></li> <li>CrashLoopBackOff, ImagePullBackOff</li> <li>Control plane failure detection (kubelet, etcd, API server)</li> <li>Networking and DNS issues</li> <li>Resource scheduling issues (taints, affinity, nodeSelector)</li> <li>CNI problems</li> </ul>"},{"location":"cka/#practice-tips","title":"Practice Tips","text":"<ul> <li>Set up a local cluster using <code>kubeadm</code> (or use labs like Killer.sh)</li> <li>Use <code>kubectl explain</code> often to understand object structure</li> <li>Use <code>kubectl -n kube-system get pods</code> to monitor system health</li> <li>Alias these commands:</li> </ul> <pre><code>alias k=kubectl\nalias kgp='kubectl get pods'\nalias kaf='kubectl apply -f'\n</code></pre> <ul> <li>Practice writing manifests quickly with:</li> </ul> <pre><code>kubectl run nginx --image=nginx --dry-run=client -o yaml\n</code></pre> <ul> <li>Use <code>kubectl edit</code> and <code>kubectl patch</code> to modify resources live</li> </ul>"},{"location":"cka/#test-environment-tips","title":"Test Environment Tips","text":"<ul> <li>Open multiple terminal tabs (one for docs, one for kubectl)</li> <li> <p>Bookmark key doc pages:</p> <ul> <li>Install tools</li> <li>Tasks \u2192 Configure Pods</li> <li>Reference</li> </ul> </li> <li> <p>Use <code>/etc/kubernetes/manifests/</code> for static Pod edits</p> </li> <li>Save <code>etcd</code> backup and restore syntax</li> </ul>"},{"location":"cka/#recommended-resources","title":"Recommended Resources","text":"<ul> <li>Kubernetes Official Docs</li> <li>Killer.sh Simulator (free with CKA)</li> <li>KodeKloud CKA Course</li> <li>Linux Foundation CKA Training</li> <li>YouTube: TechWorld with Nana \u2013 CKA Series</li> </ul>"},{"location":"cka/#summary","title":"Summary","text":"<p>The CKA exam simulates real-world cluster admin tasks. You\u2019ll be troubleshooting, configuring, deploying, and debugging in a live cluster. With good YAML speed and familiarity with <code>kubectl</code>, you\u2019ll be ready to pass with confidence.</p> <p>Start with the fundamentals. Practice under time pressure. Know where to look in the docs.</p>"},{"location":"ckad/","title":"Certified Kubernetes Application Developer (CKAD)","text":"<p>The CKAD certification tests your ability to design, build, and run applications in Kubernetes. It's focused on real-world usage of Kubernetes primitives - deployments, configs, probes, volumes, and services - from a developer's perspective.</p>"},{"location":"ckad/#exam-overview","title":"Exam Overview","text":"<ul> <li>Format: Hands-on, browser-based lab</li> <li>Duration: 2 hours</li> <li>Passing score: 66%</li> <li>Price: $395 USD (includes one retake)</li> <li>Open book: Access to kubernetes.io/docs</li> </ul>"},{"location":"ckad/#domains-weights","title":"Domains &amp; Weights","text":"Domain Weight Core Concepts 13% Configuration 18% Multi-Container Pods 10% Observability 18% Pod Design 20% Services &amp; Networking 13% State Persistence 8%"},{"location":"ckad/#what-you-should-master","title":"What You Should Master","text":""},{"location":"ckad/#1-core-concepts-13","title":"1. Core Concepts (13%)","text":"<ul> <li>Pod lifecycle and restart policies</li> <li>YAML basics: <code>kind</code>, <code>metadata</code>, <code>spec</code></li> <li><code>kubectl explain</code>, <code>run</code>, <code>logs</code>, <code>exec</code></li> </ul>"},{"location":"ckad/#2-configuration-18","title":"2. Configuration (18%)","text":"<ul> <li>ConfigMaps &amp; Secrets (env and volumes)</li> <li><code>env</code>, <code>envFrom</code>, <code>valueFrom</code></li> <li>Probes: liveness, readiness, startup</li> <li>Resource <code>requests</code> and <code>limits</code></li> <li><code>initContainers</code></li> </ul>"},{"location":"ckad/#3-pod-design-20","title":"3. Pod Design (20%)","text":"<ul> <li>Deployments, ReplicaSets, Jobs, CronJobs</li> <li>Multi-container Pods (sidecar pattern)</li> <li>Labels &amp; selectors</li> <li>Rolling updates &amp; rollbacks</li> </ul>"},{"location":"ckad/#4-multi-container-pods-10","title":"4. Multi-Container Pods (10%)","text":"<ul> <li>Sharing volumes, network namespace</li> <li>Common patterns:</li> <li>Sidecar (logging, proxy)</li> <li>Adapter (log converter, translator)</li> <li>Ambassador (external traffic entrypoint)</li> </ul>"},{"location":"ckad/#5-observability-18","title":"5. Observability (18%)","text":"<ul> <li><code>kubectl logs</code>, <code>describe</code>, <code>top</code></li> <li>Events and debugging Pods</li> <li>Container exit codes and status</li> <li>Custom probes for health checks</li> <li>Monitoring concepts (but not setup)</li> </ul>"},{"location":"ckad/#6-services-networking-13","title":"6. Services &amp; Networking (13%)","text":"<ul> <li>ClusterIP, NodePort (no LoadBalancer config needed)</li> <li>Headless Services</li> <li>DNS-based Pod discovery</li> <li>Understanding service selectors</li> </ul>"},{"location":"ckad/#7-state-persistence-8","title":"7. State Persistence (8%)","text":"<ul> <li>Volumes and volumeMounts</li> <li>PersistentVolumeClaims (PVCs)</li> <li>AccessModes: <code>ReadWriteOnce</code>, <code>ReadOnlyMany</code></li> <li>EmptyDir (for temporary scratch space)</li> </ul>"},{"location":"ckad/#practice-tips","title":"Practice Tips","text":"<ul> <li>Alias often-used commands:</li> </ul> <pre><code>alias k=kubectl\nalias kgp='kubectl get pods'\nalias kaf='kubectl apply -f'\n</code></pre> <ul> <li>Use dry-run + output:</li> </ul> <pre><code>kubectl run nginx --image=nginx --dry-run=client -o yaml\n</code></pre> <p>Practice common configs: - YAML for Pods with ConfigMap/Secret env vars - Liveness and readiness probes - Multi-container Pod with shared volume</p>"},{"location":"ckad/#test-environment-tips","title":"Test Environment Tips","text":"<ul> <li>Open docs in one tab, terminal in another</li> <li> <p>Bookmark these:</p> <ul> <li>Tasks</li> <li>kubectl Cheat Sheet</li> <li>Workloads Overview</li> </ul> </li> <li> <p>Use <code>kubectl explain</code> to recall spec fields quickly</p> </li> <li>Copy/paste manifest scaffolds from the docs to save time</li> </ul>"},{"location":"ckad/#recommended-resources","title":"Recommended Resources","text":"<ul> <li>Kubernetes Official Docs</li> <li>Killer.sh Simulator</li> <li>KodeKloud CKAD Course</li> <li>Linux Foundation CKAD Training</li> <li>YouTube: TechWorld with Nana \u2013 CKAD Series</li> </ul>"},{"location":"ckad/#summary","title":"Summary","text":"<p>The CKAD exam tests your Kubernetes fluency as a developer. You\u2019ll create and configure Pods, manage configs and secrets, debug issues, and expose applications.</p> <p>If you\u2019re confident writing manifests and using <code>kubectl</code> with speed, you\u2019re ready to pass.</p>"},{"location":"cks/","title":"Certified Kubernetes Security Specialist (CKS)","text":"<p>The CKS certification tests your ability to secure Kubernetes clusters and workloads. It\u2019s hands-on, intense, and assumes you already understand Kubernetes deeply (CKA is a prerequisite).</p>"},{"location":"cks/#exam-overview","title":"Exam Overview","text":"<ul> <li>Format: Hands-on lab with scenarios</li> <li>Duration: 2 hours</li> <li>Passing score: 67%</li> <li>Prerequisite: Active CKA certification</li> <li>Open book: Access to kubernetes.io + GitHub repos</li> </ul>"},{"location":"cks/#domains-weights","title":"Domains &amp; Weights","text":"Domain Weight Cluster Setup 10% System Hardening 15% Minimize Microservice Vulnerabilities 20% Supply Chain Security 20% Monitoring, Logging &amp; Runtime Security 25% RBAC &amp; Network Policies 10%"},{"location":"cks/#what-you-should-master","title":"What You Should Master","text":""},{"location":"cks/#1-cluster-setup-10","title":"1. Cluster Setup (10%)","text":"<ul> <li>TLS certificates &amp; CA bundles</li> <li>Encrypt secrets at rest (KMS + <code>EncryptionConfiguration</code>)</li> <li>Audit policy config and log location</li> <li>API server flags: <code>--audit-log-path</code>, <code>--enable-admission-plugins</code></li> </ul>"},{"location":"cks/#2-system-hardening-15","title":"2. System Hardening (15%)","text":"<ul> <li>Restrict host access: block <code>hostPath</code>, <code>hostNetwork</code>, <code>privileged</code></li> <li> <p>Use <code>securityContext</code>:</p> <ul> <li><code>runAsNonRoot</code>, <code>readOnlyRootFilesystem</code>, <code>allowPrivilegeEscalation: false</code></li> </ul> </li> <li> <p>Restrict capabilities (<code>capabilities.drop: [\"ALL\"]</code>)</p> </li> <li>Pod Security Admission (PSA) with restricted profile</li> <li>Runtime namespace protections (AppArmor / seccomp)</li> </ul>"},{"location":"cks/#3-minimize-microservice-vulnerabilities-20","title":"3. Minimize Microservice Vulnerabilities (20%)","text":"<ul> <li>Scan images with Trivy, Grype, or Dockle</li> <li>Sign images with cosign and verify before deployment</li> <li>Use scratch/minimal base images</li> <li>Avoid running as root in Dockerfiles</li> <li>Validate liveness/readiness probe security</li> </ul>"},{"location":"cks/#4-supply-chain-security-20","title":"4. Supply Chain Security (20%)","text":"<ul> <li>Use trusted registries and signed images</li> <li>Scan YAML manifests for insecure configurations (e.g., <code>kubesec</code>, <code>kube-score</code>)</li> <li> <p>Admission control:</p> <ul> <li>Validating/mutating webhooks</li> <li>Gatekeeper/OPA policies</li> </ul> </li> <li> <p>ImagePullPolicy: <code>Always</code></p> </li> </ul>"},{"location":"cks/#5-monitoring-logging-runtime-security-25","title":"5. Monitoring, Logging &amp; Runtime Security (25%)","text":"<ul> <li>Audit policy and log filtering</li> <li> <p>Tools:</p> <ul> <li>Falco (real-time threat detection)</li> <li>Sysdig, AuditD, or <code>ausearch</code></li> </ul> </li> <li> <p>Monitor execs, privilege escalation, network anomalies</p> </li> <li>Understand and tune Falco rules</li> </ul>"},{"location":"cks/#6-rbac-network-policies-10","title":"6. RBAC &amp; Network Policies (10%)","text":"<ul> <li>Create <code>Role</code>, <code>ClusterRole</code>, <code>RoleBinding</code>, <code>ClusterRoleBinding</code></li> <li>Apply <code>NetworkPolicy</code> to restrict Pod traffic (ingress/egress)</li> <li>Avoid <code>*</code> verbs and <code>*</code> resources in RBAC</li> <li>Restrict access by namespace and API group</li> </ul>"},{"location":"cks/#practice-tips","title":"Practice Tips","text":"<ul> <li>Practice scanning + signing images:</li> <li><code>trivy image nginx:latest</code></li> <li><code>cosign sign --key cosign.key myrepo/app:1.0</code></li> <li> <p>Create test policies for:</p> <ul> <li>PSA</li> <li>RBAC + <code>kubectl auth can-i</code></li> <li>NetworkPolicy deny-by-default rules</li> </ul> </li> <li> <p>Trigger and detect audit events</p> </li> <li>Write Falco rules for suspicious behaviors</li> </ul>"},{"location":"cks/#test-environment-tips","title":"Test Environment Tips","text":"<ul> <li> <p>Use bookmarks:</p> <ul> <li>Pod Security Standards</li> <li>Audit Logging</li> <li>Sysdig Falco</li> </ul> </li> <li> <p>Open multiple terminals: cluster work, docs lookup, test scripts</p> </li> <li>Save frequently used YAML snippets</li> </ul>"},{"location":"cks/#recommended-resources","title":"Recommended Resources","text":"<ul> <li>Kubernetes Official Docs</li> <li>Killer.sh Simulator (CKS)</li> <li>KodeKloud CKS Course</li> <li>Linux Foundation CKS Training</li> <li>Sysdig Falco + GitHub rules</li> </ul>"},{"location":"cks/#summary","title":"Summary","text":"<p>CKS is all about applying security best practices under pressure. You\u2019ll configure audit logs, write PodSecurity controls, patch RBAC, restrict networks, and scan or sign container images - all in live clusters.</p> <p>Hands-on practice is key. Read YAML fast. Think like an attacker.</p>"},{"location":"configmaps-secrets/","title":"ConfigMaps & Secrets","text":"ConfigMaps &amp; Secrets <p>Kubernetes lets you separate your app\u2019s configuration from your container images using two special resources:</p> <ul> <li>ConfigMaps for non-sensitive data (like settings, URLs, etc.)</li> <li>Secrets for sensitive data (like passwords, tokens, certificates)</li> </ul> <p>This makes your apps more secure, portable, and easier to manage.</p> ConfigMaps (Non-Sensitive Configuration) <p>A ConfigMap is a key-value store for plain-text configuration. Use it for:</p> <ul> <li>Environment settings (like <code>LOG_LEVEL</code>, <code>API_BASE_URL</code>)</li> <li>Hostnames, ports, feature flags</li> <li>Complete config files or CLI arguments</li> </ul> Example <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  LOG_LEVEL: debug\n  DB_HOST: db.default.svc.cluster.local\n</code></pre> Secrets (Sensitive Data) <p>Secrets are also key-value stores - but for private data:</p> <ul> <li>Passwords, tokens, API keys</li> <li>SSH keys or TLS certs</li> <li>Docker registry credentials</li> </ul> <p>Kubernetes encodes all Secret values in base64 (for transport, not real security).</p> Example <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-secret\ntype: Opaque\ndata:\n  DB_PASSWORD: c3VwZXJzZWNyZXQ=\n</code></pre> <p>Tip</p> <p>Decode with <code>echo c3VwZXJzZWNyZXQ= | base64 -d</code>. Use <code>stringData:</code> if you want Kubernetes to handle encoding for you.</p>"},{"location":"configmaps-secrets/#ways-to-use-configmaps-and-secrets","title":"Ways to Use ConfigMaps and Secrets","text":"<p>There are three main ways to expose values inside your Pods:</p>"},{"location":"configmaps-secrets/#1-environment-variables","title":"1. Environment Variables","text":"<p>Inject all key-value pairs from a ConfigMap or Secret:</p> <pre><code>envFrom:\n  - configMapRef:\n      name: app-config\n  - secretRef:\n      name: db-secret\n</code></pre> <p>Or reference individual keys:</p> <pre><code>env:\n  - name: DB_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: db-secret\n        key: DB_PASSWORD\n</code></pre>"},{"location":"configmaps-secrets/#2-mounted-volumes","title":"2. Mounted Volumes","text":"<p>Map each key to a file inside the container:</p> <pre><code>volumes:\n  - name: config-vol\n    configMap:\n      name: app-config\ncontainers:\n  - name: app\n    volumeMounts:\n      - name: config-vol\n        mountPath: /etc/config\n</code></pre> <p>In the container, this results in:</p> <pre><code>/etc/config/LOG_LEVEL\n/etc/config/DB_HOST\n</code></pre> <p>You can do the same for Secrets:</p> <pre><code>volumes:\n  - name: creds\n    secret:\n      secretName: db-secret\n</code></pre> <p>\u26a0\ufe0f Secrets mounted as files on disk are only base64-decoded. They are not encrypted unless you've enabled encryption at rest.</p>"},{"location":"configmaps-secrets/#3-cli-arguments-or-command-overrides","title":"3. CLI Arguments or Command Overrides","text":"<pre><code>containers:\n  - name: app\n    image: myapp\n    args:\n      - \"--log-level=$(LOG_LEVEL)\"\n    env:\n      - name: LOG_LEVEL\n        valueFrom:\n          configMapKeyRef:\n            name: app-config\n            key: LOG_LEVEL\n</code></pre> <p>Kubernetes allows you to configure runtime behavior of containers using environment variables, and to monitor their health using liveness and readiness probes. These features are essential for building reliable, configurable, and observable applications in the cluster.</p> Environment Variables <p>You can pass key-value pairs into containers using environment variables. These can be hardcoded, referenced from ConfigMaps, Secrets, or even dynamically derived from field references.</p>"},{"location":"configmaps-secrets/#static-environment-variables","title":"Static Environment Variables","text":"<pre><code>env:\n  - name: LOG_LEVEL\n    value: \"debug\"\n</code></pre>"},{"location":"configmaps-secrets/#from-configmap","title":"From ConfigMap","text":"<pre><code>envFrom:\n  - configMapRef:\n      name: app-config\n</code></pre> <p>Or individual keys:</p> <pre><code>env:\n  - name: APP_PORT\n    valueFrom:\n      configMapKeyRef:\n        name: app-config\n        key: port\n</code></pre>"},{"location":"configmaps-secrets/#from-secret","title":"From Secret","text":"<pre><code>env:\n  - name: DB_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: db-secret\n        key: password\n</code></pre>"},{"location":"configmaps-secrets/#from-pod-metadata","title":"From Pod Metadata","text":"<pre><code>env:\n  - name: POD_NAME\n    valueFrom:\n      fieldRef:\n        fieldPath: metadata.name\n</code></pre> Using ConfigMaps &amp; Secrets <p>You can mount ConfigMaps and Secrets as environment variables or files inside your Pods. This keeps your app configuration flexible and secure.</p> Best Practices <ul> <li>Never store sensitive data in ConfigMaps. Use Secrets for anything private.</li> <li>Restrict access to Secrets using RBAC.</li> <li>Avoid hardcoding values in your manifests. Reference ConfigMaps and Secrets instead.</li> <li>Use external secret managers (like AWS Secrets Manager, HashiCorp Vault) for extra-sensitive data.</li> </ul> Summary <ul> <li>ConfigMaps: For non-sensitive, environment-specific configuration.</li> <li>Secrets: For sensitive data, encoded for transport.</li> <li>Both improve security, portability, and flexibility in your Kubernetes apps.</li> </ul>"},{"location":"daemonsets/","title":"DaemonSets","text":"<p>Most Kubernetes controllers (like Deployments) care about how many replicas you have (e.g., \"I want 3 copies of my app\"). They don't care where those copies run.</p> <p>DaemonSets are different. They care about Node coverage.</p> <p>A DaemonSet ensures that a copy of your Pod is running on every single node (or a selected subset of nodes) in your cluster. If you add a new node to the cluster, the DaemonSet automatically detects it and spins up a Pod there. If you remove a node, the garbage collector cleans up the Pod.</p>"},{"location":"daemonsets/#the-facility-manager-analogy","title":"The \"Facility Manager\" Analogy","text":"<p>Think of your Kubernetes cluster like a large office building.</p> <ul> <li>Deployments are the employees. They move around, sit at different desks, and sometimes change floors. You just need 50 employees present; you don't care exactly which desk they sit at.</li> <li>DaemonSets are the infrastructure - like fire sprinklers or security cameras.<ul> <li>You need exactly one security camera per floor.</li> <li>You don't want two cameras on one floor and zero on another.</li> <li>If you build a new wing (add a new Node), the first thing you do is install a security camera (Pod) there.</li> </ul> </li> </ul>"},{"location":"daemonsets/#visualizing-daemonsets","title":"Visualizing DaemonSets","text":"<p>Here is how a DaemonSet differs from a Deployment. Notice how the DaemonSet rigidly places one Pod per Node.</p> <pre><code>graph TD\n    subgraph \"Cluster\"\n    DS[DaemonSet Controller] --&gt;|Ensures 1 per Node| P1\n    DS --&gt;|Ensures 1 per Node| P2\n    DS --&gt;|Ensures 1 per Node| P3\n\n    subgraph \"Node 1\"\n    P1[DaemonSet Pod]\n    W1[Workload Pod]\n    end\n\n    subgraph \"Node 2\"\n    P2[DaemonSet Pod]\n    W2[Workload Pod]\n    W3[Workload Pod]\n    end\n\n    subgraph \"Node 3\"\n    P3[DaemonSet Pod]\n    W4[Workload Pod]\n    end\n\n    end</code></pre>"},{"location":"daemonsets/#when-to-use-a-daemonset","title":"When to use a DaemonSet?","text":"<p>You should rarely use DaemonSets for your actual application logic. They are almost exclusively used for Cluster Services - background processes that need to run locally on every machine.</p> Use Case Example Tools Why DaemonSet? Log Collection Fluentd, Filebeat, Promtail The agent needs to read the log files stored on that specific node's disk. Monitoring Node Exporter, Datadog Agent Needs to read the CPU/Memory stats of the host machine it is running on. Networking Kube-proxy, CNI Plugins (Calico/Cilium) Needs to manipulate the iptables and network interface of that specific node. Storage CSI Nodes (Ceph, EBS drivers) Needs to mount the physical disk to the node."},{"location":"daemonsets/#deep-dive-how-scheduling-works","title":"Deep Dive: How Scheduling Works","text":"<p>Unlike Deployments, DaemonSets have a unique relationship with the Kubernetes Scheduler.</p>"},{"location":"daemonsets/#1-handling-new-nodes","title":"1. Handling New Nodes","text":"<p>The moment a new Node joins the cluster (e.g., you scale up your auto-scaling group), the DaemonSet controller sees it. It immediately schedules a Pod for that node. This is critical for things like CNI Plugins (networking) - if the CNI plugin doesn't start instantly on the new node, no other pods can run there.</p>"},{"location":"daemonsets/#2-taints-and-tolerations-critical","title":"2. Taints and Tolerations (Critical!)","text":"<p>This is the most common \"gotcha\" with DaemonSets.</p> <p>By default, the Kubernetes Control Plane nodes have a \"Taint\" (e.g., <code>NoSchedule</code>) that prevents normal apps from running on them. However, you usually do want your logs and monitoring to run on the Control Plane!</p> <p>To make this happen, DaemonSets often include \"Tolerations\" to ignore those taints.</p> <pre><code>spec:\n  template:\n    spec:\n      tolerations:\n      # Allow this pod to run on the Control Plane / Master nodes\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n      # Allow this pod to run on nodes that are technically \"not ready\" yet\n      # (Useful for networking plugins that fix the network!)\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n        effect: NoSchedule\n</code></pre>"},{"location":"daemonsets/#3-limiting-to-specific-nodes","title":"3. Limiting to Specific Nodes","text":"<p>Sometimes you don't want a Pod on every node. Maybe you only want your \"GPU Monitor\" running on nodes that actually have GPUs.</p> <p>You can use a Node Selector to filter where the DaemonSet runs.</p> <pre><code>spec:\n  template:\n    spec:\n      nodeSelector:\n        hardware: gpu  # Pod will ONLY start on nodes with this label\n</code></pre>"},{"location":"daemonsets/#updating-a-daemonset","title":"Updating a DaemonSet","text":"<p>What happens when you change the image version of a DaemonSet? You can't just kill them all at once, or you'll lose monitoring/logs for the entire cluster simultaneously.</p> <p>DaemonSets support two update strategies:</p> <ol> <li>RollingUpdate (Default &amp; Recommended):     Kubernetes kills one Pod, updates it, waits for it to be ready, and then moves to the next node. You can control the speed using <code>maxUnavailable</code> (e.g., \"only update 1 node at a time\").</li> <li>OnDelete:     The DaemonSet will NOT automatically update the Pods. It will only create the new version if you manually delete the old Pod. This is rarely used today but useful in sensitive manual operations.</li> </ol>"},{"location":"daemonsets/#example-fluentd-log-collector","title":"Example: Fluentd Log Collector","text":"<p>Here is a complete, real-world style YAML for a log collector.</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd-logger\n  namespace: kube-system # Usually runs in system namespace\nspec:\n  selector:\n    matchLabels:\n      name: fluentd-logger\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1 # Only update 1 node at a time\n  template:\n    metadata:\n      labels:\n        name: fluentd-logger\n    spec:\n      # Tolerations allow this to run on Master/Control Plane nodes\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n      containers:\n      - name: fluentd\n        image: fluent/fluentd:v1.14\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 100m\n        # Mount the host's log directory so we can read it\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n</code></pre>"},{"location":"daemonsets/#best-practices","title":"Best Practices","text":"<ul> <li>Always Set Limits: DaemonSets run on every node. If your DaemonSet has a memory leak, it will consume memory on every node, potentially crashing the entire cluster. Always set <code>resources.limits</code>.</li> <li>Use High Priority Classes: Since DaemonSets are usually system-critical (like networking or monitoring), you should give them a high <code>priorityClassName</code>. If a node runs out of space, you want Kubernetes to kill a standard web-app Pod, not your log collector.</li> <li>Don't use <code>hostPort</code> unless necessary: Beginners often use <code>hostPort</code> to expose DaemonSets. This works, but it causes port conflicts if you ever try to run something else on that port. Use a Service with <code>type: ClusterIP</code> if you just need internal communication.</li> </ul>"},{"location":"daemonsets/#summary","title":"Summary","text":"<ul> <li>DaemonSets ensure one Pod per Node.</li> <li>They are primarily used for system agents (logs, monitoring, networking).</li> <li>They handle Node Creation automatically (new node = new pod).</li> <li>You must manage Taints &amp; Tolerations if you want them to run on Control Plane nodes.</li> <li>Use NodeSelectors if you only want them on a subset of hardware (e.g., GPU nodes).</li> </ul> <p>Pro Tip</p> <p>If you notice a DaemonSet Pod isn't scheduling on a specific node, check <code>kubectl describe node &lt;node-name&gt;</code>. The node likely has a Taint that your DaemonSet is missing a Toleration for.</p>"},{"location":"helm-package-management/","title":"Helm: The Package Manager","text":"<p>Deploying a single Pod is easy. Deploying a production application - which needs a Deployment, Service, Ingress, ConfigMap, Secret, and HPA - is hard.</p> <p>Managing that same application across Dev, Staging, and Production (with different replica counts and image tags for each) is a nightmare.</p> <p>Helm solves this. It is the \"apt-get\" or \"npm\" for Kubernetes. It allows you to bundle related YAML files into a single package called a Chart.</p>"},{"location":"helm-package-management/#the-cookie-cutter-analogy","title":"The \"Cookie Cutter\" Analogy","text":"<p>Think of your Kubernetes YAML files as \"Cookies.\"</p> <ul> <li>Without Helm: You hand-craft every single cookie (YAML file). If you need 10 cookies, you write 10 files. If you want to change the flavor, you edit 10 files.</li> <li>With Helm: You create a Mold (Template).<ul> <li>You pour \"dough\" (Configuration Values) into the mold.</li> <li>Helm presses the button and generates perfect YAML files for you every time.</li> </ul> </li> </ul>"},{"location":"helm-package-management/#core-concepts","title":"Core Concepts","text":"Term Definition Chart The package itself. A directory containing templates and metadata. (The \"Mold\") Values The configuration settings. (The \"Dough\"). Defined in <code>values.yaml</code>. Release An instance of a Chart running in your cluster. (The \"Cookie\"). You can install the same chart 5 times to get 5 different releases. Repository A place to store and share Charts (like Docker Hub, but for Helm)."},{"location":"helm-package-management/#helm-architecture-v3","title":"Helm Architecture (v3+)","text":"<p>Forget Tiller. In the old days (Helm v2), there was a component called Tiller that ran inside your cluster with full admin rights. It was a massive security hole.</p> <p>Helm v3 is client-only. When you run <code>helm install</code>, the Helm binary on your laptop:</p> <ol> <li>Reads your local charts/values.</li> <li>Generates the final YAML manifests.</li> <li>Talks directly to the Kubernetes API to apply them.</li> <li>Stores the \"state\" of the release in a Kubernetes Secret (in the same namespace).</li> </ol>"},{"location":"helm-package-management/#the-directory-structure","title":"The Directory Structure","text":"<p>When you run <code>helm create my-chart</code>, you get this standard layout:</p> <pre><code>my-chart/\n  Chart.yaml          # Metadata (Name, Version, Dependencies)\n  values.yaml         # Default configuration (The \"Variables\")\n  charts/             # Sub-charts (Dependencies go here)\n  templates/          # The Logic\n    deployment.yaml   # A Deployment, but with {{ placeholders }}\n    service.yaml      # A Service, but with {{ placeholders }}\n    _helpers.tpl      # Reusable code snippets\n</code></pre>"},{"location":"helm-package-management/#the-templating-engine","title":"The Templating Engine","text":"<p>Helm uses the Go Templating language. This is what makes it powerful.</p> <p>1. The Template (<code>templates/deployment.yaml</code>) Instead of hardcoding \"nginx\", you use a variable.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nspec:\n  replicas: {{ .Values.replicaCount }}\n  containers:\n    - name: my-app\n      image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n</code></pre> <p>2. The Values (<code>values.yaml</code>) You define the defaults here.</p> <pre><code>replicaCount: 3\nimage:\n  repository: nginx\n  tag: 1.21\n</code></pre> <p>3. The Result (Rendered Manifest) Helm combines them to create valid Kubernetes YAML.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nspec:\n  replicas: 3\n  containers:\n    - name: my-app\n      image: \"nginx:1.21\"\n</code></pre>"},{"location":"helm-package-management/#daily-commands-cheat-sheet","title":"Daily Commands (Cheat Sheet)","text":""},{"location":"helm-package-management/#1-installation","title":"1. Installation","text":"<p>Install a chart from a repo (like Bitnami).</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install my-redis bitnami/redis\n</code></pre>"},{"location":"helm-package-management/#2-customizing-values","title":"2. Customizing Values","text":"<p>You almost never use the default values. You override them.</p> <pre><code># Option A: Command line flags (good for quick tests)\nhelm install my-web ./my-chart --set replicaCount=5\n\n# Option B: Custom values file (Best Practice)\nhelm install my-web ./my-chart -f values-prod.yaml\n</code></pre>"},{"location":"helm-package-management/#3-upgrading-day-2-operations","title":"3. Upgrading (Day 2 Operations)","text":"<p>Changed a value? Just run upgrade. Helm calculates the \"diff\" and patches the resources.</p> <pre><code>helm upgrade my-web ./my-chart -f values-prod.yaml\n</code></pre>"},{"location":"helm-package-management/#4-rollbacks-the-undo-button","title":"4. Rollbacks (The \"Undo\" Button)","text":"<p>Did your upgrade break production? Helm keeps a history of every release.</p> <pre><code>helm history my-web\n# REVISION    UPDATED     STATUS      CHART\n# 1           ...         SUPERSEDED  my-chart-1.0\n# 2           ...         DEPLOYED    my-chart-1.1\n\n# Rollback to revision 1 immediately\nhelm rollback my-web 1\n</code></pre>"},{"location":"helm-package-management/#managing-dependencies","title":"Managing Dependencies","text":"<p>In Helm v3, you declare dependencies in <code>Chart.yaml</code> (not <code>requirements.yaml</code>).</p> <pre><code># Chart.yaml\ndependencies:\n  - name: postgresql\n    version: 10.x.x\n    repository: https://charts.bitnami.com/bitnami\n    condition: postgresql.enabled\n</code></pre> <p>Then run:</p> <pre><code>helm dependency build\n</code></pre> <p>This downloads the postgres chart into your <code>charts/</code> folder automatically.</p>"},{"location":"helm-package-management/#summary","title":"Summary","text":"<ul> <li>Helm is the standard for packaging Kubernetes apps.</li> <li>It separates Configuration (<code>values.yaml</code>) from Code (<code>templates/</code>).</li> <li>It handles Dependencies (installing a database alongside your app).</li> <li>It provides Revision History and Rollbacks out of the box.</li> <li>Pro Tip: Always use <code>--dry-run --debug</code> before installing a complex chart to see exactly what YAML will be generated without actually applying it.</li> </ul>"},{"location":"image-scan-sign/","title":"Image Scanning","text":"<p>In Kubernetes, your security is only as strong as the software you run.</p> <p>If you deploy an image containing a known vulnerability (like Log4j) or a malicious backdoor, all your firewalls and RBAC rules are useless. The attacker is already inside the house.</p> <p>This domain is often called Supply Chain Security. It focuses on two questions:</p> <ol> <li>Scanning: \"Is this code safe?\" (Are there known vulnerabilities?)</li> <li>Signing: \"Is this code authentic?\" (Did my team build it, or did a hacker swap it?)</li> </ol>"},{"location":"image-scan-sign/#the-food-safety-analogy","title":"The \"Food Safety\" Analogy","text":"<p>Think of your container image like a sealed jar of food.</p> <ol> <li>Scanning (The Lab Test): You open the jar in a lab to check for bacteria (Vulnerabilities/CVEs). You want to know if the ingredients are rotten.</li> <li>Signing (The Tamper Seal): You put a holographic seal on the jar before it leaves the factory. If the seal is broken or missing when it arrives at the store (Cluster), you refuse to put it on the shelf (Deploy it).</li> </ol>"},{"location":"image-scan-sign/#1-image-scanning-checking-for-rot","title":"1. Image Scanning (Checking for Rot)","text":"<p>Scanning tools look inside your container layers for outdated packages (e.g., an old version of <code>openssl</code> or <code>glibc</code>) that have known security flaws (CVEs).</p>"},{"location":"image-scan-sign/#the-time-bomb-problem","title":"The \"Time Bomb\" Problem","text":"<p>A common mistake is scanning only during the Build phase.</p> <ul> <li>Day 1: You build an image. Scan says \"0 Vulnerabilities.\" You deploy it.</li> <li>Day 10: A new vulnerability is discovered in that image.</li> <li>Result: You are running a vulnerable image, but your CI pipeline is green.</li> </ul> <p>Solution: You need Continuous Scanning. Your registry (Harbor, ECR, Quay) or a cluster-operator (Starboard/Trivy Operator) should re-scan running images daily.</p>"},{"location":"image-scan-sign/#tools-of-the-trade","title":"Tools of the Trade","text":"Tool Focus Best For Trivy OS + App Dependencies The gold standard. Fast, CLI-based, scans everything. Grype OS + App Dependencies Excellent integration with Syft (SBOM generation). Clair OS Packages Often built into registries (like Quay)."},{"location":"image-scan-sign/#remediation-the-distroless-strategy","title":"Remediation: The \"Distroless\" Strategy","text":"<p>Scanning will find thousands of vulnerabilities in the Linux OS layer (Debian/Ubuntu/Alpine). The best fix? Remove the OS.</p> <p>Use Distroless images. They contain only your application and its runtime (e.g., Python/Java). They have no shell, no package manager, and no extensive OS libraries.</p> <ul> <li>Standard Node image: \\~600 vulnerabilities.</li> <li>Distroless Node image: \\~5 vulnerabilities.</li> </ul>"},{"location":"image-scan-sign/#2-image-signing-the-tamper-seal","title":"2. Image Signing (The Tamper Seal)","text":"<p>Scanning proves the code was safe when built. Signing proves the code hasn't changed since then.</p> <p>If a hacker compromises your Docker Hub account and pushes a malicious image tagged <code>v1.0</code>, your cluster will pull it without complaint. Image Signing prevents this.</p>"},{"location":"image-scan-sign/#the-modern-standard-cosign-sigstore","title":"The Modern Standard: Cosign &amp; Sigstore","text":"<p>Historically, signing (Docker Content Trust) was painful to manage (managing private keys is hard). Today, we use Sigstore/Cosign.</p> <p>Keyless Signing is the magic feature. Instead of managing a private key file, it uses your OIDC Identity (like your GitHub or Google login) to sign the image temporarily.</p> <pre><code># 1. Sign the image (opens browser to login via OIDC)\ncosign sign --keyless ghcr.io/my-user/my-image:v1\n\n# 2. Verify it\ncosign verify --keyless --certificate-identity=my-email@gmail.com ...\n</code></pre>"},{"location":"image-scan-sign/#3-sbom-software-bill-of-materials","title":"3. SBOM (Software Bill of Materials)","text":"<p>An SBOM is a list of ingredients. It doesn't say if the ingredients are bad; it just lists them. \"This image contains Log4j v2.14, OpenSSL v1.1, and React v16.\"</p> <p>Why do you need it? When the next massive zero-day hits (like Log4j), you don't want to scan 5,000 images. You want to query your database of SBOMs: \"Show me every app running Log4j v2.14.\"</p> <p>Tools: <code>Syft</code> (generates SBOMs) and <code>Grype</code> (scans them).</p>"},{"location":"image-scan-sign/#4-enforcement-the-bouncer","title":"4. Enforcement (The Bouncer)","text":"<p>Scanning and Signing are useless if you don't enforce them. You need an Admission Controller to block bad images at the door.</p> <pre><code>graph TD\n    Dev[\"Developer\"] --&gt;|kubectl apply| API[\"API Server\"]\n    API --&gt;|ValidatingWebhook| Policy[\"Policy Engine&lt;br/&gt;(Kyverno / Gatekeeper)\"]\n\n    Policy --&gt;|Check Registry| Reg[\"Container Registry\"]\n    Reg -- \"Scan: 5 Critical Vulns\" --&gt; Block1[\"Deny: Image Vulnerable\"]\n    Reg -- \"Signature: Missing\" --&gt; Block2[\"Deny: Unsigned Image\"]\n    Reg -- \"Scan: Clean &amp; Signed\" --&gt; Allow[\"Allow Deployment\"]</code></pre>"},{"location":"image-scan-sign/#example-kyverno-policy","title":"Example: Kyverno Policy","text":"<p>This policy blocks any image that hasn't been signed by your specific public key.</p> <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: check-image-signature\nspec:\n  validationFailureAction: Enforce\n  rules:\n    - name: verify-signature\n      match:\n        resources:\n          kinds:\n            - Pod\n      verifyImages:\n      - imageReferences:\n        - \"ghcr.io/my-company/*\"\n        attestors:\n        - entries:\n          - keys:\n              publicKeys: |-\n                -----BEGIN PUBLIC KEY-----\n                ... (Your Cosign Public Key) ...\n</code></pre>"},{"location":"image-scan-sign/#summary","title":"Summary","text":"<ol> <li>Scan Early, Scan Often: Scan in CI, but also scan continuously in the registry.</li> <li>Use Distroless: Reducing the attack surface is better than patching it.</li> <li>Sign with Cosign: Ensure trusted provenance.</li> <li>Enforce with Admission: Use Kyverno or Gatekeeper to block unsigned or vulnerable images from ever starting.</li> <li>Generate SBOMs: Know what is running in your cluster so you can react to zero-days instantly.</li> </ol>"},{"location":"ingress/","title":"Ingress","text":"<p>Up until now, we\u2019ve looked at how to network Pods inside the cluster (ClusterIP) and crude ways to get traffic in (NodePort, LoadBalancer).</p> <p>But what if you have a complex application? What if you want <code>my-app.com</code> to go to your frontend, and <code>my-app.com/api</code> to go to your backend? Do you need a separate expensive cloud LoadBalancer for every single service?</p> <p>No. You need an Ingress.</p> <p>Ingress is the smart receptionist for your cluster. It sits at the edge, accepts incoming HTTP/S traffic, and routes it to the correct internal backend Services based on rules you define (like hostnames or URL paths).</p>"},{"location":"ingress/#the-hotel-concierge-analogy","title":"The \"Hotel Concierge\" Analogy","text":"<p>Think of your Kubernetes cluster as a large hotel.</p> <ol> <li>Services (ClusterIP) are the actual rooms where people stay.</li> <li>NodePort/LoadBalancer is like opening a side door directly from the street into a specific room. It works, but it\u2019s not secure or scalable if you have 500 rooms.</li> <li>Ingress is the hotel concierge standing in the main lobby.<ul> <li>Guests arrive at the main entrance (a single public IP).</li> <li>They approach the concierge and say, \"I'm here for the Smith convention\" (hostname) or \"I need the restaurant\" (URL path).</li> <li>The concierge looks at their list of rules and directs the guest to the correct room number (internal Service).</li> </ul> </li> </ol> <p>Ingress operates at Layer 7 (Application Layer) of the OSI model, meaning it understands HTTP. It can look at headers, paths, and hostnames to make smart routing decisions.</p>"},{"location":"ingress/#crucial-concept-resource-vs-controller","title":"Crucial Concept: Resource vs. Controller","text":"<p>This is the #1 point of confusion for Kubernetes beginners, so let\u2019s get it out of the way first. Ingress is a two-part system.</p>"},{"location":"ingress/#1-the-ingress-resource-the-rules","title":"1. The Ingress Resource (The \"Rules\")","text":"<p>This is a YAML file that you write. It is a standard Kubernetes object that defines the routing rules.</p> <ul> <li>\"If traffic comes for <code>api.example.com</code>, send it to the <code>backend-service</code> on port 8080.\"</li> </ul> <p>If you only create this resource, nothing will happen. It's just a piece of paper with rules on it.</p>"},{"location":"ingress/#2-the-ingress-controller-the-implementation","title":"2. The Ingress Controller (The \"Implementation\")","text":"<p>This is a piece of software (usually a Pod itself, running a reverse proxy like Nginx or Traefik) that runs in your cluster. It monitors the Kubernetes API for new Ingress Resources. When it sees one, it automatically reconfigures its own internal routing tables to make those rules reality.</p> <p>Kubernetes does not come with an Ingress Controller by default. You must choose one and install it (usually via Helm).</p> <p>Popular Controllers:</p> <ul> <li>Nginx Ingress Controller: The community standard and most common choice.</li> <li>Traefik: Popular, modern alternative.</li> <li>Cloud Provider specific: (e.g., AWS ALB Ingress Controller) which turns Ingress rules into actual AWS ALBs.</li> </ul>"},{"location":"ingress/#how-traffic-flows","title":"How traffic flows","text":"<p>Here is how traffic moves from a user's browser to your application Pod. In this example, we assume you have installed the Nginx Ingress Controller.</p> <pre><code>graph LR\n    User(User Browser) -- \"https://example.com/api\" --&gt; PublicIP(Public Cloud LB IP)\n    PublicIP --&gt; ControllerService(K8s Service: Nginx Controller LoadBalancer)\n    ControllerService --&gt; ControllerPod(Pod: Nginx Ingress Controller)\n\n    subgraph \"Inside Cluster\"\n    ControllerPod -- \"Reads Ingress Rules &amp; Routes\" --&gt; BackendService(K8s Service: Backend ClusterIP)\n    BackendService -- \"Loads balances\" --&gt; BackendPod1(App Pod 1)\n    BackendService --&gt; BackendPod2(App Pod 2)\n    end</code></pre> <ol> <li>The user hits the one public IP address exposed by your cluster's Ingress Controller.</li> <li>The request lands on the Ingress Controller Pod (e.g., Nginx).</li> <li>Nginx checks the headers: \"Oh, you want <code>example.com/api</code>? My rules say that goes to the <code>backend</code> service.\"</li> <li>Nginx forwards the traffic to the internal ClusterIP of the backend service.</li> <li>The backend service balances it to an actual application Pod.</li> </ol>"},{"location":"ingress/#common-use-cases-examples","title":"Common Use Cases &amp; Examples","text":"<p>You define Ingress rules in the <code>spec.rules</code> section of the YAML.</p>"},{"location":"ingress/#1-path-based-routing-the-fanout","title":"1. Path-Based Routing (The \"Fanout\")","text":"<p>This is the most common scenario: hosting multiple microservices under one domain name, separated by URL paths.</p> <ul> <li><code>example.com/web</code> -&gt; goes to Frontend Service</li> <li><code>example.com/api</code> -&gt; goes to Backend Service</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: simple-fanout\nspec:\n  ingressClassName: nginx # Telling the Nginx controller to handle this\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /web\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-service\n            port:\n              number: 80\n      - path: /api\n        pathType: Prefix\n        backend:\n          service:\n            name: backend-service\n            port:\n              number: 8080\n</code></pre>"},{"location":"ingress/#2-host-based-routing-virtual-hosts","title":"2. Host-Based Routing (Virtual Hosts)","text":"<p>This is used when you want different domains to route to different services within the same cluster.</p> <ul> <li><code>foo.example.com</code> -&gt; Service A</li> <li><code>bar.example.com</code> -&gt; Service B</li> </ul> <pre><code>spec:\n  rules:\n  - host: foo.example.com\n    http:\n      paths:\n      # ... points to Service A ...\n  - host: bar.example.com\n    http:\n      paths:\n      # ... points to Service B ...\n</code></pre>"},{"location":"ingress/#tlsssl-termination","title":"TLS/SSL Termination","text":"<p>One of the biggest benefits of an Ingress is centralizing your SSL certificates.</p> <p>Instead of managing certificates in every single one of your application Pods, you put the certificate in a Kubernetes Secret and tell the Ingress Controller to use it.</p> <p>The Ingress Controller handles the encrypted HTTPS connection from the user, decrypts it, and sends plain HTTP to your internal backend services. This is called \"TLS termination\" and it offloads a lot of computational work from your app.</p> <pre><code>spec:\n  tls:\n  - hosts:\n      - example.com\n    secretName: my-site-cert-secret # A secret containing tls.crt and tls.key\n  rules:\n    # ... routing rules ...\n</code></pre>"},{"location":"ingress/#a-note-on-the-future-gateway-api","title":"A Note on the Future: Gateway API","text":"<p>While learning Ingress is essential today, you should be aware that a new standard called the Gateway API is evolving to eventually replace it.</p> <p>Ingress is great for simple HTTP routing, but it struggles with advanced concepts like traffic splitting (e.g., \"send 10% of traffic to the canary version\") or non-HTTP protocols. The Gateway API is designed to handle these complex, modern networking needs.</p> <p>For now, focus on mastering Ingress, as it is still the standard for 95% of deployments.</p>"},{"location":"ingress/#summary","title":"Summary","text":"<ul> <li>Services (ClusterIP) provide stable networking inside the cluster.</li> <li>Ingress is a Layer 7 router that gets traffic from outside to the right internal Service.</li> <li>An Ingress Resource is just a list of rules (YAML).</li> <li>You must install an Ingress Controller (like Nginx) to actually make those rules work.</li> <li>Ingress is perfect for path-based routing, host-based routing, and centralizing SSL termination.</li> </ul> <p>Tip</p> <p>If your Ingress isn't working, 90% of the time the issue isn't in your application Pod logs. Check the logs of the Ingress Controller Pod (e.g., in the <code>ingress-nginx</code> namespace). It will tell you if your YAML rules made sense or if it can't find the backend service.</p>"},{"location":"init-containers/","title":"Init Containers","text":"<p>In Kubernetes, a Pod is not just a single container. It can hold multiple containers that work together.</p> <p>Init Containers are special containers that run - and complete - before your main application containers start. They are the \"prep cooks\" of Kubernetes: they chop the vegetables and prep the station so the \"head chef\" (your app) can start cooking immediately.</p>"},{"location":"init-containers/#why-use-them","title":"Why Use Them?","text":"<ul> <li>Security: Your main app image can be small and secure (e.g., <code>distroless</code>), while the Init Container contains the heavy tools (curl, git, netcat) needed for setup.</li> <li>Blocking Startup: You can force your app to wait until a Database or API is actually online.</li> <li>Sequential Setup: You can chain multiple steps: \"Clone Git Repo\" $\\rightarrow$ \"Decrypt Secrets\" $\\rightarrow$ \"Start App\".</li> </ul>"},{"location":"init-containers/#visualizing-the-flow","title":"Visualizing the Flow","text":"<p>Init Containers run sequentially. If you have three Init Containers, Number 2 will not start until Number 1 finishes successfully.</p> <pre><code>graph LR\n    subgraph \"Pod Lifecycle\"\n    Start((Pod Created)) --&gt; Init1[Init Container 1]\n    Init1 --&gt;|Success| Init2[Init Container 2]\n    Init2 --&gt;|Success| Main[Main App Container Starts]\n\n    Init1 -.-&gt;|Fail| Restart[Restart Pod]\n    Init2 -.-&gt;|Fail| Restart\n    end</code></pre>"},{"location":"init-containers/#the-shared-volume-pattern","title":"The \"Shared Volume\" Pattern","text":"<p>The most common use case for Init Containers is to fetch data (like a config file or a plugin) and hand it off to the main application.</p> <p>Since containers in a Pod share storage, you use a simple <code>emptyDir</code> volume as a \"drop box.\"</p> <ol> <li>Init Container: Mounts volume at <code>/data</code>. Downloads file. Exits.</li> <li>Main Container: Mounts volume at <code>/config</code>. Reads the file.</li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: git-sync-demo\nspec:\n  volumes:\n  - name: content\n    emptyDir: {}\n  initContainers:\n  - name: git-cloner\n    image: alpine/git\n    args:\n    - clone\n    - --single-branch\n    - --\n    - https://github.com/kubernetes/kubernetes\n    - /data\n    volumeMounts:\n    - name: content\n      mountPath: /data\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: content\n      mountPath: /usr/share/nginx/html\n</code></pre>"},{"location":"init-containers/#new-in-v129-native-sidecars","title":"New in v1.29: Native Sidecars","text":"<p>Historically, Init Containers had to die before the main app started. This made it hard to run \"Sidecars\" (helper apps like Log Shippers or Service Mesh proxies) that need to start before the app but keep running.</p> <p>As of Kubernetes v1.29, we have SidecarContainers.</p> <p>If you set <code>restartPolicy: Always</code> on an init container, Kubernetes treats it as a \"Sidecar\":</p> <ol> <li>It starts before the main app.</li> <li>It keeps running in the background.</li> <li>It does not block the main app from starting (once the sidecar is \"Ready\").</li> </ol> <pre><code>initContainers:\n  - name: my-sidecar\n    image: my-log-agent\n    restartPolicy: Always # &lt;--- The Magic Switch\n</code></pre>"},{"location":"init-containers/#best-practices","title":"Best Practices","text":"<ol> <li>Idempotency: Init containers may run multiple times (if the Pod restarts). Ensure your script handles this (e.g., \"If file exists, skip download\").</li> <li>Lightweight Images: Don't use a 1GB Ubuntu image just to run <code>sleep 5</code>. Use <code>busybox</code> or <code>alpine</code>.</li> <li>Active Deadline: If your Init Container gets stuck (e.g., trying to reach a firewall-blocked DB), your Pod will be stuck in <code>Init:0/1</code> forever. Set <code>activeDeadlineSeconds</code> on the Pod if you want it to eventually fail.</li> </ol>"},{"location":"init-containers/#summary","title":"Summary","text":"<ul> <li>Init Containers run sequentially and must complete successfully before the main app starts.</li> <li>Use them to wait for dependencies or configure data.</li> <li>Use Shared Volumes to pass data from the Init Container to the Main Container.</li> <li>New Feature: Use <code>restartPolicy: Always</code> to create Native Sidecars that start first but keep running.</li> </ul>"},{"location":"jobs-cronjobs/","title":"Jobs & Cronjobs","text":"<p>So far, we have looked at Deployments and DaemonSets. These are designed for \"long-running services\" - processes like web servers that should never stop. If they crash, we restart them immediately.</p> <p>But what about tasks that should stop? What about a database backup script, a batch image processor, or a database migration? You don't want those to restart forever if they finish successfully.</p> <p>That is where Jobs and CronJobs come in.</p>"},{"location":"jobs-cronjobs/#the-contractor-analogy","title":"The \"Contractor\" Analogy","text":"<p>Think of your Workloads like staff members:</p> <ul> <li>Deployments are your Full-Time Employees. They are at their desks 24/7. If one leaves, you immediately hire a replacement.</li> <li>Jobs are Contractors. You hire them to do one specific task (e.g., \"paint the wall\").<ul> <li>If they finish the job, they go home (Process exits with code 0).</li> <li>If they fail (e.g., the ladder breaks), they try again until they succeed or you fire them (Backoff Limit).</li> <li>Once the job is done, you don't call them back unless you have a new job.</li> </ul> </li> </ul>"},{"location":"jobs-cronjobs/#jobs-run-to-completion","title":"Jobs: Run to Completion","text":"<p>A Job creates one or more Pods and ensures that a specified number of them successfully terminate.</p>"},{"location":"jobs-cronjobs/#1-handling-failures","title":"1. Handling Failures","text":"<p>Unlike a Deployment (which uses <code>restartPolicy: Always</code>), a Job uses <code>OnFailure</code> or <code>Never</code>.</p> <ul> <li><code>OnFailure</code>: If the container crashes, the Pod stays, but Kubernetes restarts the container inside it.</li> <li><code>Never</code>: If the container crashes, Kubernetes doesn't touch it. It starts a brand new Pod to try again. (Useful if you want to inspect the logs of the failed pod separately).</li> </ul>"},{"location":"jobs-cronjobs/#2-the-backoff-limit","title":"2. The \"Backoff Limit\"","text":"<p>If your code is broken, you don't want Kubernetes to retry it infinitely in a tight loop. The <code>backoffLimit</code> controls how many times Kubernetes retries before giving up. The default is 6 retries, with an exponential delay (10s, 20s, 40s...) between attempts.</p>"},{"location":"jobs-cronjobs/#3-automatic-cleanup-ttl","title":"3. Automatic Cleanup (TTL)","text":"<p>By default, when a Job finishes, the Pods stay in your cluster with a status of <code>Completed</code>. This is so you can read the logs. However, if you run a job every minute, you will have 1,440 dead pods by the end of the day.</p> <p>Use <code>ttlSecondsAfterFinished</code> to have Kubernetes automatically delete the Job and its Pods after a set time.</p>"},{"location":"jobs-cronjobs/#robust-job-example","title":"Robust Job Example","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: image-processor\nspec:\n  # Clean up this job 100 seconds after it finishes\n  ttlSecondsAfterFinished: 100 \n  # Retry 4 times before marking the job as \"Failed\"\n  backoffLimit: 4\n  template:\n    spec:\n      containers:\n      - name: processor\n        image: my-image-processor:v1\n        command: [\"python\", \"process_images.py\"]\n      # \"Always\" is invalid for Jobs. Must be OnFailure or Never.\n      restartPolicy: OnFailure\n</code></pre>"},{"location":"jobs-cronjobs/#4-parallelism-completions","title":"4. Parallelism &amp; Completions","text":"<p>Sometimes you have a massive queue of work (e.g., 1,000 images to resize). You don't want one Pod to do it; you want 10 Pods working at once.</p> <ul> <li><code>completions</code>: \"I want this job to succeed 50 times total.\"</li> <li><code>parallelism</code>: \"Run 5 Pods at the same time.\"</li> </ul>"},{"location":"jobs-cronjobs/#cronjobs-scheduling-the-work","title":"CronJobs: Scheduling the Work","text":"<p>A CronJob is just a manager that creates Jobs on a schedule. It is exactly like the <code>cron</code> utility in Linux.</p>"},{"location":"jobs-cronjobs/#the-concurrency-problem","title":"The Concurrency Problem","text":"<p>This is the #1 issue beginners face with CronJobs.</p> <p>Imagine you have a backup job scheduled to run every 5 minutes.</p> <ul> <li>12:00: Job A starts.</li> <li>12:05: Job A is still running (it's a slow backup).</li> <li>12:05: The Schedule says it's time for Job B. What should happen?</li> </ul> <p>You control this with <code>concurrencyPolicy</code>.</p> Policy Behavior Risk <code>Allow</code> (Default) Job B starts alongside Job A. Now you have 2 backups running. High Risk: If they write to the same file, you get corruption. If the job is slow, you might eventually crash the cluster with infinite jobs. <code>Forbid</code> (Recommended) Job B is skipped. It will not run. Job A continues alone. Safe. Prevents overlap. <code>Replace</code> Job A is killed. Job B starts. Useful if \"fresh\" data is more important than finishing the old task."},{"location":"jobs-cronjobs/#visualizing-concurrency-policies","title":"Visualizing Concurrency Policies","text":"<pre><code>graph TD\n    Start[Schedule Triggers] --&gt; Check{Is Job Running?}\n\n    Check -- No --&gt; RunNew[Start New Job]\n    Check -- Yes --&gt; Policy{Check Policy}\n\n    Policy -- Allow --&gt; Parallel[Start New Job]\n    Parallel --&gt; Result1[Result: Two Jobs Running]\n\n    Policy -- Forbid --&gt; Skip[Skip New Job]\n    Skip --&gt; Result2[Result: Old Job Continues]\n\n    Policy -- Replace --&gt; Kill[Kill Old Job]\n    Kill --&gt; RunReplace[Start New Job]\n    RunReplace --&gt; Result3[Result: New Job Takes Over]</code></pre>"},{"location":"jobs-cronjobs/#robust-cronjob-example","title":"Robust CronJob Example","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: nightly-backup\nspec:\n  schedule: \"0 0 * * *\" # Run at midnight\n  # If the cluster is down at midnight, start the job \n  # up to 2 hours (7200s) late. If later, skip it.\n  startingDeadlineSeconds: 7200 \n  # Critical: Don't let backups overlap!\n  concurrencyPolicy: Forbid \n  # Keep the last 3 successful jobs for log inspection\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: backup-tool:v1\n            args: [\"/bin/sh\", \"-c\", \"backup.sh\"]\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"jobs-cronjobs/#best-practices","title":"Best Practices","text":"<ol> <li>Idempotency is King: Your jobs must be able to run twice without breaking things. If a node crashes, Kubernetes might restart your Job on another node. If your script says \"Charge Customer Credit Card,\" ensure it checks \"Has customer already been charged?\" first.</li> <li>Always set <code>ttlSecondsAfterFinished</code>: Unless you love manually deleting 5,000 \"Completed\" pods, let Kubernetes clean up after itself.</li> <li>Prefer <code>concurrencyPolicy: Forbid</code>: Unless you are 100% sure your app can handle parallel execution, stick to <code>Forbid</code>. It is the safest default.</li> <li>Monitor CronJobs: CronJobs are \"silent failures.\" If a backup fails, no one notices until you need the restore. Use a monitoring tool (like Prometheus with <code>kube-state-metrics</code>) to alert if the last successful run was &gt; 25 hours ago.</li> </ol>"},{"location":"jobs-cronjobs/#summary","title":"Summary","text":"<ul> <li>Jobs are for tasks that run to completion (batch work).</li> <li>CronJobs schedule Jobs based on time.</li> <li><code>restartPolicy</code> must be <code>OnFailure</code> or <code>Never</code>.</li> <li>Use <code>concurrencyPolicy: Forbid</code> to prevent overlapping jobs from causing race conditions or resource exhaustion.</li> <li>Use <code>ttlSecondsAfterFinished</code> to keep your cluster clean.</li> </ul> <p>Pro Tip</p> <p>Want to test your CronJob immediately without waiting for the schedule? You can manually create a Job from the CronJob template:  <pre><code>kubectl create job --from=cronjob/nightly-backup manual-test-run\n</code></pre></p>"},{"location":"kubectl-cheatsheet/","title":"Kubectl Cheat Sheet","text":"<p>This isn't just a list of commands; it's a collection of the workflows you use every day. From switching contexts to debugging crashed pods, this reference cuts through the noise.</p>"},{"location":"kubectl-cheatsheet/#setup-configuration","title":"Setup &amp; Configuration","text":"<p>Before you do anything, make sure you are talking to the right cluster.</p> Action Command List Contexts <code>kubectl config get-contexts</code> Switch Cluster <code>kubectl config use-context &lt;context_name&gt;</code> Switch Namespace <code>kubectl config set-context --current --namespace=&lt;ns&gt;</code> View Config <code>kubectl config view --minify</code> Who Am I? <code>kubectl auth can-i create pods</code> (Check your own permissions) <p>Pro Tip</p> <p>Install <code>kubectx</code> and <code>kubens</code>. Stop typing long commands. Use these standard tools:</p> <ul> <li><code>kubectx my-cluster</code> (Switch cluster)</li> <li><code>kubens my-namespace</code> (Switch namespace)</li> </ul>"},{"location":"kubectl-cheatsheet/#inspection-observation","title":"Inspection &amp; Observation","text":"<p>The \"Read\" operations. Most of your day is spent here.</p> Object Command Notes Pods <code>kubectl get pods -o wide</code> Shows Node IP and Pod IP. All Namespaces <code>kubectl get pods -A</code> The \"God View\" of the cluster. Watch Live <code>kubectl get pods -w</code> Live stream of status changes. Events <code>kubectl get events --sort-by=.metadata.creationTimestamp</code> Crucial: Shows errors chronologically. Labels <code>kubectl get pods --show-labels</code> Debug Selector issues. Resource Usage <code>kubectl top pod --containers</code> Requires metrics-server."},{"location":"kubectl-cheatsheet/#debugging-the-fix-it-phase","title":"Debugging (The \"Fix It\" Phase)","text":"<p>When things go red, run these in order.</p> Scenario Command Why use it? Why did it die? <code>kubectl describe pod &lt;pod&gt;</code> Read the \"Events\" section at the bottom. App Logs <code>kubectl logs &lt;pod&gt;</code> Standard output of the app. Previous Logs <code>kubectl logs &lt;pod&gt; --previous</code> Gold. See logs of the container before it crashed. Specific Container <code>kubectl logs &lt;pod&gt; -c &lt;sidecar&gt;</code> For multi-container pods (like Service Mesh). Shell Access <code>kubectl exec -it &lt;pod&gt; -- /bin/sh</code> Jump inside to check files/network. Distroless Debug <code>kubectl debug -it &lt;pod&gt; --image=busybox --target=&lt;container&gt;</code> Attaches a shell to a locked-down pod."},{"location":"kubectl-cheatsheet/#creation-modification","title":"Creation &amp; Modification","text":"<p>The \"Write\" operations.</p> Action Command Apply YAML <code>kubectl apply -f my-app.yaml</code> Restart App <code>kubectl rollout restart deployment/my-app</code> (Zero downtime!) Scale Up <code>kubectl scale deployment/my-app --replicas=5</code> Edit Live <code>kubectl edit svc/my-service</code> (Opens in VI/Nano) Force Delete <code>kubectl delete pod &lt;pod&gt; --grace-period=0 --force</code> (Use responsibly!) Quick Job <code>kubectl create job manual-job --image=busybox -- echo \"Done\"</code>"},{"location":"kubectl-cheatsheet/#power-user-tricks-jsonpath","title":"Power User Tricks (JSONPath)","text":"<p>Stop using <code>grep</code>. Use native filtering to get exactly the data you need.</p> <p>1. Get only the Pod IPs:</p> <pre><code>kubectl get pods -o jsonpath='{.items[*].status.podIP}'\n</code></pre> <p>2. List all images running in the cluster:</p> <pre><code>kubectl get pods -A -o jsonpath='{.items[*].spec.containers[*].image}'\n</code></pre> <p>3. Find which node a specific pod is on:</p> <pre><code>kubectl get pod my-pod -o jsonpath='{.spec.nodeName}'\n</code></pre> <p>4. Decode a Secret instantly:</p> <pre><code>kubectl get secret my-secret -o jsonpath='{.data.password}' | base64 -d\n</code></pre>"},{"location":"kubectl-cheatsheet/#housekeeping","title":"Housekeeping","text":"<p>Keep your cluster clean.</p> Command Description <code>kubectl delete pod --field-selector=status.phase=Failed -A</code> Delete all \"Evicted\" or \"Failed\" pods. <code>kubectl api-resources</code> List every object type your cluster supports. <code>kubectl explain pod.spec.containers.livenessProbe</code> Documentation: Read the manual for any field without leaving the terminal."},{"location":"kubectl-cheatsheet/#shell-aliases-save-your-fingers","title":"Shell Aliases (Save Your Fingers)","text":"<p>Add these to your <code>.bashrc</code> or <code>.zshrc</code>. You will thank yourself later.</p> <pre><code>alias k=\"kubectl\"\nalias kg=\"kubectl get\"\nalias kgp=\"kubectl get pods\"\nalias kga=\"kubectl get pods -A\"\nalias kd=\"kubectl describe\"\nalias kdel=\"kubectl delete\"\nalias klogs=\"kubectl logs\"\nalias kex=\"kubectl exec -it\"\n</code></pre> <p>Now you can just type: <code>kex my-pod -- bash</code></p>"},{"location":"kubernetes-api/","title":"Kubernetes API","text":"<p>The Kubernetes API is the brain of your cluster.</p> <p>It is the single point of truth. Whether you are a human using <code>kubectl</code>, a robot (Controller), or a Node (Kubelet), you never talk to each other directly. You only talk to the API Server.</p>"},{"location":"kubernetes-api/#the-hub-and-spoke-architecture","title":"The \"Hub and Spoke\" Architecture","text":"<p>The most important architectural rule in Kubernetes is: Only the API Server talks to etcd.</p> <p><code>etcd</code> is the database where all cluster state is stored. It is highly sensitive. To protect it, Kubernetes forces every single component to go through the API Server to read or write data.</p> <pre><code>graph TD\n    User[User / kubectl] --&gt;|HTTP REST| API[API Server]\n    Controller[Controllers] &lt;--&gt;|Watch Changes| API\n    Kubelet[Worker Node / Kubelet] &lt;--&gt;|Fetch Work| API\n\n    API &lt;--&gt;|Read/Write State| Etcd[(etcd Database)]</code></pre>"},{"location":"kubernetes-api/#anatomy-of-an-object-spec-vs-status","title":"Anatomy of an Object: Spec vs. Status","text":"<p>Every API object represents a \"record of intent.\" To understand Kubernetes, you must understand the war between <code>spec</code> and <code>status</code>.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:        # THE DESIRED STATE (You write this)\n  replicas: 3\nstatus:      # THE OBSERVED STATE (K8s writes this)\n  replicas: 1\n</code></pre> <ol> <li><code>spec</code> (Specification): This is your wish list. You tell the API: \"I desire 3 replicas.\"</li> <li><code>status</code>: This is reality. The system tells you: \"I currently see 1 replica.\"</li> <li>Reconciliation: The Controller Manager sees the difference (3 vs 1) and wakes up to fix it (create 2 more pods).</li> </ol> <p>You typically only write to <code>spec</code>. The system writes to <code>status</code>.</p>"},{"location":"kubernetes-api/#the-request-lifecycle","title":"The Request Lifecycle","text":"<p>What actually happens when you type <code>kubectl apply -f pod.yaml</code>? It's not just \"saving to the database.\" It goes through a rigorous security pipeline.</p> <pre><code>sequenceDiagram\n    participant User\n    participant AuthN as Authentication\n    participant AuthZ as Authorization (RBAC)\n    participant Admission as Admission Controllers\n    participant Etcd\n\n    User-&gt;&gt;AuthN: \"I am Alice. Here is my cert.\"\n    AuthN-&gt;&gt;AuthZ: \"Is Alice allowed to create Pods?\"\n    AuthZ-&gt;&gt;Admission: \"Yes. Now, check the content.\"\n    Admission-&gt;&gt;Admission: \"Is the image secure? Are limits set?\"\n    Admission-&gt;&gt;Etcd: \"Looks good. Persist to DB.\"\n    Etcd--&gt;&gt;User: \"201 Created\"</code></pre> <ol> <li>Authentication (AuthN): \"Who are you?\" (Certificates, Tokens, OIDC).</li> <li>Authorization (AuthZ): \"Are you allowed to do this?\" (RBAC checks).</li> <li>Admission Control: \"Is this request smart?\"<ul> <li>Mutating: \"You forgot to set a default CPU limit, so I'll add one for you.\"</li> <li>Validating: \"You are trying to run as <code>root</code> user? Denied.\"</li> </ul> </li> <li>Persistence: The object is written to <code>etcd</code>.</li> </ol>"},{"location":"kubernetes-api/#api-groups-versioning","title":"API Groups &amp; Versioning","text":"<p>Kubernetes APIs change. To keep things stable, they are grouped and versioned.</p> <ul> <li>Alpha (<code>v1alpha1</code>): Experimental. Might be deleted in the next release. Do not use in production.</li> <li>Beta (<code>v1beta1</code>): Useful and well-tested, but details might change. Enabled by default.</li> <li>Stable (<code>v1</code>): Rock solid. Will be supported for years.</li> </ul>"},{"location":"kubernetes-api/#common-api-paths","title":"Common API Paths","text":"Path URL Group Name Resources <code>/api/v1</code> Core Pods, Services, Nodes, ConfigMaps, Secrets <code>/apis/apps/v1</code> Apps Deployments, DaemonSets, StatefulSets <code>/apis/batch/v1</code> Batch Jobs, CronJobs <code>/apis/networking.k8s.io/v1</code> Networking Ingress, NetworkPolicies <p>Pro Tip</p> <p>Run <code>kubectl api-resources</code> to see a full list of every object your cluster supports, including its shortname (e.g., <code>po</code> for Pods) and API Group.</p>"},{"location":"kubernetes-api/#declarative-vs-imperative","title":"Declarative vs. Imperative","text":"<p>The API supports two ways of working.</p> <p>1. Imperative (The \"Do It\" Command) You tell the API exactly what action to take.</p> <ul> <li><code>kubectl run nginx --image=nginx</code></li> <li><code>kubectl scale deployment web --replicas=5</code></li> <li>Downside: If you run it twice, it might fail. It's hard to reproduce.</li> </ul> <p>2. Declarative (The \"Make It So\" Command) You give the API a file representing your final desired state.</p> <ul> <li><code>kubectl apply -f my-app.yaml</code></li> <li>Upside: You can run this 100 times. If the state is already correct, nothing happens. If it's different, K8s fixes it. Always use this for production.</li> </ul>"},{"location":"kubernetes-api/#debugging-the-api","title":"Debugging the API","text":"<p>Want to see the API in action? You don't need a proxy. You can ask <code>kubectl</code> to show you the raw HTTP requests it is making.</p> <p>Try this command:</p> <pre><code>kubectl get pods -v=6\n</code></pre> <p>Output:</p> <pre><code>I1215... loader.go:372] Config loaded from file: /root/.kube/config\nI1215... round_trippers.go:420] GET https://10.96.0.1:443/api/v1/namespaces/default/pods?limit=500\nI1215... round_trippers.go:427] Response Status: 200 OK in 12 milliseconds\n</code></pre> <p>You can see the exact GET request <code>kubectl</code> sent to the API server! This is incredibly useful for debugging authentication or permission issues.</p>"},{"location":"kubernetes-api/#summary","title":"Summary","text":"<ul> <li>The API Server is the only component that talks to <code>etcd</code>.</li> <li>Objects consist of <code>spec</code> (what you want) and <code>status</code> (what you have).</li> <li>Controllers watch the API to make <code>status</code> match <code>spec</code>.</li> <li>Every request goes through AuthN, AuthZ, and Admission Control.</li> <li>Use <code>kubectl -v=6</code> to peek under the hood.</li> </ul>"},{"location":"limits-requests/","title":"Limits & Requests","text":"<p>Kubernetes creates a \"shared universe\" where many applications run on the same physical server (Node).</p> <p>Without rules, one greedy application (like a memory-leaking Java app or a crypto-miner) could consume 100% of the CPU and RAM, causing every other application on that node to crash.</p> <p>To prevent this \"Noisy Neighbor\" problem, Kubernetes uses Resource Requests and Limits.</p>"},{"location":"limits-requests/#1-the-units-what-do-the-numbers-mean","title":"1. The Units (What do the numbers mean?)","text":"<p>Before setting rules, you need to speak the language.</p>"},{"location":"limits-requests/#cpu-millicores","title":"CPU: Millicores","text":"<p>CPU is measured in \"cores.\" But since a container rarely needs a full core, we use millicores (m).</p> <ul> <li><code>1</code> or <code>1000m</code> = 100% of one CPU core (vCPU).</li> <li><code>500m</code> = 50% of one core.</li> <li><code>100m</code> = 10% of one core.</li> </ul> <p>Pro Tip</p> <p>You can just say <code>0.5</code> instead of <code>500m</code>. They mean the exact same thing.</p>"},{"location":"limits-requests/#memory-bytes","title":"Memory: Bytes","text":"<p>Memory is measured in bytes. You can use standard suffixes (E, P, T, G, M, K) or their power-of-two equivalents (Ei, Pi, Ti, Gi, Mi, Ki).</p> <ul> <li><code>128Mi</code> = 128 Mebibytes (approx 134 MB).</li> <li><code>1Gi</code> = 1 Gibibyte.</li> </ul> <p>Warning</p> <p>Mi vs M: Be careful! <code>1000M</code> (Megabytes) is actually smaller than <code>1000Mi</code> (Mebibytes). In Kubernetes, we almost always use Mi and Gi.</p>"},{"location":"limits-requests/#2-requests-vs-limits","title":"2. Requests vs. Limits","text":"<p>This is the most important concept to master.</p> Setting The Analogy Technical Behavior Request The Reservation Kubernetes guarantees this amount. It uses this number to decide which node to place the Pod on. Limit The Speed Governor The absolute hard maximum the container can ever use. If it tries to go over, Kubernetes stops it."},{"location":"limits-requests/#visualizing-the-difference","title":"Visualizing the difference","text":"<p>Imagine a Node with 10GB of RAM.</p> <ul> <li>You have a Pod asking for <code>requests: 2GB</code> / <code>limits: 8GB</code>.</li> <li>Scheduling: Kubernetes sees \"2GB Request\" and puts it on the node.</li> <li>Runtime: The Pod sits idle using 1GB.</li> <li>Traffic Spike: The Pod jumps to 4GB usage. This is allowed (it's under the Limit).</li> <li>Memory Leak: The Pod hits 8GB. BAM! Kubernetes kills it.</li> </ul> <pre><code>graph TD\n    subgraph Node Capacity [Total Node CPU: 4 Cores]\n        P1[Pod A Request: 1 Core]\n        P2[Pod B Request: 2 Cores]\n        Free[Free Space: 1 Core]\n    end\n\n    subgraph \"Runtime Behavior\"\n        P1 -- \"Can burst up to limit\" --&gt; P1Limit[Pod A Limit: 3 Cores]\n        P2 -- \"Capped strictly\" --&gt; P2Limit[Pod B Limit: 2 Cores]\n    end</code></pre>"},{"location":"limits-requests/#3-what-happens-when-you-hit-the-limit","title":"3. What happens when you hit the Limit?","text":"<p>The consequence depends heavily on which resource you run out of.</p>"},{"location":"limits-requests/#memory-the-oomkill","title":"Memory: The \"OOMKill\"","text":"<p>Memory is a non-compressible resource. You can't just \"slow down\" memory usage; you either have the bytes or you don't.</p> <ul> <li>If a container exceeds Memory Limit: The Linux Kernel invokes the OOM Killer (Out Of Memory Killer). It immediately sends a <code>SIGKILL</code> (Exit Code 137) to your container.</li> <li>Result: The Pod crashes and restarts.</li> </ul>"},{"location":"limits-requests/#cpu-throttling","title":"CPU: Throttling","text":"<p>CPU is a compressible resource.</p> <ul> <li>If a container exceeds CPU Limit: Kubernetes starts Throttling. It artificially denies CPU time to the container for short periods (microseconds).</li> <li>Result: Your app gets slow and laggy, but it does not crash.</li> </ul>"},{"location":"limits-requests/#4-qos-classes-who-dies-first","title":"4. QoS Classes (Who dies first?)","text":"<p>Kubernetes assigns every Pod a \"Quality of Service\" (QoS) class based on how you configured your resources. When a Node runs out of resources, this class determines who gets evicted first.</p> QoS Class Configuration Priority Guaranteed Requests == Limits (for both CPU &amp; Mem) Highest (Last to be killed) Burstable Requests \\&lt; Limits Medium BestEffort No Requests or Limits set Lowest (First to be killed) <p>Avoid BestEffort in Production</p> <p>If you don't set requests/limits, your Pod is \"BestEffort.\" If the node gets busy, Kubernetes will delete your Pod first to save the others.</p>"},{"location":"limits-requests/#5-best-practices","title":"5. Best Practices","text":"<ol> <li>Always set Requests: Without them, the Scheduler is flying blind and will overbook nodes, causing stability issues.</li> <li>Requests = Limits for Databases: For critical components (Postgres, Redis), set <code>requests</code> equal to <code>limits</code> (Guaranteed QoS). You don't want your database getting throttled or killed when the node is busy.</li> <li>Use LimitRanges: As an admin, you can create a <code>LimitRange</code> object in a namespace to force every new Pod to have default limits. This stops developers from deploying \"naked\" pods that consume infinite resources.</li> <li>Don't starve the Sidecars: If you use service meshes (like Istio or Linkerd), remember their sidecar proxies need CPU/Memory too!</li> </ol>"},{"location":"limits-requests/#summary","title":"Summary","text":"<ul> <li>Requests = The minimum guarantee (used for Scheduling).</li> <li>Limits = The hard maximum (used for Throttling/Killing).</li> <li>CPU Limit Hit = Slowness (Throttling).</li> <li>Memory Limit Hit = Death (OOMKill).</li> <li>Millicores (m) = 1/1000th of a core. <code>1000m</code> = 1 vCPU.</li> <li>QoS Classes determine eviction order. Aim for Burstable or Guaranteed in production.</li> </ul>"},{"location":"maintenance/","title":"Maintenance","text":"<p>Maintenance is the most nerve-wracking part of being a Kubernetes Admin. Unlike a standard Linux server where you just run <code>apt-get upgrade</code>, upgrading Kubernetes requires a specific dance to ensure you don't drop traffic or corrupt your database.</p>"},{"location":"maintenance/#1-the-save-game-button-etcd-backup","title":"1. The \"Save Game\" Button (Etcd Backup)","text":"<p>Before you touch anything, back up etcd. Etcd contains the state of your entire cluster. If an upgrade fails and corrupts etcd, your cluster is gone.</p> <p>The Snapshot Command: You typically run this on a Control Plane node.</p> <pre><code>ETCDCTL_API=3 etcdctl snapshot save snapshot.db \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key\n</code></pre> <ul> <li>Verify it worked: <code>ETCDCTL_API=3 etcdctl snapshot status snapshot.db</code></li> </ul>"},{"location":"maintenance/#2-upgrade-order-of-operations","title":"2. Upgrade Order of Operations","text":"<p>You cannot just upgrade everything at once. You must follow the Version Skew Policy. Generally, components can lag behind, but they can never be newer than the API Server.</p> <p>The Strict Order:</p> <ol> <li>Primary Control Plane Node: Upgrade the API Server &amp; Controller Manager first.</li> <li>Other Control Plane Nodes: Upgrade the rest of the masters.</li> <li>Worker Nodes: Upgrade them one by one (or in batches).</li> </ol> <p>The Golden Rule</p> <p>Never upgrade a Kubelet (Worker Node) to a version newer than the API Server (Control Plane). The API Server must always be the highest version in the cluster.</p>"},{"location":"maintenance/#3-node-maintenance-workflow-drain-cordon","title":"3. Node Maintenance Workflow (Drain &amp; Cordon)","text":"<p>When you need to patch the OS (e.g., kernel update) or upgrade the Kubernetes version on a specific node, you must evacuate the workloads first.</p> <p>Concept:</p> <ol> <li>Cordon: Put up a \"Do Not Enter\" sign. No new pods will be scheduled here.</li> <li>Drain: Gently kick everyone out. It sends a shutdown signal to existing pods so they can finish their requests and move to another node.</li> <li>Uncordon: Open the node back up for business.</li> </ol>"},{"location":"maintenance/#the-visual-workflow","title":"The Visual Workflow","text":"<pre><code>graph TD\n    Start[\"Start Maintenance\"] --&gt; Cordon[\"1. Cordon Node&lt;br/&gt;(Stop Scheduling)\"]\n    Cordon --&gt; Drain[\"2. Drain Node&lt;br/&gt;(Evict Pods)\"]\n\n    Drain --&gt; Work[\"3. Perform Work&lt;br/&gt;(Reboot / Upgrade)\"]\n\n    Work --&gt; Ready{\"Is Node Healthy?\"}\n    Ready -- No --&gt; Debug[\"Troubleshoot\"]\n    Ready -- Yes --&gt; Uncordon[\"4. Uncordon Node&lt;br/&gt;(Resume Scheduling)\"]\n\n    Uncordon --&gt; End[\"Done\"]</code></pre>"},{"location":"maintenance/#the-commands","title":"The Commands","text":"<p>Step 1: Evacuate</p> <pre><code># Safely evict all pods (respecting PodDisruptionBudgets)\nkubectl drain node-01 --ignore-daemonsets --delete-emptydir-data\n</code></pre> <ul> <li><code>--ignore-daemonsets</code>: Necessary because you can't \"move\" a DaemonSet (it lives on every node).</li> <li><code>--delete-emptydir-data</code>: Necessary if a pod uses local temporary storage (data will be lost).</li> </ul> <p>Step 2: Upgrade (Example: kubeadm)</p> <pre><code># (On the node node-01)\nsudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm=1.29.0-00\nsudo kubeadm upgrade node\nsudo apt-get install -y kubelet=1.29.0-00 kubectl=1.29.0-00\nsudo systemctl restart kubelet\n</code></pre> <p>Step 3: Resume</p> <pre><code>kubectl uncordon node-01\n</code></pre>"},{"location":"maintenance/#4-certificate-rotation","title":"4. Certificate Rotation","text":"<p>If you built your cluster with <code>kubeadm</code>, the internal TLS certificates (used for components to talk to each other) expire after 1 year. If you forget this, your cluster will suddenly stop working one day with \"x509: certificate has expired\" errors.</p> <p>Check Expiration:</p> <pre><code>kubeadm certs check-expiration\n</code></pre> <p>Renew Certificates:</p> <pre><code>kubeadm certs renew all\n# Then restart the control plane static pods (move manifest files out and back in)\n</code></pre>"},{"location":"maintenance/#5-os-patching","title":"5. OS Patching","text":"<p>Kubernetes runs on Linux. You still need to patch the underlying OS (Ubuntu, RHEL, Rocky).</p> <ul> <li>Do not configure \"Unattended Upgrades\" to auto-reboot.</li> <li>If all your nodes reboot at the same time (e.g., a Tuesday night patch window), your entire cluster goes down.</li> <li>Best Practice: Use a tool like Kured (Kubernetes Reboot Daemon). It watches for the <code>/var/run/reboot-required</code> flag and reboots nodes one by one, ensuring the cluster stays healthy.</li> </ul>"},{"location":"maintenance/#summary","title":"Summary","text":"<ul> <li>Backup First: Always snapshot etcd before an upgrade.</li> <li>Order Matters: Control Plane $\\rightarrow$ Workers.</li> <li>Drain Safely: Use <code>kubectl drain</code> to move workloads without downtime.</li> <li>Watch Certs: Remember the 1-year expiration on <code>kubeadm</code> clusters.</li> <li>Automate Reboots: Use Kured to handle OS patching reboots safely.</li> </ul>"},{"location":"namespaces/","title":"Namespaces","text":"<p>If a Kubernetes cluster is a physical building, Namespaces are the separate offices (or apartments) inside it.</p> <p>Everyone shares the same electricity (CPU) and plumbing (Memory), but they have their own keys, their own furniture, and they can't see what's happening in the office next door unless they are explicitly invited.</p> <p>Namespaces provide Logical Isolation. They allow you to host \"Dev\", \"Staging\", and \"Prod\" on the same cluster without them accidentally overwriting each other's configurations.</p>"},{"location":"namespaces/#the-default-namespaces","title":"The \"Default\" Namespaces","text":"<p>When you start a fresh cluster, Kubernetes isn't empty. It comes with four built-in namespaces:</p> Namespace Purpose <code>default</code> Where your work goes if you don't specify a namespace. <code>kube-system</code> The \"Engine Room.\" Contains the API Server, Scheduler, and DNS. Do not touch this unless you know what you are doing. <code>kube-public</code> Readable by everyone (even unauthenticated users). Rarely used, mostly for cluster bootstrapping. <code>kube-node-lease</code> A technical namespace used by Kubelets to send \"heartbeats\" to the master."},{"location":"namespaces/#scoping-who-lives-where","title":"Scoping: Who lives where?","text":"<p>Not everything fits in a namespace. This is a crucial concept for administrators.</p> <ul> <li>Namespaced Resources: These live inside a room. (e.g., Pods, Deployments, Services, ConfigMaps). Two different namespaces can both have a Deployment named \"my-app\".</li> <li>Cluster-Scoped Resources: These are the building itself. They exist globally. (e.g., Nodes, PersistentVolumes, StorageClasses). You cannot have two Nodes with the same name, ever.</li> </ul> <p>How to check? Run this command to see which resources are namespaced:</p> <pre><code>kubectl api-resources --namespaced=true\n</code></pre>"},{"location":"namespaces/#cross-namespace-communication-dns","title":"Cross-Namespace Communication (DNS)","text":"<p>Beginners often ask: \"Can a Pod in Dev talk to a Service in Prod?\" Yes. (Unless blocked by a NetworkPolicy).</p> <p>By default, namespaces are not network firewalls. They are just organizational folders. However, the DNS name changes.</p> <ul> <li>Same Namespace: You can just call the service name.<ul> <li><code>curl http://my-database</code></li> </ul> </li> <li>Different Namespace: You must use the Fully Qualified Domain Name (FQDN).<ul> <li><code>curl http://my-database.production.svc.cluster.local</code></li> </ul> </li> </ul>"},{"location":"namespaces/#best-practices-for-organization","title":"Best Practices for Organization","text":""},{"location":"namespaces/#1-environments-vs-teams","title":"1. Environments vs. Teams","text":"<p>How should you slice your cluster?</p> <ul> <li>Small Company: <code>dev</code>, <code>staging</code>, <code>prod</code>.</li> <li>Large Enterprise: <code>team-a-dev</code>, <code>team-a-prod</code>, <code>team-b-dev</code>.</li> </ul>"},{"location":"namespaces/#2-resource-quotas-the-budget","title":"2. Resource Quotas (The Budget)","text":"<p>You can assign a \"Budget\" to a namespace to prevent one team from using all the cluster's RAM.</p> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: dev-quota\n  namespace: dev\nspec:\n  hard:\n    pods: \"10\"             # Max 10 pods allowed\n    requests.cpu: \"4\"      # Max 4 CPU cores total\n    requests.memory: 2Gi   # Max 2GB RAM total\n</code></pre>"},{"location":"namespaces/#3-limitranges-the-rules","title":"3. LimitRanges (The Rules)","text":"<p>You can force every Pod in a namespace to have a default size, so users don't have to guess.</p> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: default-mem-limit\n  namespace: dev\nspec:\n  limits:\n  - default:\n      memory: 512Mi\n    type: Container\n</code></pre>"},{"location":"namespaces/#managing-context-the-kubens-tool","title":"Managing Context (The <code>kubens</code> Tool)","text":"<p>Typing <code>-n my-long-namespace-name</code> on every command is painful.</p> <p>The Hard Way:</p> <pre><code>kubectl config set-context --current --namespace=my-team-dev\n</code></pre> <p>The Pro Way: Install a tool called <code>kubens</code> (part of the <code>kubectx</code> package).</p> <pre><code>kubens my-team-dev\n</code></pre> <p>Now all your future <code>kubectl</code> commands automatically run in that namespace until you switch back.</p>"},{"location":"namespaces/#summary","title":"Summary","text":"<ul> <li>Namespaces allow you to partition a single cluster into virtual sub-clusters.</li> <li>They are ideal for multi-tenancy (separating teams or environments).</li> <li>Resources like Nodes and PVs are global; Pods and Services are namespaced.</li> <li>Services in different namespaces can talk to each other using their full DNS name (FQDN).</li> <li>Use ResourceQuotas to prevent a single namespace from hogging all the cluster resources.</li> </ul>"},{"location":"netpol/","title":"Network Policies","text":"<p>By default, Kubernetes is an \"Open Network.\"</p> <ul> <li>Any Pod can talk to any other Pod.</li> <li>A hacked web server in the <code>default</code> namespace can port-scan your database in the <code>secure</code> namespace.</li> </ul> <p>NetworkPolicies are the firewall rules of Kubernetes. They let you enforce a Zero Trust environment where traffic is blocked unless explicitly allowed.</p>"},{"location":"netpol/#1-the-isolation-switch","title":"1. The \"Isolation\" Switch","text":"<p>Network Policies work differently than traditional firewalls.</p> <ol> <li>No Policy: The Pod is \"Non-Isolated.\" It accepts traffic from anywhere.</li> <li>Policy Exists: As soon as a NetworkPolicy selects a Pod, that Pod becomes \"Isolated.\"<ul> <li>It immediately blocks ALL traffic that isn't explicitly allowed by the policy.</li> </ul> </li> </ol>"},{"location":"netpol/#visualizing-the-switch","title":"Visualizing the Switch","text":"<pre><code>graph TD\n    subgraph \"Scenario A: No Policy\"\n    Web1[Web Pod] --&gt;|Allowed| DB1[DB Pod]\n    Hacker1[Hacker Pod] --&gt;|Allowed!| DB1\n    end\n\n    subgraph \"Scenario B: With NetworkPolicy\"\n    Web2[Web Pod] --&gt;|Allowed by Rule| DB2[DB Pod]\n    Hacker2[Hacker Pod] -- \"Blocked (Default Deny)\" --&gt; DB2\n\n    NP[NetworkPolicy] -.-&gt;|Selects| DB2\n    end\n\n    style DB2 stroke:#ff0000</code></pre>"},{"location":"netpol/#2-the-default-deny-start-here","title":"2. The \"Default Deny\" (Start Here)","text":"<p>The best practice for security is to start by locking everything down, and then punching holes for what you need.</p> <p>This policy acts as a \"Catch-All\" to block all incoming and outgoing traffic for every Pod in the namespace.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: default\nspec:\n  podSelector: {} # Selects ALL pods in this namespace\n  policyTypes:\n    - Ingress\n    - Egress\n</code></pre> <p>Once you apply this, your application will stop working. Now you must explicitly allow traffic.</p>"},{"location":"netpol/#3-allow-specific-traffic-ingress","title":"3. Allow Specific Traffic (Ingress)","text":"<p>Let's say we want to allow our <code>backend</code> to receive traffic from the <code>frontend</code>, but nothing else.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\nspec:\n  podSelector:\n    matchLabels:\n      app: backend # The target (Who gets protected?)\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              app: frontend # The source (Who can knock?)\n      ports:\n        - port: 8080\n          protocol: TCP\n</code></pre>"},{"location":"netpol/#4-the-namespace-gotcha","title":"4. The \"Namespace\" Gotcha","text":"<p>A common mistake is trying to allow traffic from a different namespace using <code>podSelector</code>. <code>podSelector</code> only works within the same namespace.</p> <p>To allow traffic from any Pod in the <code>monitoring</code> namespace:</p> <pre><code>ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring # Requires the namespace to have this label!\n</code></pre> <p>Label Your Namespaces!</p> <p><code>namespaceSelector</code> selects based on Labels, not the Namespace name. You must run: <code>kubectl label namespace monitoring name=monitoring</code></p>"},{"location":"netpol/#5-egress-the-dns-trap","title":"5. Egress &amp; The DNS Trap","text":"<p>If you block Egress (outbound traffic), you block DNS lookups too. If your Pod can't reach the DNS server (CoreDNS), it can't resolve <code>google.com</code> or <code>my-db</code>, and your app will crash.</p> <p>Always allow UDP port 53 if you restrict Egress.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-dns-egress\nspec:\n  podSelector: {} # Apply to all pods\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n      - namespaceSelector:\n          matchLabels:\n            name: kube-system # Where CoreDNS lives\n      ports:\n        - port: 53\n          protocol: UDP\n        - port: 53\n          protocol: TCP\n</code></pre>"},{"location":"netpol/#6-combining-selectors-and-vs-or","title":"6. Combining Selectors (AND vs OR)","text":"<p>Be careful with your YAML indentation. It changes the logic completely.</p> <p>Logic: OR (Any of these) Traffic allowed from <code>frontend</code> OR from <code>monitoring</code>.</p> <pre><code>ingress:\n  - from:\n    - podSelector: { matchLabels: { app: frontend } }\n    - namespaceSelector: { matchLabels: { name: monitoring } }\n</code></pre> <p>Logic: AND (Must match BOTH) Traffic allowed only from a pod labeled <code>frontend</code> running inside the <code>monitoring</code> namespace.</p> <pre><code>ingress:\n  - from:\n    - podSelector: { matchLabels: { app: frontend } }\n      namespaceSelector: { matchLabels: { name: monitoring } }\n</code></pre>"},{"location":"netpol/#summary","title":"Summary","text":"<ul> <li>NetworkPolicies require a CNI that supports them (Calico, Cilium, Antrea).</li> <li>They act as a firewall for Pods.</li> <li>Default Behavior: Allow All.</li> <li>Selected Behavior: Deny Everything Else.</li> <li>Best Practice: Apply a \"Default Deny\" policy first.</li> <li>Critical Gotcha: If you block Egress, remember to Allow DNS (Port 53), or nothing will work.</li> </ul>"},{"location":"networking/","title":"Networking","text":"Networking Overview <p>Networking in Kubernetes is simple on the surface, but powerful under the hood. Every Pod gets an IP address, Services provide stable endpoints, and the network model enables communication across the entire cluster - often without needing to understand the low-level implementation details.</p> Core Principles of Kubernetes Networking <ol> <li>Each Pod gets a unique IP</li> <li>No NAT between Pods</li> <li> <p>All containers within a Pod share the same network namespace</p> </li> <li> <p>All Pods can reach each other</p> </li> <li> <p>Flat network model (no IP masquerading between Pods)</p> </li> <li> <p>Services provide stable access to Pods</p> </li> <li>Pods are ephemeral - Services give them a consistent IP + DNS name</li> </ol> Network Abstraction Layers Layer Purpose Pod Network Every Pod gets an IP, routable in-cluster Service Provides a stable endpoint for Pod groups Ingress Exposes HTTP/S services externally NetworkPolicy Controls traffic between Pods (optional) DNS in Kubernetes <p>Kubernetes includes built-in DNS resolution for:</p> <ul> <li>Services: <code>my-service.my-namespace.svc.cluster.local</code></li> <li>Pods (not recommended for direct use)</li> </ul> <p>DNS is powered by CoreDNS by default, running in the <code>kube-system</code> namespace.</p> <pre><code>nslookup my-service.default.svc.cluster.local\n</code></pre>"},{"location":"networking/#pod-to-pod-communication","title":"Pod-to-Pod Communication","text":"<ul> <li>All Pods are routable via their internal IP addresses</li> <li>No need for manual port forwarding</li> <li>Backed by a Container Network Interface (CNI) plugin (e.g., Calico, Flannel)</li> </ul>"},{"location":"networking/#service-types-covered-in-next-section","title":"Service Types (Covered in next section)Summary","text":"<ul> <li><code>ClusterIP</code> \u2013 default; internal-only</li> <li><code>NodePort</code> \u2013 exposes on every node</li> <li><code>LoadBalancer</code> \u2013 cloud provider external IP</li> <li><code>ExternalName</code> \u2013 DNS alias</li> </ul> <ul> <li>Kubernetes networking gives every Pod a unique IP and makes service discovery simple.</li> <li>All Pods can talk to each other by default-use NetworkPolicies to restrict if needed.</li> <li>Understanding the network model is key for debugging, scaling, and securing your apps.</li> </ul> <p>Tip</p> <p>Use DNS names for service discovery, and always test network policies and connectivity in staging before rolling out to production.</p>"},{"location":"operators-crds/","title":"Operators & CRDs","text":"<p>Kubernetes is great at managing stateless things. If a web server dies, Kubernetes replaces it. Easy.</p> <p>But what about complex, stateful applications like a Database?</p> <ul> <li>You can't just kill a database pod randomly; you might corrupt data.</li> <li>You can't just scale up a database by adding replicas; you need to configure leader election and data replication.</li> <li>You need to take backups before upgrades.</li> </ul> <p>Standard Kubernetes (Deployments/StatefulSets) doesn't know how to do any of that specific logic.</p> <p>Operators solve this. An Operator is essentially a Robot Sysadmin packaged as software. It runs inside your cluster and knows exactly how to manage a specific application (like Postgres, Kafka, or Prometheus) day-to-day.</p>"},{"location":"operators-crds/#1-custom-resource-definitions-crds","title":"1. Custom Resource Definitions (CRDs)","text":"<p>Before we can have a robot, we need a language to talk to it.</p> <p>Kubernetes comes with standard resources: <code>Pod</code>, <code>Service</code>, <code>Deployment</code>. But what if you want to create a <code>PostgresCluster</code> or a <code>KafkaTopic</code>?</p> <p>CRDs (Custom Resource Definitions) allow you to extend the Kubernetes API. They let you invent your own YAML objects.</p>"},{"location":"operators-crds/#the-crd-the-noun","title":"The CRD (The \"Noun\")","text":"<p>The CRD is just the definition of the new object. It tells Kubernetes: \"Hey, <code>Prometheus</code> is now a valid word in this cluster, and it looks like this schema.\"</p> <p>Once a CRD is installed, you can apply YAMLs like this:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: my-monitoring\nspec:\n  version: v2.26.0\n  replicas: 2\n  retention: 30d\n</code></pre> <p>Without the CRD, Kubernetes would reject this file saying <code>error: unknown resource type \"Prometheus\"</code>.</p>"},{"location":"operators-crds/#2-the-operator-the-verb","title":"2. The Operator (The \"Verb\")","text":"<p>A CRD is just a piece of paper. If you create a <code>Prometheus</code> object, nothing happens... unless there is a Controller watching it.</p> <p>The Operator is a Pod running custom code (usually written in Go).</p> <ol> <li>It watches for changes to its specific Custom Resources (e.g., someone created a <code>Prometheus</code> object).</li> <li>It reads the spec (e.g., \"User wants 2 replicas and 30d retention\").</li> <li>It translates that into standard Kubernetes objects (creating StatefulSets, ConfigMaps, Services, and PersistentVolumes).</li> <li>It maintains the state. If a backup fails, the Operator tries again.</li> </ol>"},{"location":"operators-crds/#the-operator-pattern","title":"The Operator Pattern","text":"<p>The \"Operator Pattern\" combines these two things:</p> <ol> <li>CRDs: To define the Desired State.</li> <li>Custom Controller: To implement the logic to reach that state.</li> </ol>"},{"location":"operators-crds/#example-the-prometheus-operator","title":"Example: The Prometheus Operator","text":"<p>Instead of manually managing thousands of lines of config files for Prometheus, you simply install the Prometheus Operator.</p> <ol> <li>You: <code>kubectl apply -f my-monitor.yaml</code> (asking for a monitoring stack).</li> <li>Operator: Sees the file.</li> <li>Operator: Automatically generates the complex <code>StatefulSet</code> configurations, creates the <code>Service</code>, mounts the correct <code>Secrets</code>, and reloads the configuration if it changes.</li> </ol>"},{"location":"operators-crds/#operator-capability-levels","title":"Operator Capability Levels","text":"<p>Not all Operators are created equal. The Operator Capability Model defines how smart the robot is.</p> Level Name Description I Basic Install Can deploy the app and minimal config. II Seamless Upgrades Can handle version upgrades (e.g., v1.0 -&gt; v1.1) automatically. III Full Lifecycle Can manage storage, backups, and failure recovery. IV Deep Insights Provides metrics, alerts, and log processing. V Auto Pilot Automatically scales, tunes performance, and heals without humans. <p>Aim for Level III+ operators for critical databases.</p>"},{"location":"operators-crds/#how-to-find-install-operators","title":"How to Find &amp; Install Operators","text":"<p>You don't usually write Operators; you buy or download them.</p> <p>OperatorHub.io is the public registry for Kubernetes Operators. It includes verified operators for:</p> <ul> <li>Databases (Postgres, MongoDB, Redis)</li> <li>Messaging (Kafka, RabbitMQ)</li> <li>Monitoring (Prometheus, Grafana)</li> </ul>"},{"location":"operators-crds/#installation-methods","title":"Installation Methods","text":"<ol> <li>Plain YAML / Helm: Many operators can be installed just by <code>kubectl apply</code> or <code>helm install</code>.</li> <li>OLM (Operator Lifecycle Manager): A system that runs on your cluster to manage the installation and automatic upgrades of Operators (similar to \"Windows Update\" for K8s apps).</li> </ol>"},{"location":"operators-crds/#developing-your-own-briefly","title":"Developing Your Own (Briefly)","text":"<p>If you are a software vendor, you might need to build an operator for your product. You typically use:</p> <ul> <li>Operator SDK: A toolkit to bootstrap the code.</li> <li>Kubebuilder: A framework for building Kubernetes APIs in Go.</li> </ul> <pre><code># Concept: Scaffolding a new Operator\noperator-sdk init --domain=my-company.com\noperator-sdk create api --group database --version v1 --kind PostgreSQL\n</code></pre> <p>This generates the boilerplate Go code so you can focus on the business logic: \"When user creates X, do Y.\"</p>"},{"location":"operators-crds/#summary","title":"Summary","text":"<ul> <li>CRDs let you extend the Kubernetes API with your own custom objects.</li> <li>Operators are the software brains that manage those custom objects.</li> <li>Operators replace human operational knowledge (backups, scaling, upgrades) with code.</li> <li>Use OperatorHub.io to find ready-to-use operators for popular software.</li> </ul> <p>Pro Tip</p> <p>Be careful with \"Level 1\" operators. If an operator only helps you install the app but doesn't help you back it up or upgrade it, you might be better off just using a Helm Chart. The real value of an Operator is \"Day 2\" management.</p>"},{"location":"overview/","title":"Getting Started","text":""},{"location":"overview/#historical-background","title":"Historical BackgroundIntroduction to KubernetesWhat is Kubernetes?Key Concepts of Kubernetes","text":"<p>Kubernetes was born from Google's internal systems like Borg and Omega, which managed containerized applications like Search and Gmail at a massive scale. In 2014, Google open-sourced Kubernetes, and it quickly became the standard for container orchestration.</p> <p>Kubernetes (K8s) is like the air traffic controller for your applications - making sure everything is running, scaling, and healing automatically. Originally created by Google and now maintained by the CNCF, Kubernetes helps you run containers (small, portable application units) across clusters of computers.</p> <p>Kubernetes is a platform that automates the deployment, scaling, and management of containerized applications. Think of it as an operating system for your data center, making sure your apps are always running the way you want.</p> <p>Key things Kubernetes does for you:</p> <ul> <li>Deployment: Launches and manages containers for your apps.</li> <li>Scaling: Adds or removes copies of your app as needed.</li> <li>Self-healing: Restarts or replaces containers if they fail.</li> <li>Rolling Updates/Rollbacks: Updates your app with zero downtime and can revert if something goes wrong.</li> </ul> Declarative Model <p>Kubernetes uses a \"declarative\" approach: you describe how you want your system to look, and Kubernetes works to make it so - automatically.</p> <p>Three key ideas:</p> <ol> <li>Observed State: What\u2019s actually running right now.</li> <li>Desired State: What you want running (defined in YAML or JSON).</li> <li>Reconciliation: Kubernetes constantly checks and adjusts to make observed = desired.</li> </ol> <p>How it works:</p> <ul> <li>You tell Kubernetes (with <code>kubectl</code> or a YAML file) what you want.</li> <li>Kubernetes saves this in its database (etcd).</li> <li>Controllers keep checking: does reality match what you asked for?</li> <li>If not, Kubernetes takes action to fix it.</li> </ul> Declarative Approach in Kubernetes <p>You define what you want, Kubernetes keeps it that way. Here\u2019s a visual summary:</p> <pre><code>sequenceDiagram\n    participant User\n    participant APIServer as API Server\n    participant etcd\n    participant Controller as Controller Manager\n    participant Scheduler\n\n    User-&gt;&gt;APIServer: Declare desired state\n    APIServer-&gt;&gt;etcd: Persist desired state\n\n    Controller-&gt;&gt;APIServer: Check actual vs. desired\n    APIServer--&gt;&gt;Controller: current != desired\n    Controller-&gt;&gt;APIServer: Reconcile differences\n\n    APIServer-&gt;&gt;Scheduler: Trigger scheduling if needed</code></pre> <p>Tip: Most real-world Kubernetes work is about describing the desired state in YAML files.</p> <p>This diagram illustrates how Kubernetes manages resources declaratively, ensuring the system's state aligns with the user's specifications.</p> Kubernetes Architecture <p>Kubernetes architecture consists of several key components:</p> <ul> <li>API Server: The front-end for the Kubernetes control plane, handling all REST operations.</li> <li>etcd: A consistent and highly-available key-value store used as Kubernetes' backing store for all cluster data.</li> <li>Scheduler: Assigns workloads to nodes based on resource availability.</li> <li>Controller Manager: Runs controllers to regulate the state of the cluster.</li> <li>Kubelet: Ensures containers are running in a Pod on each node.</li> </ul> Services <p>Services provide stable networking endpoints for Pods, enabling reliable communication between different parts of an application. They abstract away the ephemeral nature of Pods, which can be created and destroyed dynamically, and give you a stable, long-lived connection point to the underlying Pods.</p>"},{"location":"overview/#common-features-primer","title":"Common Features PrimerSummary","text":"Pods and Deployments <ul> <li>Pods: The smallest deployable units in Kubernetes, which can contain one or more containers. Containers within Pods share resources like network and storage.</li> <li>Deployments: Higher-level controllers that manage Pods, providing features like scaling, rolling updates, and rollbacks.</li> </ul> Self-Healing and Scaling <p>If you use a Deployment or StatefulSet, Kubernetes will:</p> <ul> <li>Replace failed Pods automatically</li> <li>Scale your app up or down based on demand</li> <li>Keep your app highly available and efficient</li> </ul> Rolling Updates and Rollbacks <p>With Deployments, Kubernetes can update your app with zero downtime - gradually swapping out old Pods for new ones. If a problem is detected, it can roll back to the previous version automatically.</p> <p>Kubernetes is all about automation, reliability, and making sure your apps run the way you want. By describing your desired state, Kubernetes does the heavy lifting to keep everything running smoothly - so you can focus on building, not babysitting, your infrastructure.</p>"},{"location":"pods-deployments/","title":"Pods & Deployments","text":"<p>Kubernetes is often described as an \"Operating System for the Cloud.\" If that is true, then Pods are the processes, and Deployments are the init system (like systemd) that keeps them running.</p> <p>This is the most foundational concept in Kubernetes. If you understand this, everything else (Services, Ingress, ConfigMaps) will make sense.</p>"},{"location":"pods-deployments/#what-is-a-pod","title":"What is a Pod?","text":"<p>A Pod is the smallest execution unit in Kubernetes.</p> <p>Newcomers often ask: \"Why doesn't Kubernetes just run containers directly? Why do we need this 'Pod' wrapper?\"</p> <p>The answer is Shared Context. Sometimes, you need multiple containers to work together tightly - like they are running on the same physical server.</p> <p>A Pod allows multiple containers to run inside a shared environment where they:</p> <ul> <li>Share the same IP Address: They can talk to each other using <code>localhost</code>.</li> <li>Share Storage: They can mount the same Volumes to read/write shared files.</li> <li>Share Lifecycle: They are scheduled, started, and killed together.</li> </ul>"},{"location":"pods-deployments/#visualizing-the-pod","title":"Visualizing the Pod","text":"<p>The diagram below illustrates how multiple containers coexist in a single Pod.</p> <p> </p> <p>Notice three critical things in this diagram:</p> <ol> <li>One IP Address: The entire Pod has the IP <code>10.0.0.5</code>.</li> <li>Localhost Communication: \"Container 1\" can talk to \"Container 2\" just by calling <code>localhost:1717</code>. They don't need fancy networking.</li> <li>Shared Files: Both containers can read/write to the same shared \"filesystem\" volume.</li> </ol> <p>The \"One Process Per Container\" Rule</p> <p>Even though you can put multiple containers in a Pod, you usually shouldn't. 95% of Pods contain just one container. Only add a second container (a \"Sidecar\") if it is a helper process (like a log shipper or proxy) that strictly supports the main app.</p>"},{"location":"pods-deployments/#what-is-a-deployment","title":"What is a Deployment?","text":"<p>If a Pod is a worker, the Deployment is the Manager.</p> <p>Pods are mortal. They are designed to die.</p> <ul> <li>If a Node runs out of memory, it kills a Pod.</li> <li>If a Node crashes, the Pod is lost forever.</li> <li>If you want to update your app, you must kill the old Pod and start a new one.</li> </ul> <p>You never want to manage this manual lifecycle yourself. Instead, you create a Deployment.</p> <p>A Deployment ensures that a specified number of Pods (Replicas) are running at all times, matching the Desired State you defined.</p>"},{"location":"pods-deployments/#the-self-healing-loop","title":"The \"Self-Healing\" Loop","text":"<pre><code>graph TD\n    User[You] --&gt;|Apply YAML| API[API Server]\n    API --&gt;|Desired: 3 Replicas| Controller[Deployment Controller]\n    Controller --&gt;|Check Current State| Cluster\n\n    subgraph Cluster\n    Pod1[Pod 1]\n    Pod2[Pod 2]\n    end\n\n    Controller -- \"Only 2 found! Start 1 more!\" --&gt; Pod3[create Pod 3]</code></pre> <ol> <li>You: \"I want 3 copies of nginx.\"</li> <li>Deployment: \"I see 0 copies. I will create 3.\"</li> <li>Disaster: One node crashes, taking a Pod with it.</li> <li>Deployment: \"I see 2 copies. I need 3. Creating a replacement immediately.\"</li> </ol>"},{"location":"pods-deployments/#rolling-updates-zero-downtime","title":"Rolling Updates (Zero Downtime)","text":"<p>The other superpower of Deployments is updates.</p> <p>When you change the image version in a Deployment (e.g., <code>v1</code> to <code>v2</code>), it doesn't kill everything at once. It performs a Rolling Update:</p> <ol> <li>It spins up one new Pod (v2).</li> <li>It waits for it to become \"Ready.\"</li> <li>It kills one old Pod (v1).</li> <li>It repeats this until all Pods are on v2.</li> </ol> <p>If the new version crashes on startup, the Deployment stops the rollout automatically, ensuring you don't take down your production site with a bad update.</p>"},{"location":"pods-deployments/#creating-them-the-modern-way","title":"Creating Them (The Modern Way)","text":"<p>Don't write YAML from scratch. Use the CLI to generate it for you.</p>"},{"location":"pods-deployments/#1-create-a-deployment","title":"1. Create a Deployment","text":"<p>This is the standard command to launch an app.</p> <pre><code># syntax: kubectl create deployment &lt;name&gt; --image=&lt;image&gt;\nkubectl create deployment my-web --image=nginx --replicas=3\n</code></pre>"},{"location":"pods-deployments/#2-create-a-naked-pod-rare","title":"2. Create a \"naked\" Pod (Rare)","text":"<p>Use this only for debugging.</p> <pre><code># syntax: kubectl run &lt;name&gt; --image=&lt;image&gt;\nkubectl run my-debug-pod --image=busybox\n</code></pre> <p>Don't use <code>kubectl run</code> for applications!</p> <p><code>kubectl run</code> creates a Pod without a Deployment. If that Pod crashes, it stays dead. It will not self-heal.</p>"},{"location":"pods-deployments/#the-hidden-layer-replicasets","title":"The \"Hidden\" Layer: ReplicaSets","text":"<p>Technically, Deployments don't manage Pods directly. They manage a middleman called a ReplicaSet.</p> <ul> <li>Deployment: Manages updates and rollouts.</li> <li>ReplicaSet: Ensures X number of pods are running.</li> <li>Pod: Runs the container.</li> </ul> <p>You rarely touch ReplicaSets directly, but you will see them when you run <code>kubectl get all</code>.</p> <pre><code>NAME                          READY   STATUS    AGE\npod/my-web-6d4b-xyz           1/1     Running   5m  &lt;-- The Pod\n\nNAME                          READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/my-web        1/1     1            1           5m  &lt;-- The Manager\n\nNAME                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/my-web-6d4b   1         1         1       5m  &lt;-- The Counter\n</code></pre>"},{"location":"pods-deployments/#summary","title":"Summary","text":"Feature Pod Deployment Purpose Run a container Manage a fleet of Pods Lifespan Ephemeral (Dies easily) Long-lived (Self-healing) Scalability None (1 instance) Horizontal (Change <code>replicas</code>) Updates Impossible (Must delete &amp; recreate) Rolling Updates (Zero downtime) Production? No (Debug only) Yes (Always use this) <p>Key Takeaway: Pods are the \"atoms\" of Kubernetes, but Deployments are the \"molecules\" you actually work with. Always use a Deployment to keep your Pods healthy, scalable, and up-to-date.</p>"},{"location":"probes/","title":"Health Probes","text":"<p>Distributed systems are messy. Apps hang, freeze, or take 3 minutes to warm up. Kubernetes needs to know the difference between \"I'm busy starting up\" and \"I am dead, please restart me.\"</p> <p>That is where Probes come in. They are the health checks that let Kubernetes automate the repair and routing of your applications.</p>"},{"location":"probes/#the-traffic-light-analogy","title":"The \"Traffic Light\" Analogy","text":"<p>Think of your Pod like a car trying to enter a highway.</p> <ol> <li>Startup Probe (The Ignition): \"Has the engine started?\"<ul> <li>If No: Keep turning the key. Don't check anything else yet.</li> <li>If Yes: Okay, move on to the other checks.</li> </ul> </li> <li>Readiness Probe (The Green Light): \"Is it safe to merge into traffic?\"<ul> <li>If No: Stop sending cars (requests) to this vehicle. Let it idle on the shoulder until it's ready. Do not kill it.</li> <li>If Yes: Add its IP to the Service load balancer.</li> </ul> </li> <li>Liveness Probe (The Heartbeat): \"Is the driver still conscious?\"<ul> <li>If No: The driver has had a heart attack. Call the ambulance (Restart the Pod).</li> <li>If Yes: Keep driving.</li> </ul> </li> </ol>"},{"location":"probes/#1-startup-probe-the-slow-starter","title":"1. Startup Probe (The \"Slow Starter\")","text":"<p>Purpose: Protect slow-starting apps from being killed by the Liveness Probe.</p> <p>Legacy Java or Windows apps might take 2-3 minutes to boot. If you set a Liveness Probe to check every 10 seconds, it will kill the app before it ever finishes booting.</p> <p>The Startup Probe pauses all other probes until it passes once.</p> <pre><code>startupProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  # Allow up to 5 minutes (30 * 10s) for startup\n  failureThreshold: 30\n  periodSeconds: 10\n</code></pre>"},{"location":"probes/#2-readiness-probe-the-traffic-controller","title":"2. Readiness Probe (The \"Traffic Controller\")","text":"<p>Purpose: Control whether the Pod receives traffic from the Service.</p> <p>Use this to handle temporary load spikes or \"warm-up\" periods. If a dependency (like a database) is momentarily unreachable, you might want your app to fail readiness so traffic stops flowing to it, but you don't want to restart the whole Pod.</p> <ul> <li>Failure Action: Remove Pod IP from the Service endpoints. (No restart).</li> </ul> <pre><code>readinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 10\n</code></pre>"},{"location":"probes/#3-liveness-probe-the-defibrillator","title":"3. Liveness Probe (The \"Defibrillator\")","text":"<p>Purpose: Catch deadlocks or frozen processes.</p> <p>If your app has a bug where it enters an infinite loop and stops responding, a Liveness Probe detects this and restarts the container to reset the state.</p> <ul> <li>Failure Action: RESTART the container.</li> </ul> <pre><code>livenessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 15\n  periodSeconds: 20\n</code></pre> <p>The Liveness Loop of Death</p> <p>Be very careful with Liveness Probes. If your <code>initialDelaySeconds</code> is too short, Kubernetes will kill your app before it finishes booting. It will then restart, fail again, and restart again forever. Always use a Startup Probe for slow apps.</p>"},{"location":"probes/#probe-mechanisms-how-to-check","title":"Probe Mechanisms: How to Check?","text":"<p>You can check health in three ways. Choose the one that matches your app's architecture.</p> Type How it works Best For HTTP Get K8s sends a GET request. Codes 200-399 are \"Success\". Web Servers, APIs TCP Socket K8s tries to open a TCP connection to the port. Databases, Redis, Non-HTTP apps Exec Command K8s runs a command inside the container. Exit code 0 is \"Success\". Apps that write status files, CLI tools gRPC K8s sends a standard gRPC Health Check request. gRPC Microservices"},{"location":"probes/#example-exec-probe-checking-a-file","title":"Example: Exec Probe (Checking a File)","text":"<pre><code>livenessProbe:\n  exec:\n    command:\n    - cat\n    - /tmp/healthy\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>"},{"location":"probes/#tuning-your-probes","title":"Tuning Your Probes","text":"<p>Don't stick with the defaults. Tune these fields to match your application's reality.</p> <ul> <li><code>initialDelaySeconds</code>: How long to wait before the first check.</li> <li><code>periodSeconds</code>: How often to check. (Default: 10s).</li> <li><code>timeoutSeconds</code>: How long to wait for a response before calling it a \"failure\". (Default: 1s).</li> <li><code>failureThreshold</code>: How many times it must fail before taking action. (Default: 3). \"Flapping\" protection.</li> </ul>"},{"location":"probes/#visualizing-the-decision-logic","title":"Visualizing the Decision Logic","text":"<pre><code>graph TD\n    Start[Pod Starts] --&gt; Startup{Startup Probe&lt;br/&gt;Passed?}\n\n    Startup -- No --&gt; Wait[Wait / Check Again]\n    Startup -- Fails limit --&gt; Kill[Kill &amp; Restart]\n\n    Startup -- Yes --&gt; Phase2[Start Main Loops]\n\n    subgraph \"Main Loop\"\n    Phase2 --&gt; LiveCheck{Liveness&lt;br/&gt;Probe Passed?}\n    Phase2 --&gt; ReadyCheck{Readiness&lt;br/&gt;Probe Passed?}\n\n    LiveCheck -- No --&gt; Kill\n\n    ReadyCheck -- Yes --&gt; Service[Add to Service/Traffic]\n    ReadyCheck -- No --&gt; Block[Remove from Service]\n    end</code></pre>"},{"location":"probes/#summary","title":"Summary","text":"<ul> <li>Startup Probes are for slow-booting legacy apps.</li> <li>Readiness Probes determine if the Pod should get traffic.</li> <li>Liveness Probes determine if the Pod should be restarted.</li> <li>Use HTTP checks for web apps and TCP checks for databases.</li> <li>Never make your Readiness probe depend on an external service (like \"Is https://www.google.com/ up?\"). If the internet blips, you will take down your entire cluster.</li> </ul>"},{"location":"psa/","title":"Pod Security (PSA)","text":"<p>For years, Kubernetes used PodSecurityPolicy (PSP) to lock down pods. It was complex, buggy, and confusing. In v1.25, it was completely removed.</p> <p>The replacement is Pod Security Admission (PSA). It is built-in, on by default, and much easier to use. It relies on the Pod Security Standards (PSS) - a predefined set of rules maintained by the Kubernetes community.</p>"},{"location":"psa/#the-club-bouncer-analogy","title":"The \"Club Bouncer\" Analogy","text":"<p>Think of PSA as the bouncer at the door of a nightclub (Namespace). You tell the bouncer what the \"Dress Code\" (Policy Level) is for that room.</p> <ol> <li>Privileged (The VIP Room): Anything goes. You can come in wearing pajamas, carrying weapons (root access), or acting wild.</li> <li>Baseline (The General Floor): Standard dress code. No obvious threats allowed, but we won't inspect your socks. (Prevents known privilege escalations).</li> <li>Restricted (The Black Tie Gala): Extremely strict. You must have a tie, polished shoes, and perfect behavior. (Follows hard-core hardening best practices).</li> </ol>"},{"location":"psa/#1-the-three-levels-the-standards","title":"1. The Three Levels (The Standards)","text":"<p>You cannot invent your own rules anymore. You must pick one of these three standard profiles.</p> Level Goal Use Case Privileged Unrestricted. Allows known privilege escalations. System agents (CNI, Storage Drivers), Logging agents. Baseline Minimally Restrictive. Prevents known escalations. Standard applications, web servers, microservices. (90% of use cases). Restricted Hardened. Requires pods to explicitly drop capabilities and run as non-root. High-security financial/gov apps."},{"location":"psa/#2-the-three-modes-the-action","title":"2. The Three Modes (The Action)","text":"<p>What happens when someone breaks the dress code?</p> Mode Action When to use? Enforce Block the Pod. Return 403 Forbidden. Production (after testing). Audit Log the violation to the audit log, but allow the pod. Monitoring &amp; Compliance. Warn Tell the user immediately in the CLI, but allow the pod. Developer feedback loop."},{"location":"psa/#visualizing-the-flow","title":"Visualizing the Flow","text":"<pre><code>graph TD\n    User[Developer] --&gt;|kubectl apply| API[API Server]\n    API --&gt;|Check Namespace Labels| PSA[PSA Controller]\n\n    PSA -- \"Namespace is 'restricted'?\" --&gt; Check{Check Pod Spec}\n\n    Check -- \"Pod runs as Root!\" --&gt; Violation\n\n    Violation -- \"Mode: Enforce\" --&gt; Block[Deny: 403 Forbidden]\n    Violation -- \"Mode: Warn\" --&gt; Message[Warning: This pod violates policy...]\n\n    Message --&gt; Allow[Allow Pod]</code></pre>"},{"location":"psa/#3-how-to-configure-namespace-labels","title":"3. How to Configure (Namespace Labels)","text":"<p>PSA is controlled entirely via Namespace Labels. You don't need to create any CRDs or objects.</p>"},{"location":"psa/#the-syntax","title":"The Syntax","text":"<pre><code>pod-security.kubernetes.io/&lt;MODE&gt;=&lt;LEVEL&gt;\npod-security.kubernetes.io/&lt;MODE&gt;-version=&lt;VERSION&gt;\n</code></pre>"},{"location":"psa/#practical-example-the-safe-rollout","title":"Practical Example: The Safe Rollout","text":"<p>You want to enforce <code>restricted</code> security, but you are afraid of breaking the app.</p> <p>Step 1: Turn on Warnings &amp; Audit (Dry Run)</p> <pre><code>kubectl label namespace backend \\\n  pod-security.kubernetes.io/enforce=baseline \\\n  pod-security.kubernetes.io/warn=restricted \\\n  pod-security.kubernetes.io/audit=restricted\n</code></pre> <p>Result: The app runs (Baseline enforcement), but if the developer tries to deploy something that isn't fully \"Restricted\" compliant, they get a warning in their terminal.</p> <p>Step 2: Fix the Warnings The developer updates their <code>securityContext</code> in the YAML to meet the Restricted standard.</p> <p>Step 3: Enforce Restricted</p> <pre><code>kubectl label namespace backend \\\n  pod-security.kubernetes.io/enforce=restricted \\\n  pod-security.kubernetes.io/warn=restricted --overwrite\n</code></pre>"},{"location":"psa/#4-the-versioning-trap-critical","title":"4. The Versioning Trap (Critical!)","text":"<p>You will notice the label <code>enforce-version</code>. You can set this to <code>latest</code> or a specific version like <code>v1.28</code>.</p> <p>Avoid <code>latest</code> in Production</p> <p>Kubernetes security standards change. A setting that is allowed in <code>Restricted v1.25</code> might be banned in <code>Restricted v1.29</code>.</p> <p>If you use <code>enforce-version=latest</code>, and you upgrade your cluster, your existing valid pods might suddenly fail to restart because the rules changed under your feet.</p> <p>Best Practice: Pin the version to your current cluster version (e.g., <code>v1.29</code>) and manually update it only when you have verified your apps against the new rules.</p>"},{"location":"psa/#5-troubleshooting-what-a-block-looks-like","title":"5. Troubleshooting: What a Block Looks Like","text":"<p>If you try to deploy a standard Nginx pod into a Restricted namespace, it will fail because Nginx tries to run as <code>root</code> by default, and it doesn't drop capabilities.</p> <p>Command: <code>kubectl apply -f nginx.yaml</code></p> <p>Error Output:</p> <pre><code>Error from server (Forbidden): error when creating \"nginx.yaml\": \npods \"nginx\" is forbidden: violates PodSecurity \"restricted:v1.29\": \n- (container \"nginx\" must not set securityContext.allowPrivilegeEscalation=true)\n- (container \"nginx\" must set securityContext.runAsNonRoot=true)\n- (container \"nginx\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n</code></pre> <p>The Fix: You must update your Pod YAML to explicitly adhere to the rules:</p> <pre><code>spec:\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: nginx\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n</code></pre>"},{"location":"psa/#summary","title":"Summary","text":"<ul> <li>PSA is the standard, built-in way to secure pods.</li> <li>Levels: <code>Privileged</code> (Unsafe), <code>Baseline</code> (Standard), <code>Restricted</code> (Hardened).</li> <li>Modes: <code>Enforce</code> (Block), <code>Audit</code> (Log), <code>Warn</code> (Notify).</li> <li>Configuration: Done strictly via labels on the Namespace.</li> <li>Production Tip: Pin your versions (don't use <code>latest</code>) and use <code>Warn/Audit</code> modes before switching <code>Enforce</code> to a stricter level.</li> </ul>"},{"location":"quiz/","title":"Quiz","text":"Which command lists all Pods in the current namespace?<code>kubectl get pods</code> [why: This is the standard command to list all Pods in the current namespace.]<code>kubectl list pods</code> [why: There is no <code>list</code> subcommand in <code>kubectl</code>.]<code>kubectl pods</code> [why: <code>pods</code> is a resource, not a standalone command.]<code>kubectl ps</code> [why: <code>ps</code> is a Linux process listing command, not a Kubernetes command.]Submit  The <code>kubectl get pods</code> command lists all Pods in the current namespace by default. Add <code>-A</code> to show Pods across all namespaces. Which object is responsible for ensuring the desired number of Pods are running?DaemonSet [why: Ensures one Pod per node, not a specific count.]StatefulSet [why: Manages ordered, persistent Pods but not generic counts.]ReplicaSet [why: It maintains a specified number of identical Pods by creating or deleting them as needed.]Job [why: Ensures completion of tasks, not persistent Pod count.]Submit  ReplicaSets monitor and maintain the number of replicas specified in their configuration to match the desired state. What does the <code>kubectl describe pod</code> command do?Shows detailed information about a specific Pod, including events and status [why: It provides verbose details like labels, containers, IPs, and recent events.]Shows only Pod names [why: That\u2019s what <code>kubectl get pods</code> does, not <code>describe</code>.]Deletes the Pod [why: The <code>delete</code> command is used to remove resources.]Creates a new Pod [why: <code>apply</code> or <code>create</code> are used to add resources, not <code>describe</code>.]Submit <code>kubectl describe</code> provides detailed resource information useful for debugging Pod failures or scheduling issues. Which controller ensures a Pod runs on every node?ReplicaSet [why: Ensures a set number of Pods exist globally, not per node.]Deployment [why: Manages ReplicaSets and rolling updates, not one-per-node Pods.]DaemonSet [why: Deploys one Pod per node, commonly used for logging and monitoring agents.]StatefulSet [why: Provides stable identifiers and volumes, not per-node deployment.]Submit  DaemonSets are ideal for node-level agents such as log shippers, metrics collectors, and network tools. What is the smallest deployable unit in Kubernetes?Pod [why: A Pod is the smallest unit that can be deployed and managed in Kubernetes.]Node [why: A node runs Pods but is not deployed via Kubernetes workloads.]Container [why: Containers are part of Pods; Pods are the actual deployable units.]ReplicaSet [why: Controls Pods but isn\u2019t itself a workload unit.]Submit  Pods encapsulate one or more containers, storage, and networking resources as the basic execution unit. What is the default namespace in Kubernetes?kube-system [why: Reserved for system-level components.]kube-public [why: Used for cluster-wide public resources.]default [why: User-created resources without a specified namespace go here by default.]kube-node-lease [why: Used for node heartbeats, not user workloads.]Submit  The <code>default</code> namespace is where Kubernetes places user workloads when no namespace is specified. Which command shows the cluster\u2019s API resources?<code>kubectl api-resources</code> [why: Lists all resource kinds supported by the API server.]<code>kubectl get api</code> [why: There\u2019s no such subcommand.]<code>kubectl describe api</code> [why: Doesn\u2019t list resource types; used for describing specific resources.]<code>kubectl resources</code> [why: Not a valid command.]Submit <code>kubectl api-resources</code> displays all resource types and their short names, namespaces, and API groups. Which object provides stable network identity for a set of Pods?Deployment [why: Ensures rollout of Pods, not networking identity.]Service [why: Provides a consistent virtual IP and DNS name for Pods.]ReplicaSet [why: Manages Pod replicas but doesn\u2019t handle networking.]Ingress [why: Handles external HTTP/S access, not internal service discovery.]Submit  Services provide stable internal networking and load balancing for Pods, even when Pods are replaced. Which Kubernetes resource is used to store confidential data?Secret [why: Stores sensitive data like tokens or passwords, encoded and mounted securely.]ConfigMap [why: Stores non-sensitive configuration data.]PersistentVolume [why: Provides storage, not configuration or secrets.]Role [why: Used for RBAC permissions, not data storage.]Submit  Secrets keep sensitive information separate from code, and can be injected into Pods via environment variables or volumes. Which file defines how a Kubernetes object should be created?Manifest file [why: A YAML or JSON file describing object metadata, spec, and desired state.]kubeconfig [why: Configures client connection, not objects.]Dockerfile [why: Builds container images, not Kubernetes resources.]ServiceAccount [why: Controls access, not object definition.]Submit  Kubernetes manifests define resources declaratively so they can be version-controlled and applied with <code>kubectl apply</code>. What is the purpose of <code>kubectl apply</code>?Deletes a resource [why: <code>delete</code> removes resources, not <code>apply</code>.]Creates or updates resources declaratively from manifests [why: It compares desired vs actual state and reconciles them.]Lists resources [why: <code>get</code> lists, <code>apply</code> manages configuration.]Runs a Pod interactively [why: <code>run</code> is used to launch a Pod directly.]Submit <code>kubectl apply</code> declaratively manages configuration, enabling continuous reconciliation of resource definitions. What is the default network model in Kubernetes?Flat, routable Pod network [why: All Pods can communicate directly without NAT by default.]NAT-based network [why: Kubernetes avoids NAT between Pods for simplicity.]Segmented per-node network [why: Not the default; overlays or policies can add segmentation.]External-only network [why: Pods need internal networking to function.]Submit  Kubernetes networking assumes every Pod can reach every other Pod directly using its IP. Which command displays cluster node information?<code>kubectl get pods</code> [why: Lists Pods, not nodes.]<code>kubectl get nodes</code> [why: Shows node names, status, roles, and versions.]<code>kubectl describe cluster</code> [why: Not a valid command.]<code>kubectl show nodes</code> [why: No <code>show</code> command in <code>kubectl</code>.]Submit <code>kubectl get nodes</code> lists the nodes registered in your cluster along with their status and roles. Which controller ensures completed Pods don\u2019t restart?Deployment [why: Used for long-running workloads.]ReplicaSet [why: Ensures Pod count, not job completion.]Job [why: Runs Pods to completion and doesn\u2019t restart successful ones.]StatefulSet [why: Manages ordered Pods with persistent identity.]Submit  Jobs are ideal for one-time tasks like batch processing or migrations. What is the difference between a ReplicaSet and a Deployment?They\u2019re identical [why: A Deployment manages ReplicaSets but adds rollout control.]Deployment manages ReplicaSets and handles rolling updates [why: Deployments add versioning, rollback, and declarative updates.]ReplicaSet handles rollbacks [why: It doesn\u2019t; Deployments do.]ReplicaSet manages multiple Deployments [why: Reverse relationship; Deployments manage ReplicaSets.]Submit  Deployments provide a higher-level abstraction for updates and rollbacks on top of ReplicaSets. Which Kubernetes component schedules Pods to nodes?kubelet [why: Runs Pods on nodes but doesn\u2019t schedule them.]kube-scheduler [why: Assigns Pods to nodes based on constraints and resources.]kube-controller-manager [why: Handles replication and other controllers, not scheduling.]etcd [why: Key-value store for cluster state, not scheduling.]Submit  The kube-scheduler evaluates resource availability and assigns Pods to suitable nodes. Which API resource provides cluster configuration for users?ConfigMap [why: Stores configuration for applications, not cluster access.]Secret [why: Stores sensitive data, not kubeconfig information.]kubeconfig [why: Defines cluster connection details, authentication, and context for users.]ServiceAccount [why: Grants in-cluster access for Pods, not external users.]Submit <code>kubeconfig</code> files store credentials, cluster API URLs, and contexts for user access. What is a ServiceAccount used for?Assigning RBAC roles to users [why: RBAC Roles are bound to Subjects; ServiceAccounts are for Pods.]Providing Pods with in-cluster identity [why: Enables Pods to authenticate with the API server.]Managing network policies [why: NetworkPolicies define traffic rules, not access credentials.]Storing environment variables [why: That\u2019s done via ConfigMaps or Secrets.]Submit  ServiceAccounts are automatically mounted into Pods to provide secure in-cluster authentication tokens. What does <code>kubectl logs</code> show?Output from a container running in a Pod [why: Displays standard output and error logs for debugging.]Event history for a Pod [why: <code>kubectl describe pod</code> shows events.]Node system logs [why: Node logs are external to <code>kubectl logs</code>.]Resource usage metrics [why: Use <code>kubectl top</code> for metrics.]Submit <code>kubectl logs</code> retrieves container logs directly from the Kubernetes API. Which command runs a temporary Pod for debugging?<code>kubectl logs</code> [why: Shows logs but doesn\u2019t create Pods.]<code>kubectl run -it --rm</code> [why: Creates an interactive Pod that deletes itself afterward.]<code>kubectl debug node</code> [why: Used to debug node issues, not ephemeral Pods.]<code>kubectl exec -it</code> [why: Executes into existing Pods, doesn\u2019t create new ones.]Submit <code>kubectl run -it --rm</code> launches a throwaway interactive container useful for quick debugging. Which command allows you to execute a command inside a running Pod?<code>kubectl exec -it pod-name -- command</code> [why: Executes an interactive shell or command inside an existing container.]<code>kubectl run</code> [why: Creates a new Pod; does not exec into an existing one.]<code>kubectl attach</code> [why: Attaches to output of the main process; doesn\u2019t start a new command.]<code>kubectl connect</code> [why: Not a valid kubectl subcommand.]Submit <code>kubectl exec</code> is used to run commands inside containers for debugging or manual inspection. What is the role of the kubelet?Ensures containers described in PodSpecs are running [why: It monitors Pods on its node and reports status to the API server.]Assigns Pods to nodes [why: That\u2019s the scheduler\u2019s job.]Stores cluster state [why: etcd stores state, not the kubelet.]Controls networking between nodes [why: Managed by CNI plugins, not kubelet.]Submit  The kubelet is the node agent that makes sure containers are healthy and running as expected. Which component stores the cluster\u2019s configuration and state?kube-scheduler [why: Handles scheduling, not persistent storage.]etcd [why: Serves as the distributed key-value store backing all cluster data.]kubelet [why: Manages Pods on nodes, not cluster state.]API server [why: Fronts the control plane but persists state in etcd.]Submit  etcd stores all configuration and state data that define the cluster\u2019s desired and current state. Which command lists available contexts in your kubeconfig?<code>kubectl get contexts</code> [why: Not a valid subcommand; use config view or get-contexts.]<code>kubectl config get-contexts</code> [why: Lists contexts with cluster, user, and namespace info.]<code>kubectl show-contexts</code> [why: Doesn\u2019t exist.]<code>kubectl config list</code> [why: Not a valid config verb.]Submit <code>kubectl config get-contexts</code> shows all contexts defined in your kubeconfig file. Which type of Service exposes an application on a static IP outside the cluster?ClusterIP [why: Exposes only internally within the cluster.]NodePort [why: Exposes on each node\u2019s port, not a stable external IP.]LoadBalancer [why: Provisions an external IP via the cloud provider for external access.]ExternalName [why: Maps a DNS name to an external service, not a load balancer IP.]Submit  LoadBalancer Services integrate with cloud provider APIs to expose apps externally with stable IPs. Which Kubernetes object defines access rules within a namespace?Role [why: Grants permissions to resources within a single namespace.]ClusterRole [why: Applies cluster-wide, not limited to a namespace.]ServiceAccount [why: Represents an identity, not permissions.]ConfigMap [why: Stores configuration data, not permissions.]Submit  Roles specify allowed actions on resources within a namespace, paired with RoleBindings. What is the difference between a Role and a ClusterRole?Role is cluster-wide [why: False; Role is namespace-scoped.]ClusterRole is namespace-scoped [why: Incorrect; it\u2019s cluster-scoped.]Role applies to one namespace, ClusterRole applies cluster-wide [why: That\u2019s the correct scope distinction.]They are identical [why: They differ in their scope and where they\u2019re bound.]Submit  ClusterRoles grant permissions across all namespaces; Roles are confined to a single namespace. What does a ConfigMap store?Non-sensitive key-value configuration data [why: Used for environment variables, config files, and command arguments.]Secrets [why: Secrets store sensitive data separately.]Pod logs [why: Logs are transient output, not configuration.]Node metrics [why: Metrics are runtime data, not configuration.]Submit  ConfigMaps separate configuration from container images to improve portability. Which command deletes a resource?<code>kubectl stop</code> [why: Deprecated command; replaced by <code>delete</code>.]<code>kubectl delete</code> [why: Removes resources specified by name, file, or label selector.]<code>kubectl remove</code> [why: Not a valid kubectl subcommand.]<code>kubectl clear</code> [why: Doesn\u2019t exist; deletion handled by <code>delete</code>.]Submit <code>kubectl delete</code> cleanly removes resources, triggering appropriate cleanup controllers. Which controller manages the rollout and rollback of application versions?Deployment [why: Automates rolling updates and version rollbacks using ReplicaSets.]ReplicaSet [why: Manages Pods but not version history.]StatefulSet [why: Focuses on ordered deployment with persistence.]DaemonSet [why: Ensures one Pod per node, not version control.]Submit  Deployments abstract ReplicaSets to enable declarative versioned updates to applications. Which Kubernetes resource defines how Pods communicate externally via HTTP/HTTPS?Service [why: Provides stable internal networking but doesn\u2019t manage HTTP routing.]Ingress [why: Manages HTTP routing, SSL termination, and host/path-based rules.]ConfigMap [why: Used for configuration, not networking routes.]Role [why: Manages permissions, not traffic.]Submit  Ingress controllers handle inbound HTTP/S traffic routing to Services inside the cluster. What does the <code>kubectl top</code> command display?CPU and memory usage for Pods or nodes [why: Uses Metrics Server to show resource utilization.]Disk usage [why: Kubernetes doesn\u2019t report disk stats with <code>top</code>.]Events [why: Use <code>kubectl get events</code> for that.]Logs [why: Use <code>kubectl logs</code> instead.]Submit <code>kubectl top</code> helps monitor resource consumption for capacity planning and performance debugging. Which resource defines persistent storage in Kubernetes?PersistentVolume [why: Abstracts storage backend and provides lifecycle-managed storage resources.]ConfigMap [why: Stores configuration, not storage.]Pod [why: Consumes storage, doesn\u2019t define it.]Service [why: Provides networking, not storage.]Submit  PersistentVolumes decouple storage provisioning from Pods, allowing reuse across workloads. Which resource requests specific storage for a Pod?PersistentVolume [why: Defines storage supply, not demand.]PersistentVolumeClaim [why: Represents a request for storage by a Pod.]StorageClass [why: Defines dynamic provisioning behavior, not the claim itself.]Secret [why: Stores credentials, not storage requests.]Submit  PersistentVolumeClaims abstract how Pods request storage independently from the underlying infrastructure. What is a StatefulSet primarily used for?Managing stateful applications needing stable identity [why: Provides predictable names and persistent volumes per Pod.]Stateless workloads [why: Use Deployments for that.]Batch jobs [why: Use Jobs or CronJobs for that purpose.]Node agents [why: DaemonSets run node-level agents.]Submit  StatefulSets guarantee stable Pod names and persistent storage for databases and stateful workloads. What does a Kubernetes Taint do?Prevents Pods from being scheduled unless tolerated [why: It marks nodes to repel certain Pods unless a matching Toleration exists.]Forces Pods onto a node [why: That\u2019s an Affinity rule, not a Taint.]Deletes unresponsive nodes [why: Node controller handles that, not Taints.]Changes Pod priority [why: PriorityClasses handle scheduling priority.]Submit  Taints and Tolerations work together to control which Pods can be scheduled on specific nodes. Which concept defines Pod scheduling preference rather than enforcement?Taint [why: Prevents scheduling unless tolerated.]Node Affinity [why: Expresses soft scheduling preferences that the scheduler tries to honor.]Toleration [why: Allows Pods onto tainted nodes but doesn\u2019t express preference.]Selector [why: Filters resources, doesn\u2019t define preference strength.]Submit  Preferred affinities allow gentle steering of Pods toward specific nodes without hard enforcement. Which command upgrades a running Deployment to a new image?<code>kubectl set image deployment/myapp mycontainer=newimage:tag</code> [why: Updates the container image field in the Deployment.]<code>kubectl rollout undo</code> [why: Rolls back to a previous version, not upgrade.]<code>kubectl apply -f pod.yaml</code> [why: Creates or updates individual Pods; not used for Deployment rolling updates.]<code>kubectl edit node</code> [why: Modifies node config, not workloads.]Submit  Use <code>kubectl set image</code> to trigger a rolling update for Deployments. Which object ensures recurring job execution on a schedule?Job [why: Runs once to completion, not scheduled.]CronJob [why: Wraps Jobs and schedules them using cron syntax.]Deployment [why: Maintains long-running Pods, not scheduled Jobs.]DaemonSet [why: Runs Pods per node, not per time schedule.]Submit  CronJobs are ideal for periodic tasks like backups, cleanup, or reports. Which resource defines policies controlling network traffic between Pods?Role [why: Manages RBAC, not networking.]NetworkPolicy [why: Specifies allowed ingress and egress traffic between Pods.]ConfigMap [why: Holds configuration, not firewall rules.]Ingress [why: Handles external traffic, not Pod-to-Pod security.]Submit  NetworkPolicies enable fine-grained network segmentation and zero-trust design inside clusters. What is the function of an Admission Controller?Intercepts API requests to enforce policies or mutate objects [why: Validates and modifies requests before persistence in etcd.]Schedules Pods [why: Scheduler handles placement, not admission.]Controls node networking [why: Not its function; CNI handles that.]Manages user authentication [why: Handled earlier by API server authentication chain.]Submit  Admission Controllers enforce governance, security, and compliance at resource creation time. What does <code>kubectl rollout undo</code> do?Reverts a Deployment to its previous revision [why: It rolls back to the last successful ReplicaSet version.]Deletes the Deployment [why: That\u2019s <code>kubectl delete</code>.]Pauses a rollout [why: <code>kubectl rollout pause</code> handles that.]Shows rollout status [why: <code>kubectl rollout status</code> reports progress.]Submit  Rollback restores a Deployment\u2019s prior configuration, useful after a bad release. Which field in a Pod manifest specifies resource limits?<code>resources.limits</code> [why: Defines maximum CPU/memory usage per container.]<code>spec.replicas</code> [why: Used in Deployments, not Pods.]<code>spec.containers.env</code> [why: Defines environment variables, not resources.]<code>spec.volumes</code> [why: Defines volumes, not resource constraints.]Submit  Resource limits prevent any single container from consuming excessive CPU or memory. Which command shows all API versions supported by the cluster?<code>kubectl api-versions</code> [why: Lists API groups and versions exposed by the API server.]<code>kubectl get versions</code> [why: Not a valid subcommand.]<code>kubectl version</code> [why: Shows client/server version info, not API groups.]<code>kubectl get api</code> [why: Invalid subcommand.]Submit <code>kubectl api-versions</code> lists available API groups to verify supported resource versions. Which resource defines storage provisioning templates?StorageClass [why: Describes how volumes are dynamically provisioned and reclaimed.]PersistentVolumeClaim [why: Requests storage, doesn\u2019t define how to create it.]Secret [why: Stores sensitive data, not storage settings.]Pod [why: Consumes storage but doesn\u2019t define storage classes.]Submit  StorageClasses enable dynamic storage provisioning and abstract backend details like disk type or performance. What is the function of kube-proxy?Manages virtual networking and Service IP routing on nodes [why: Maintains iptables/ipvs rules for Services.]Schedules Pods [why: The scheduler does that.]Stores cluster config [why: etcd holds cluster state.]Controls container runtime [why: kubelet interfaces with the runtime, not kube-proxy.]Submit  kube-proxy maintains network rules so Services and Pods can communicate reliably. What is a Pod Security Policy (PSP)?A deprecated policy for defining allowed security contexts [why: PSP restricted Pod privilege levels and was replaced by Pod Security Standards.]A NetworkPolicy type [why: NetworkPolicy controls traffic, not security contexts.]A RoleBinding [why: RBAC manages permissions, not security posture.]An Admission Controller plugin [why: PSPs were enforced via admission, but not identical.]Submit  PSPs controlled Pod-level security but are deprecated in favor of PodSecurity admission controls. Which Kubernetes feature limits resource consumption per namespace?ResourceQuota [why: Defines maximum CPU, memory, and object counts per namespace.]LimitRange [why: Sets per-Pod or per-container min/max, not namespace-wide totals.]NetworkPolicy [why: Controls traffic, not resources.]Role [why: Manages permissions, not resource usage.]Submit  ResourceQuotas ensure fair resource distribution and prevent namespace overconsumption. Which command displays running controllers and their versions?<code>kubectl get deployment -n kube-system</code> [why: Shows controllers running as Deployments in the system namespace.]<code>kubectl controllers</code> [why: Invalid subcommand.]<code>kubectl describe nodes</code> [why: Describes nodes, not controllers.]<code>kubectl get pods</code> [why: Shows Pods but not specifically controllers.]Submit  System controllers often run as Deployments inside the <code>kube-system</code> namespace. Which scheduling concept prevents Pods with specific labels from sharing the same node?Pod Anti-Affinity [why: Ensures Pods with matching labels avoid co-location on a single node.]Taint [why: Repels Pods globally, not label-based co-location.]NodeSelector [why: Forces placement on matching nodes but doesn\u2019t separate Pods.]Toleration [why: Lets Pods tolerate taints; doesn\u2019t enforce anti-placement.]Submit  Pod Anti-Affinity helps distribute workloads for redundancy and high availability. Which Kubernetes command displays current cluster context?<code>kubectl config current-context</code> [why: Prints the active context name from kubeconfig.]<code>kubectl show context</code> [why: Not a valid subcommand.]<code>kubectl get-context</code> [why: Missing the correct prefix (<code>config</code>).]<code>kubectl use-context</code> [why: Switches context; doesn\u2019t display it.]Submit  Contexts represent combinations of cluster, user, and namespace in the kubeconfig file. Which field defines the restart behavior for failed containers?<code>restartPolicy</code> [why: Controls whether a container restarts on failure; valid values: Always, OnFailure, Never.]<code>livenessProbe</code> [why: Detects failure but doesn\u2019t control restart policy directly.]<code>readinessProbe</code> [why: Determines traffic eligibility, not restarts.]<code>backoffLimit</code> [why: Used by Jobs, not general Pods.]Submit <code>restartPolicy</code> dictates how Kubernetes handles container restarts within a Pod."},{"location":"quotas-limits/","title":"Quotas & Limits","text":"<p>In a shared cluster, trust is good, but enforcement is better.</p> <p>Without controls, a single developer with a typo (<code>replicas: 100</code> instead of <code>10</code>) can accidentally consume every CPU core in the cluster, causing production outages for everyone else.</p> <p>To prevent this \"Tragedy of the Commons,\" Kubernetes provides two layers of defense: ResourceQuotas (The Budget) and LimitRanges (The Rules).</p>"},{"location":"quotas-limits/#the-analogy-corporate-credit-cards","title":"The Analogy: Corporate Credit Cards","text":"<p>Think of a Namespace like a Department in a company (e.g., \"Engineering\").</p> <ol> <li>ResourceQuota (The Department Budget):     \"The Engineering department has a total budget of $10,000/month.\"<ul> <li>They can spend it on 1 big server or 100 small ones.</li> <li>Once they hit $10,000, their card is declined.</li> </ul> </li> <li>LimitRange (The Expense Policy):     \"No single lunch expense can be over $50.\"     \"If you forget to write down the cost, we assume it was $20.\"<ul> <li>This controls individual transactions (Pods).</li> </ul> </li> </ol>"},{"location":"quotas-limits/#1-resourcequota-the-ceiling","title":"1. ResourceQuota (The Ceiling)","text":"<p>A <code>ResourceQuota</code> limits the aggregate total of resources used by all objects in a specific namespace.</p> <p>If a new Pod tries to start, Kubernetes checks: $$\\text{Current Usage} + \\text{New Pod Request} \\le \\text{Quota Limit}$$</p> <p>If the answer is No, the API Server rejects the request with a <code>403 Forbidden</code>.</p>"},{"location":"quotas-limits/#example-the-hard-stop","title":"Example: The \"Hard\" Stop","text":"<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: team-a-budget\n  namespace: team-a\nspec:\n  hard:\n    # Compute Resources\n    requests.cpu: \"4\"        # Max 4 vCPUs reserved\n    requests.memory: 10Gi    # Max 10Gi RAM reserved\n    limits.cpu: \"8\"          # Max 8 vCPUs limit\n    limits.memory: 20Gi      # Max 20Gi RAM limit\n\n    # Object Counts (Prevent \"Pod Spam\")\n    pods: \"10\"               # Max 10 Pods total\n    services.loadbalancers: \"2\" # Max 2 expensive Cloud LBs\n</code></pre>"},{"location":"quotas-limits/#checking-usage","title":"Checking Usage","text":"<p>To see how much budget you have left:</p> <pre><code>kubectl describe quota -n team-a\n</code></pre> <p>Output:</p> <pre><code>Resource        Used    Hard\n--------        ----    ----\nrequests.cpu    500m    4\npods            3       10\n</code></pre>"},{"location":"quotas-limits/#2-limitrange-the-defaults-guardrails","title":"2. LimitRange (The Defaults &amp; Guardrails)","text":"<p>A <code>LimitRange</code> operates at the Pod/Container level. It does two critical things:</p> <ol> <li>Enforcement: \"Your container cannot be smaller than 100m CPU or larger than 2 CPUs.\"</li> <li>Defaulting (Mutation): \"You forgot to set resources? I will set them to 500m/512Mi automatically.\"</li> </ol> <p>This is the most useful feature for Admins. It ensures that even lazy developers create safe Pods.</p>"},{"location":"quotas-limits/#example-enforcing-standards","title":"Example: Enforcing Standards","text":"<pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: container-limits\n  namespace: team-a\nspec:\n  limits:\n  - default:            # &lt;--- If user sets nothing, give them this LIMIT\n      memory: 512Mi\n      cpu: 500m\n    defaultRequest:     # &lt;--- If user sets nothing, give them this REQUEST\n      memory: 256Mi\n      cpu: 250m\n    max:                # &lt;--- No container can exceed this\n      memory: 1Gi\n      cpu: 1\n    min:                # &lt;--- No container can be smaller than this\n      memory: 64Mi\n      cpu: 100m\n    type: Container\n</code></pre>"},{"location":"quotas-limits/#the-admission-workflow","title":"The Admission Workflow","text":"<p>It is important to understand the order of operations. LimitRanges run first (because they might add default values), and ResourceQuotas run last (to check the final total).</p> <pre><code>graph TD\n    User[User applies Pod YAML] --&gt;|1. Request| API[API Server]\n\n    API --&gt;|2. Mutating| LR[LimitRange Controller]\n    LR -- \"No resources set? Add defaults.\" --&gt; Pod[Updated Pod Spec]\n\n    API --&gt;|3. Validating| RQ[ResourceQuota Controller]\n    RQ -- \"Is (Current + New) &gt; Quota?\" --&gt; Check{Check}\n\n    Check -- Yes --&gt; Deny[Error 403: Forbidden]\n    Check -- No --&gt; Allow[Persist to etcd]</code></pre>"},{"location":"quotas-limits/#troubleshooting-common-errors","title":"Troubleshooting Common Errors","text":"<p>When these policies block you, the error messages are specific.</p>"},{"location":"quotas-limits/#error-1-quota-exceeded","title":"Error 1: Quota Exceeded","text":"<p>Error from server (Forbidden): pods \"my-pod\" is forbidden: exceeded quota: compute-resources, requested: requests.cpu=1, used: requests.cpu=3, limited: requests.cpu=4</p> <p>Translation: You asked for 1 CPU, but the namespace only has 1 CPU left in the budget. Fix: Delete old pods or ask admin to increase Quota.</p>"},{"location":"quotas-limits/#error-2-limitrange-violation","title":"Error 2: LimitRange Violation","text":"<p>Error from server (Forbidden): pods \"my-pod\" is forbidden: minimum cpu usage per container is 100m, but request is 50m.</p> <p>Translation: Your Pod spec is too small. The admin requires at least 100m. Fix: Increase your <code>resources.requests.cpu</code> to match the minimum.</p>"},{"location":"quotas-limits/#interaction-the-hidden-requirement","title":"Interaction: The \"Hidden\" Requirement","text":"<p>The Catch-22</p> <p>If you create a ResourceQuota for CPU/Memory, EVERY Pod in that namespace MUST have <code>requests/limits</code> defined.</p> <p>If you try to create a \"naked\" Pod (no resources) in a Quota-enabled namespace, the API will reject it immediately because it cannot calculate the cost.</p> <p>Solution: Always pair a <code>ResourceQuota</code> with a <code>LimitRange</code> that provides defaults. This ensures \"naked\" Pods get default values automatically and pass the Quota check.</p>"},{"location":"quotas-limits/#summary","title":"Summary","text":"Feature Scope Primary Purpose ResourceQuota Namespace (Aggregate) Budgeting. \"Stop the team from using too much.\" LimitRange Pod/Container (Individual) Policy. \"Ensure every pod has reasonable defaults and sizes.\" <p>Best Practice:</p> <ol> <li>Create a ResourceQuota on every namespace to prevent accidents.</li> <li>Create a LimitRange on every namespace to inject default requests/limits for developers who forget them.</li> </ol>"},{"location":"rbac/","title":"RBAC","text":"<p>In Kubernetes, Authentication asks \"Who are you?\" (Alice, Bob, or a Robot). Authorization (RBAC) asks \"What are you allowed to do?\"</p> <p>RBAC is the glue that connects Users to Actions. Without it, your authenticated user is just a person standing in the lobby with no keys to any doors.</p>"},{"location":"rbac/#the-three-pillars-of-rbac","title":"The Three Pillars of RBAC","text":"<p>To understand RBAC, you must visualize it as a triangle. You cannot grant permissions directly to a user. You must use a \"Binding\" to connect them.</p> <ol> <li>Subject: The \"Who\" (User, Group, or ServiceAccount).</li> <li>Role: The \"What\" (A list of permissions, like \"Can read Pods\").</li> <li>RoleBinding: The \"Connector\" (Assigns the Role to the Subject).</li> </ol> <pre><code>graph LR\n    User((User/Alice))\n    Role[Role: Pod-Reader]\n    Binding{RoleBinding}\n\n    User --&gt;|Referenced by| Binding\n    Binding --&gt;|Points to| Role\n    Role --&gt;|Allows access to| Pods[(Pods)]</code></pre>"},{"location":"rbac/#1-the-where-role-vs-clusterrole","title":"1. The \"Where\": Role vs. ClusterRole","text":"<p>The first decision you make is Scope. Does this permission apply to one room (Namespace) or the whole building (Cluster)?</p> Object Scope Use Case Role Namespaced \"Alice can read Pods in <code>dev</code>.\" ClusterRole Global \"Bob can read Nodes.\" OR \"The CNI plugin can read all Pods in all namespaces.\" <p>Pro Tip</p> <p><code>ClusterRoles</code> are reusable! You can define a generic \"view-only\" ClusterRole and reuse it in multiple namespaces.</p>"},{"location":"rbac/#2-the-how-rolebinding-vs-clusterrolebinding","title":"2. The \"How\": RoleBinding vs. ClusterRoleBinding","text":"<p>This is where beginners get stuck. The binding determines the final scope of the permission.</p> <ul> <li>RoleBinding: Grants the permissions within a specific namespace.</li> <li>ClusterRoleBinding: Grants the permissions across the entire cluster.</li> </ul>"},{"location":"rbac/#the-cookie-cutter-pattern-important","title":"The \"Cookie Cutter\" Pattern (Important!)","text":"<p>You can bind a ClusterRole using a RoleBinding.</p> <p>Why? Imagine you have 50 namespaces. You don't want to write a <code>Role</code> yaml for \"view-pods\" 50 times.</p> <ol> <li>Create one <code>ClusterRole</code> named <code>view-pods</code>.</li> <li>Create a <code>RoleBinding</code> in the <code>dev</code> namespace pointing to it.</li> <li>Create a <code>RoleBinding</code> in the <code>prod</code> namespace pointing to it.</li> </ol> <p>Now Alice has the \"view-pods\" permission, but only inside the specific namespaces where you bound it.</p> <pre><code>graph TD\n    CR[ClusterRole: view-pods]\n\n    RB1[RoleBinding: dev] --&gt;|References| CR\n    RB2[RoleBinding: prod] --&gt;|References| CR\n\n    User1[Alice] --&gt; RB1\n    User2[Bob] --&gt; RB2\n\n    User1 -.-&gt;|Can view pods in| Dev[Namespace: dev]\n    User2 -.-&gt;|Can view pods in| Prod[Namespace: prod]</code></pre>"},{"location":"rbac/#3-service-accounts-robots","title":"3. Service Accounts (Robots)","text":"<p>Users (Humans) live outside the cluster (in Google/AWS IAM, or OIDC). ServiceAccounts live inside the cluster. They are the identities for your Pods.</p> <p>If your Jenkins Build Pod needs to deploy to Kubernetes, you:</p> <ol> <li>Create a <code>ServiceAccount</code>.</li> <li>Create a <code>Role</code> (Deployer).</li> <li>Create a <code>RoleBinding</code> (Bind Jenkins SA to Deployer Role).</li> <li>Mount the ServiceAccount into the Jenkins Pod.</li> </ol> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins-bot\n  namespace: ci-cd\n</code></pre>"},{"location":"rbac/#4-debugging-can-i","title":"4. Debugging: \"Can I?\"","text":"<p>RBAC is complex. Sometimes you don't know why permission is denied. Kubernetes includes a tool to check permissions for yourself or others.</p> <p>Check my own permissions:</p> <pre><code>kubectl auth can-i create deployments\n# Output: yes\n\nkubectl auth can-i delete nodes\n# Output: no\n</code></pre> <p>Check someone else's permissions (Impersonation): (Great for admins testing a new user's setup)</p> <pre><code>kubectl auth can-i list secrets --as alice --namespace dev\n# Output: no\n</code></pre>"},{"location":"rbac/#5-aggregated-clusterroles-advanced","title":"5. Aggregated ClusterRoles (Advanced)","text":"<p>Kubernetes has a cool feature where you can create a \"Super Role\" that automatically combines other roles.</p> <p>The default <code>admin</code> ClusterRole is an aggregated role. If you install a CRD (like <code>Prometheus</code>), the CRD can automatically add its permissions to the default <code>admin</code> role so your admins don't need to manually update their permissions.</p>"},{"location":"rbac/#summary","title":"Summary","text":"<ul> <li>Role/RoleBinding: For Namespace-level access (90% of use cases).</li> <li>ClusterRole/ClusterRoleBinding: For Node-level access or \"All Namespaces\" access.</li> <li>ClusterRole + RoleBinding: The efficient way to reuse standard permissions in specific namespaces.</li> <li>ServiceAccounts: The users for your Pods.</li> <li><code>kubectl auth can-i</code>: The best way to test your rules.</li> </ul> <p>Security Warning</p> <p>Be extremely careful with the <code>cluster-admin</code> ClusterRole. It allows users to do anything, including deleting the entire cluster or reading all secrets. Follow Least Privilege: Only grant the specific verbs (<code>get</code>, <code>list</code>) needed for the specific resource (<code>pods</code>, <code>services</code>).</p>"},{"location":"scaling-hpa/","title":"Scaling & HPA","text":"<p>One of the primary reasons companies choose Kubernetes is its ability to handle variable traffic. When your app gets featured on the front page of a major site, you need more power now. When everyone goes to sleep at 3 AM, you want to stop paying for idle servers.</p> <p>Kubernetes handles this through scaling. While you can manually scale things up and down, the real magic lies in automation.</p>"},{"location":"scaling-hpa/#the-manual-way-imperative-scaling","title":"The Manual Way (Imperative Scaling)","text":"<p>The easiest way to scale a Deployment or StatefulSet is to just tell Kubernetes to change the replica count.</p> <pre><code>kubectl scale deployment my-web-app --replicas=10\n</code></pre> <p>Within seconds, the Deployment controller sees that you want 10 Pods but only have 2 running, and it immediately rushes to create 8 more.</p> <p>This is great for testing or known events, but it requires a human to wake up at 3 AM when the servers get overloaded. We want robots to do that for us.</p>"},{"location":"scaling-hpa/#introducing-the-hpa-horizontal-pod-autoscaler","title":"Introducing the HPA (Horizontal Pod Autoscaler)","text":"<p>The HorizontalPodAutoscaler (HPA) is a robot designed for one job: to watch a metric (like CPU usage) and automatically adjust the number of replicas in your Deployment up or down based on rules you define.</p>"},{"location":"scaling-hpa/#the-thermostat-analogy","title":"The Thermostat Analogy","text":"<p>Think of the HPA like the thermostat in your house.</p> <ol> <li>You set a target temperature: \"I want this room to stay at 72\u00b0F.\"</li> <li>The thermostat watches the thermometer: It constantly checks current conditions.</li> <li>It takes action:<ul> <li>If it's 65\u00b0F (too cold/low load), it turns on the furnace (scales up).</li> <li>If it's 78\u00b0F (too hot/high load), it turns on the AC (scales down).</li> </ul> </li> </ol> <p>In Kubernetes, the HPA works the same way:</p> <ol> <li>You set a target: \"I want the average CPU usage across all my Pods to be 50%.\"</li> <li>The HPA watches the metrics server: It asks, \"What is the current average CPU usage?\"</li> <li>It takes action:<ul> <li>If the average is 80% (too hot), it calculates how many more replicas are needed to bring the average back down to 50%, and scales up.</li> <li>If the average is 20% (too cold), it calculates how many Pods it can remove while still staying near 50%, and scales down.</li> </ul> </li> </ol>"},{"location":"scaling-hpa/#a-simple-hpa-example","title":"A Simple HPA Example","text":"<p>To use the HPA, your Pods must have resource requests defined. The HPA makes its scaling decisions based on the percentage of the requested amount being used.</p> <p>Here is a simple HPA resource. It tells Kubernetes to manage the <code>my-web-app</code> deployment, keeping its CPU usage around 50%, with a minimum of 2 Pods and a maximum of 10.</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: web-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-web-app\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n</code></pre> <p>Once applied, you can watch it work:</p> <pre><code>kubectl get hpa\n</code></pre> <p>You'll see output showing the current state vs. the target state:</p> <pre><code>NAME          REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS\nweb-app-hpa   Deployment/my-web-app   32%/50%   2         10        3\n</code></pre> <p>In this case, current usage is 32%, which is below the 50% target, so the HPA will not scale up. If traffic spiked and usage went to 80%, the HPA would increase the replica count.</p>"},{"location":"scaling-hpa/#the-metrics-server-requirement","title":"The Metrics Server Requirement","text":"<p>Just like a thermostat needs a thermometer, the HPA needs metrics data.</p> <p>The HPA controller doesn't magically know how much CPU your Pods are using. It relies on a separate cluster add-on called the Metrics Server. The Metrics Server collects resource usage data from every node and exposes it via the Kubernetes API.</p> <ul> <li>If you run <code>kubectl top pods</code> and get an error, your Metrics Server is probably not installed or is broken.</li> <li>If the Metrics Server isn't working, the HPA cannot work.</li> </ul> <p>Most managed Kubernetes services (like EKS, AKS, GKE) come with the Metrics Server pre-installed. If you are building your own cluster with <code>kubeadm</code>, you will need to install it yourself.</p>"},{"location":"scaling-hpa/#going-deeper-other-types-of-scaling","title":"Going Deeper: Other Types of Scaling","text":"<p>While the HPA is the most common form of scaling, it's important to know about the others to see the bigger picture.</p>"},{"location":"scaling-hpa/#vertical-pod-autoscaler-vpa","title":"Vertical Pod Autoscaler (VPA)","text":"<p>While HPA scales horizontally (adding more machines), the VPA scales vertically (making existing machines bigger).</p> <p>If your Pod is constantly crashing due to OutOfMemory (OOM) errors, the VPA can notice this and automatically restart the Pod with higher memory limits.</p> <p>Warning</p> <p>VPA and HPA usually shouldn't be used on the same metric (e.g., CPU) at the same time. They will fight each other. The HPA will try to add pods to lower CPU usage, while the VPA tries to remove pods to increase CPU utilization.</p>"},{"location":"scaling-hpa/#cluster-autoscaler-ca","title":"Cluster Autoscaler (CA)","text":"<p>HPA and VPA only deal with Pods. But what happens when your HPA requests 100 new Pods, and your physical nodes run out of space to host them? Your Pods will get stuck in a <code>Pending</code> state.</p> <p>The Cluster Autoscaler watches for pending Pods that cannot be scheduled due to a lack of resources. When it sees this, it talks to your cloud provider API (AWS, Google, Azure) and provisions a brand new virtual machine (Node) to add to your cluster. Conversely, if a node is underutilized for a long time, the CA can move Pods off it and delete the node to save money.</p>"},{"location":"scaling-hpa/#summary","title":"Summary","text":"<ul> <li>Manual Scaling is easy but doesn't react to changing traffic.</li> <li>The HPA (HorizontalPodAutoscaler) automates scaling replicas up and down based on metrics like CPU or memory.</li> <li>Think of the HPA like a thermostat, trying to keep your environment at a target utilization level.</li> <li>The HPA requires the Metrics Server to be running in your cluster to function.</li> <li>For a fully elastic cluster, you combine the HPA (to scale Pods) with the Cluster Autoscaler (to scale the underlying Nodes).</li> </ul> <p>Tip</p> <p>Scaling is not instant. It takes time for Metrics Server to gather data, time for the HPA to calculate the change, and time for the new application Pod to start up and pass its readiness probes. Don't set your target too high (e.g., 95%), or your app might crash from overload before the new Pods are ready to help. A target of 50-70% is a common starting point.</p>"},{"location":"sec-context/","title":"Security Context","text":"<p>If RBAC secures the API (who can talk to Kubernetes), Security Contexts secure the Runtime (what the process can do on the Linux kernel).</p> <p>A container in Kubernetes is not a virtual machine. It is a process running on the Host's kernel. If you don't lock it down, a hacked container can attack the kernel, read other containers' memory, or modify host files.</p> <p>Security Contexts are the configuration settings in your YAML that tell the container runtime (containerd/CRI-O): \"Sandobx this process. Take away its keys. Don't let it run as root.\"</p>"},{"location":"sec-context/#1-pod-vs-container-contexts-the-hierarchy","title":"1. Pod vs. Container Contexts (The Hierarchy)","text":"<p>You can set security contexts in two places. It is vital to know which one wins.</p> <ol> <li><code>spec.securityContext</code> (Pod Level): Applies to all containers.<ul> <li>Used for: Shared settings like <code>fsGroup</code> (Volume permissions) or <code>runAsUser</code> (if identical for all).</li> </ul> </li> <li><code>spec.containers.securityContext</code> (Container Level): Applies to one container.<ul> <li>Used for: Capabilities, <code>readOnlyRootFilesystem</code>, or overriding the Pod setting.</li> </ul> </li> </ol> <p>The Rule: Container settings override Pod settings.</p> <pre><code>graph TD\n    PodCtx[Pod Security Context&lt;br/&gt;User: 1000] --&gt;|Inherits| C1[Container 1]\n    PodCtx --&gt;|Overridden by| C2[Container 2]\n\n    C2Config[Container Security Context&lt;br/&gt;User: 2000] -.-&gt; C2\n\n    Result1[Container 1 runs as&lt;br/&gt;User 1000]\n    Result2[Container 2 runs as&lt;br/&gt;User 2000]\n</code></pre>"},{"location":"sec-context/#2-the-user-identity-runasuser-vs-runasgroup","title":"2. The User Identity (<code>runAsUser</code> vs <code>runAsGroup</code>)","text":"<p>By default, containers run as <code>root</code> (UID 0). This is dangerous. If a hacker escapes the container, they are <code>root</code> on the host node.</p> <ul> <li><code>runAsUser</code>: Forces the process to start with this specific UID.</li> <li><code>runAsNonRoot: true</code>: A safety check. It checks the image metadata. If the image is built to run as root, Kubernetes refuses to start it.</li> </ul> <p>Tip</p> <p>Why <code>fsGroup</code> matters When you mount a Volume (like a PVC), it might be owned by <code>root</code>. If your container runs as User 1000, it can't write to that disk! Solution: Set <code>fsGroup: 2000</code> in the Pod Security Context. Kubernetes will automatically <code>chown</code> (change ownership) of the volume files to Group 2000 so your container can write to them.</p>"},{"location":"sec-context/#3-linux-capabilities-the-keys-to-the-kingdom","title":"3. Linux Capabilities (The Keys to the Kingdom)","text":"<p>In Linux, \"Root\" isn't just one permission. It is a bundle of super-powers called Capabilities.</p> <ul> <li><code>CAP_NET_ADMIN</code> (Change firewalls/IPs)</li> <li><code>CAP_SYS_TIME</code> (Change the system clock)</li> <li><code>CAP_CHOWN</code> (Change file ownership)</li> </ul> <p>By default, Docker/Kubernetes gives containers a \"default set\" of these keys. You usually don't need them.</p> <p>Best Practice: Drop ALL capabilities, then add back only what you strictly need.</p> <pre><code>securityContext:\n  capabilities:\n    drop: [\"ALL\"]  # Drop everything. Highly secure.\n    add: [\"NET_BIND_SERVICE\"] # Add back ability to bind port 80\n</code></pre>"},{"location":"sec-context/#4-immutable-infrastructure-readonlyrootfilesystem","title":"4. Immutable Infrastructure (<code>readOnlyRootFilesystem</code>)","text":"<p>Hackers love to download scripts (<code>wget malicious.sh</code>) and execute them. If your filesystem is Read-Only, they can't save the file.</p> <pre><code>securityContext:\n  readOnlyRootFilesystem: true\n</code></pre> <p>Note: If your app needs to write temp files (like logs), you must mount an <code>emptyDir</code> volume at that specific path (e.g., <code>/var/log</code>).</p>"},{"location":"sec-context/#5-privilege-escalation-allowprivilegeescalation","title":"5. Privilege Escalation (<code>allowPrivilegeEscalation</code>)","text":"<p>Even if you are not root, a process can \"become\" root using tools like <code>sudo</code> or binaries with the <code>setuid</code> bit set.</p> <p>Setting <code>allowPrivilegeEscalation: false</code> disables this mechanism entirely. It effectively says: \"Once you are a standard user, you stay a standard user forever.\"</p>"},{"location":"sec-context/#a-golden-standard-example","title":"A \"Golden Standard\" Example","text":"<p>Here is a snippet you can copy-paste for 90% of production web applications.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-app\nspec:\n  securityContext:\n    # Pod Level: Handle Storage\n    fsGroup: 2000\n    runAsUser: 1000\n    runAsGroup: 3000\n  containers:\n  - name: my-app\n    image: my-app:v1\n    securityContext:\n      # Container Level: Runtime Hardening\n      runAsNonRoot: true\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop: [\"ALL\"]\n    volumeMounts:\n    - name: tmp\n      mountPath: /tmp # Allow writing to /tmp even on read-only FS\n  volumes:\n  - name: tmp\n    emptyDir: {}\n</code></pre>"},{"location":"sec-context/#summary","title":"Summary","text":"<ul> <li>Security Contexts are the firewall for the Linux Kernel.</li> <li>Container Context overrides Pod Context.</li> <li><code>runAsNonRoot</code> protects you from root-level exploits.</li> <li><code>capabilities: drop: [\"ALL\"]</code> is the gold standard for least privilege.</li> <li><code>fsGroup</code> is the magic fix when your non-root container can't write to its PVC.</li> </ul>"},{"location":"security/","title":"Security Primer","text":"<p>Security in Kubernetes is not a single feature you turn on. It is a process.</p> <p>Because Kubernetes abstracts so much (networks, storage, compute), it also abstracts the attack surface. A hacker doesn't need physical access to your server if they can trick your API server into launching a privileged pod that mounts the host's filesystem.</p> <p>To secure a cluster, we use a \"Defense in Depth\" strategy known as the 4Cs of Cloud Native Security.</p>"},{"location":"security/#1-the-4cs-model","title":"1. The 4Cs Model","text":"<p>Think of security like an onion. If an attacker peels back one layer, they shouldn't immediately reach the core.</p> Layer Responsibility Key Defenses Cloud The Datacenter / Hardware Firewall rules, IAM User access, Encrypted Disks. Cluster The Control Plane RBAC, API Audit Logging, Etcd Encryption. Container The Image &amp; Runtime Image Scanning, Signing, limiting root users. Code Your Application Static Analysis, Dependency Checks, HTTPS. <p>The Weakest Link Rule</p> <p>You can have the best Firewall (Cloud) and the strictest RBAC (Cluster), but if your developer hardcodes an AWS Secret Key into their Python script (Code), you are hacked.</p>"},{"location":"security/#2-the-attack-chain-how-hackers-break-in","title":"2. The Attack Chain (How Hackers Break In)","text":"<p>Understanding how you get hacked helps you understand why we need these controls.</p> <ol> <li>The Exploit: An attacker finds a vulnerability in your web app (e.g., Log4j).</li> <li>The Foothold: They get a shell inside your Container.</li> <li>The Escalation: They notice the container is running as <code>root</code> and has a ServiceAccount mounted.</li> <li>The Lateral Move: They use the ServiceAccount to talk to the Kubernetes API and list all Secrets in the cluster.</li> <li>The Goal: They find a database password in a Secret and steal your data.</li> </ol> <p>Your Goal: Break this chain at every single step.</p>"},{"location":"security/#3-the-defense-toolkit","title":"3. The Defense Toolkit","text":"<p>Here is how they fit together.</p>"},{"location":"security/#1-cluster-access-the-front-door","title":"1. Cluster Access (The Front Door)","text":"<ul> <li>Authentication: Who are you? (OIDC, Certificates).</li> <li>Authorization (RBAC): What can you do? (Roles, Bindings).</li> </ul>"},{"location":"security/#2-workload-hardening-the-cells","title":"2. Workload Hardening (The Cells)","text":"<ul> <li>Pod Security Admission (PSA): Prevent \"super-user\" containers. Disallow privileged mode and host mounts.</li> <li>Security Context: Force containers to run as non-root users.</li> </ul>"},{"location":"security/#3-network-segmentation-the-walls","title":"3. Network Segmentation (The Walls)","text":"<ul> <li>Network Policies: By default, every Pod can talk to every other Pod. Use Policies to block traffic between \"Frontend\" and \"Database\" unless explicitly allowed.</li> </ul>"},{"location":"security/#4-supply-chain-the-ingredients","title":"4. Supply Chain (The Ingredients)","text":"<ul> <li>Image Scanning: Check your Docker images for known CVEs before they ever reach the cluster.</li> <li>Signing: Ensure only your trusted images are allowed to run.</li> </ul>"},{"location":"security/#4-shift-left-devsecops","title":"4. Shift Left: DevSecOps","text":"<p>In the old days, security was a \"gate\" at the end. In Kubernetes, security must be defined in the YAML.</p> <ul> <li>Linting: Use tools like <code>kube-linter</code> or <code>checkov</code> to scan your YAML files for mistakes (like <code>privileged: true</code>) before you commit them to Git.</li> <li>Admission Controllers: Use tools like OPA Gatekeeper or Kyverno to reject insecure YAMLs at the API level.</li> </ul>"},{"location":"security/#summary","title":"Summary","text":"<ul> <li>Security is layered. Don't rely on just one tool.</li> <li>Misconfiguration is the #1 threat. Most breaches happen because someone left a door open (permissive RBAC, no NetworkPolicy), not because of a sophisticated zero-day.</li> <li>Least Privilege: Give every user, pod, and service account the minimum permission they need to work. Nothing more.</li> </ul>"},{"location":"services-networking/","title":"Services","text":"Services &amp; Networking <p>Pods are short-lived - they can appear and disappear at any time. A Service gives you a stable way to talk to a group of Pods, no matter how often those Pods restart or move.</p> What Is a Service? <p>A Kubernetes Service is like a switchboard operator for your Pods: - Selects a group of Pods (using labels) - Gives them a stable IP and DNS name - Forwards traffic to the right Pods, even as they change</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web\nspec:\n  selector:\n    app: web\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre> <p>This exposes Pods with label <code>app=web</code> on port 80, forwarding traffic to their port 8080.</p> 1. ClusterIP (default) <p>A ClusterIP Service is for internal communication only. Think of it as a company\u2019s internal phone extension - only people inside the building (cluster) can call it.</p> <ul> <li>Internal IP address (e.g., <code>10.x.x.x</code>)</li> <li>DNS: <code>web.default.svc.cluster.local</code></li> <li>Default Service type</li> </ul> <p> </p> <p>Use When:</p> <ul> <li>Apps need to talk to each other inside the cluster (e.g., frontend \u2194 backend)</li> <li>No external access needed</li> </ul> 2. NodePort <p>A NodePort Service lets people outside your cluster reach your app using a static port on every node. It\u2019s like giving every employee in the company a direct phone number that rings their internal extension.</p> <ul> <li>Uses each node\u2019s IP + port (range: <code>30000\u201332767</code>)</li> <li>Forwards traffic from node to the right Pods</li> </ul> <pre><code>spec:\n  type: NodePort\n  ports:\n    - port: 80\n      targetPort: 8080\n      nodePort: 30080\n</code></pre> <p>Access from outside the cluster:</p> <pre><code>http://&lt;node-ip&gt;:30080\n</code></pre> <p> </p>"},{"location":"services-networking/#use-when","title":"Use When:","text":"<ul> <li>Testing external access without a LoadBalancer</li> <li>You don\u2019t have a cloud provider (e.g., on-prem clusters)</li> </ul>"},{"location":"services-networking/#3-loadbalancer","title":"3. LoadBalancer","text":"<p>A LoadBalancer Service provisions an external cloud load balancer (if supported by your environment).</p> <ul> <li>Only works with cloud providers (GCP, AWS, Azure)</li> <li>Assigns a public IP and balances across backing Pods</li> <li>Combines NodePort + external LB behind the scenes</li> </ul> <pre><code>spec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre> <p> </p>"},{"location":"services-networking/#use-when_1","title":"Use When:","text":"<ul> <li>You want public access to your app in a cloud environment</li> <li>You need external DNS + SSL termination (with Ingress)</li> </ul>"},{"location":"services-networking/#4-externalname-special-case","title":"4. ExternalName (Special Case)","text":"<p>Maps a Kubernetes Service to an external DNS name.</p> <pre><code>spec:\n  type: ExternalName\n  externalName: db.example.com\n</code></pre> <ul> <li>No selectors or backing Pods</li> <li>Useful for referencing external databases, APIs, etc.</li> </ul>"},{"location":"services-networking/#summary-table","title":"Summary Table","text":"Type Visibility Use Case Requires Cloud <code>ClusterIP</code> Internal only Pod-to-Pod communication \u274c No <code>NodePort</code> Exposes on node IP Direct external access via port \u274c No <code>LoadBalancer</code> External IP Cloud load balancer with public IP \u2705 Yes <code>ExternalName</code> DNS redirect External services via DNS \u274c No"},{"location":"services-networking/#summary","title":"Summary","text":"<ul> <li>Services abstract a group of Pods behind a stable IP and DNS name.</li> <li>ClusterIP is the default and internal-only.</li> <li>NodePort opens access via node IP and high port.</li> <li>LoadBalancer gives you a cloud-managed endpoint.</li> <li>ExternalName is a DNS-level alias.</li> </ul> <p> Understanding how each Service type works  -  and when to use it  -  is essential for building reliable, scalable apps in Kubernetes. </p>"},{"location":"statefulsets/","title":"StatefulSets","text":"StatefulSets <p>A StatefulSet is a Kubernetes controller for running stateful apps - apps that need each Pod to keep its identity and storage, even if rescheduled. Think databases, message queues, or anything that can't just be replaced with a blank copy.</p> Why Use a StatefulSet? <p>Use a StatefulSet when your app needs:</p> <ul> <li>Stable network identity (like <code>pod-0</code>, <code>pod-1</code>)</li> <li>Persistent storage per Pod that sticks around if the Pod is rescheduled</li> <li>Ordered startup, scaling, and deletion</li> </ul> <p>Examples: Databases (PostgreSQL, Cassandra), Zookeeper, Kafka, etc.</p> How It Differs from Deployments <p>StatefulSets guarantee identity and storage for each Pod, while Deployments just care about keeping the right number of Pods running (not which is which).</p> <p> </p> <p>Top Half (Deployment): Pod reschedule = new IP, broken volume mount Bottom Half (StatefulSet): Pod is recreated with the same IP, same volume</p> Key Features Feature Deployment StatefulSet Pod name Random (e.g., <code>pod-abc123</code>) Stable (e.g., <code>web-0</code>, <code>web-1</code>) Pod start/delete order Any Ordered Persistent VolumeClaim Shared/ephemeral One per Pod DNS hostname Random Stable via headless service Sample YAML <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"web\"  # Headless service\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n        - name: nginx\n          image: nginx\n          volumeMounts:\n            - name: data\n              mountPath: /usr/share/nginx/html\n      tolerations:\n      - key: \"node-role.kubernetes.io/master\"\n        operator: \"Exists\"\n        effect: \"NoSchedule\"\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes: [\"ReadWriteOnce\"]\n        resources:\n          requests:\n            storage: 1Gi\n</code></pre>"},{"location":"statefulsets/#networking-dns","title":"Networking &amp; DNS","text":"<p>Pods in a StatefulSet get predictable hostnames:</p> <pre><code>web-0.web.default.svc.cluster.local\nweb-1.web.default.svc.cluster.local\n</code></pre> <p>This is enabled by the headless Service:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web\nspec:\n  clusterIP: None  # Headless\n  selector:\n    app: web\n  ports:\n    - port: 80\n</code></pre>"},{"location":"statefulsets/#volume-behavior","title":"Volume BehaviorSummary","text":"<p>Each Pod gets its own PVC:</p> <ul> <li><code>web-0</code> \u2192 <code>data-web-0</code></li> <li><code>web-1</code> \u2192 <code>data-web-1</code></li> </ul> <p>These volumes are retained even if the Pod is deleted.</p> <ul> <li>StatefulSets are for apps that need stable identity and storage.</li> <li>Use them for databases, queues, and apps that can't just be replaced with a blank Pod.</li> <li>Deployments are for stateless, replaceable workloads.</li> </ul> <p>Tip</p> <p>Only use StatefulSets when you really need sticky identity or storage. For most apps, Deployments are simpler and easier to manage.</p>"},{"location":"storage/","title":"Storage (PV, PVC, StorageClass)","text":"<p>Containers are ephemeral. When a container crashes or stops, all the files created inside it are lost.</p> <p>For many apps (like web servers), this is fine. For others (databases, queues, key-value stores), this is catastrophic. Kubernetes solves this with a robust Storage system that decouples \"Disk\" from \"Pod.\"</p>"},{"location":"storage/#1-ephemeral-storage-the-temporary-stuff","title":"1. Ephemeral Storage (The Temporary Stuff)","text":"<p>Before jumping to disks, know that you can store data temporarily without any complex setup.</p>"},{"location":"storage/#emptydir","title":"<code>emptyDir</code>","text":"<p>This creates a temporary directory on the Node's disk that is mounted into your Pod.</p> <ul> <li>Lifespan: Deleted immediately when the Pod is deleted.</li> <li>Use Case: Cache files, scratch space, sorting large data sets.</li> </ul> <pre><code>volumes:\n  - name: cache-volume\n    emptyDir: {} # Creates a temp directory on the host\n</code></pre>"},{"location":"storage/#hostpath","title":"<code>hostPath</code>","text":"<p>This mounts a file or directory from the host node's filesystem directly into your Pod.</p> <ul> <li>Risk: If the Pod moves to a different Node, the data is gone (or different). It is also a huge security risk (giving a Pod access to <code>/var/lib/docker</code> or <code>/etc</code>).</li> <li>Use Case: Only for system agents (like Log Collectors) that need to read host internals. Avoid in normal apps.</li> </ul>"},{"location":"storage/#2-the-big-three-pv-pvc-and-storageclass","title":"2. The Big Three: PV, PVC, and StorageClass","text":"<p>To save data forever (or at least until you say so), Kubernetes uses three distinct API objects.</p> Object Analogy Who Creates It? PersistentVolume (PV) The Parking Spot. The actual piece of storage (AWS EBS, Google Disk, NFS share). Admin (or Auto-provisioner) PersistentVolumeClaim (PVC) The Ticket. A request for a spot (\"I need 10GB of fast storage\"). Developer StorageClass (SC) The Valet. An automated system that creates PVs on demand based on PVCs. Admin"},{"location":"storage/#the-workflow","title":"The Workflow","text":"<pre><code>graph LR\n    Dev[\"Developer\"] --&gt;|Creates| PVC[\"PVC (Request)\"]\n    PVC --&gt;|Points to| SC[\"StorageClass\"]\n    SC --&gt;|Dynamically Creates| PV[\"PV (Actual Disk)\"]\n    PV --&gt;|Binds to| PVC\n\n    subgraph \"Bound Pair\"\n    PVC -.-&gt; PV\n    end\n\n    Pod --&gt;|Mounts| PVC</code></pre>"},{"location":"storage/#3-persistentvolumeclaim-the-request","title":"3. PersistentVolumeClaim (The Request)","text":"<p>As a developer, this is usually the only YAML you write. You are asking Kubernetes for storage.</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: db-data\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: standard # Requests a specific \"Valet\"\n</code></pre>"},{"location":"storage/#access-modes","title":"Access Modes","text":"<p>Be careful here. \"ReadWriteMany\" does not mean \"Magic Cloud Drive.\" It depends heavily on the underlying hardware.</p> <ul> <li><code>ReadWriteOnce</code> (RWO): Can be mounted by one Node (e.g., standard EBS/disk).</li> <li><code>ReadWriteOncePod</code> (RWOP): New &amp; Safer. Can be mounted by one Pod. Prevents two Pods on the same node from fighting over the disk.</li> <li><code>ReadWriteMany</code> (RWX): Can be mounted by many Nodes at once (e.g., NFS, EFS, AzureFile). Block storage (AWS EBS, Azure Disk) generally cannot do this.</li> </ul>"},{"location":"storage/#4-storageclass-the-automation","title":"4. StorageClass (The Automation)","text":"<p>In the old days (\"Static Provisioning\"), an Admin had to manually create 100 PVs and hope they matched the PVCs. Today, we use Dynamic Provisioning via StorageClasses.</p> <p>The StorageClass defines what kind of disk to create.</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-ssd\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp3\n  encrypted: \"true\"\nreclaimPolicy: Delete # Critical Setting!\nallowVolumeExpansion: true\n</code></pre>"},{"location":"storage/#reclaim-policies-crucial","title":"Reclaim Policies (Crucial!)","text":"<p>What happens to your data when you delete the PVC?</p> <ul> <li><code>Delete</code> (Default): The PV (and the actual cloud disk) is deleted. Data is lost.</li> <li><code>Retain</code>: The PV is released but keeps the data. An admin must manually clean it up or reconnect it.</li> <li><code>Recycle</code>: (Deprecated) Scrubs the data and makes the PV available again.</li> </ul> <p>Pro Tip</p> <p>For production databases, consider setting <code>reclaimPolicy: Retain</code> so you don't accidentally wipe your database by running <code>kubectl delete pvc</code>.</p>"},{"location":"storage/#5-how-to-use-it-in-a-pod","title":"5. How to use it in a Pod","text":"<p>Once you have a PVC, you treat it just like a regular Volume.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\nspec:\n  volumes:\n    - name: my-storage\n      persistentVolumeClaim:\n        claimName: db-data # Matches the PVC Name\n  containers:\n    - name: app\n      image: nginx\n      volumeMounts:\n        - mountPath: \"/var/www/html\"\n          name: my-storage\n</code></pre>"},{"location":"storage/#summary-best-practices","title":"Summary &amp; Best Practices","text":"<ol> <li>Prefer Dynamic Provisioning: Use StorageClasses. Don't create PVs manually unless you have to (e.g., connecting to a legacy NFS share).</li> <li>Match the Access Mode: Don't ask for <code>ReadWriteMany</code> on a standard AWS/GCP disk; the Pod will fail to start.</li> <li>Watch the Reclaim Policy: Know if your data disappears when the PVC is deleted.</li> <li>StatefulSets: If you are running a replicated database (e.g., 3 replicas), don't use a single Deployment + PVC (they will all share the same disk!). Use a StatefulSet, which generates a unique PVC for every Pod.</li> </ol>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>When a Pod breaks, don't guess. Follow the evidence.</p> <p>Kubernetes is incredibly transparent - it almost always tells you exactly what is wrong, provided you know where to look. This guide outlines a systematic workflow for diagnosing clusters.</p>"},{"location":"troubleshooting/#the-triage-flowchart","title":"The Triage Flowchart","text":"<p>Before running random commands, mentally locate where the failure is occurring.</p> <pre><code>graph TD\n    Start[\"Pod Issue\"] --&gt; Status{\"Check Pod Status\"}\n\n    Status -- \"Pending\" --&gt; Scheduler[\"Scheduler Issue&lt;br/&gt;(No resources, Taints?)\"]\n    Status -- \"ImagePullBackOff\" --&gt; Image[\"Registry Issue&lt;br/&gt;(Auth, Typo?)\"]\n    Status -- \"CrashLoopBackOff\" --&gt; App[\"App Issue&lt;br/&gt;(Bug, Config?)\"]\n    Status -- \"CreateContainerConfigError\" --&gt; Config[\"Config Issue&lt;br/&gt;(Missing Secret/Map?)\"]\n    Status -- \"Running\" --&gt; Net{\"Network Issue?\"}\n\n    Net -- \"Can't connect\" --&gt; Service[\"Check Service/DNS\"]\n    Net -- \"500 Error\" --&gt; AppLogs[\"Check App Logs\"]</code></pre>"},{"location":"troubleshooting/#phase-1-the-big-three-commands","title":"Phase 1: The \"Big Three\" Commands","text":"<p>In 90% of cases, you can solve the problem using just these three commands in order.</p>"},{"location":"troubleshooting/#1-kubectl-describe-pod-name","title":"1. <code>kubectl describe pod &lt;name&gt;</code>","text":"<p>The \"Crime Scene Report\". This tells you what Kubernetes thinks happened. Look at the Events section at the bottom.</p> <ul> <li>Did the scheduler fail to find a node?</li> <li>Did the Liveness probe fail?</li> <li>Did the volume fail to mount?</li> </ul>"},{"location":"troubleshooting/#2-kubectl-logs-name","title":"2. <code>kubectl logs &lt;name&gt;</code>","text":"<p>The \"Victim's Last Words\". This shows the application's standard output (stdout).</p> <ul> <li>Did the app throw a Python stack trace?</li> <li>Did it say \"Database Connection Refused\"?</li> </ul> <p>Pro Tip</p> <p>If your pod is in a restart loop, <code>kubectl logs</code> might show you the current (empty) container starting up. To see why the last one died, use: <code>kubectl logs my-pod --previous</code></p>"},{"location":"troubleshooting/#3-kubectl-get-pod-name-o-yaml","title":"3. <code>kubectl get pod &lt;name&gt; -o yaml</code>","text":"<p>The \"Blueprint\". Check the configuration.</p> <ul> <li>Did you typo the ConfigMap name?</li> <li>Are you pulling the <code>latest</code> tag by accident?</li> <li>Did you verify the <code>command</code> arguments?</li> </ul>"},{"location":"troubleshooting/#phase-2-decoding-common-states","title":"Phase 2: Decoding Common States","text":"State Translation Where to look Pending \"I'm waiting for a home.\" The Scheduler cannot find a node that fits this Pod (CPU/Mem limits, Taints, or Affinity). <code>kubectl describe pod</code> ImagePullBackOff \"I can't get the package.\" The registry path is wrong, the tag doesn't exist, or you forgot the <code>imagePullSecret</code>. <code>kubectl describe pod</code> CrashLoopBackOff \"I started, but I died immediately.\" The app is buggy or misconfigured. <code>kubectl logs</code> CreateContainerConfigError \"I can't find my keys.\" You referenced a Secret or ConfigMap that doesn't exist. <code>kubectl describe pod</code> OOMKilled \"I ate too much.\" The app used more RAM than the <code>limit</code> allowed. <code>kubectl get pod</code> (Look for Exit Code 137)"},{"location":"troubleshooting/#phase-3-advanced-debugging-tools","title":"Phase 3: Advanced Debugging Tools","text":"<p>Sometimes logs aren't enough. You need to get inside.</p>"},{"location":"troubleshooting/#1-kubectl-exec-the-standard-way","title":"1. <code>kubectl exec</code> (The Standard Way)","text":"<p>If the Pod is running (e.g., a web server that is returning 500 errors), jump inside to poke around.</p> <pre><code>kubectl exec -it my-pod -- /bin/sh\n# Inside:\n# curl localhost:8080\n# cat /etc/config/app.conf\n</code></pre>"},{"location":"troubleshooting/#2-kubectl-debug-the-modern-way","title":"2. <code>kubectl debug</code> (The Modern Way)","text":"<p>Use this when: The Pod is crashing (<code>CrashLoopBackOff</code>) or is a \"Distroless\" image (has no shell/bash installed).</p> <p><code>kubectl debug</code> spins up a new container attached to the broken Pod. It shares the process namespace but brings its own tools.</p> <pre><code># Attach a \"busybox\" container to your broken \"my-app\" pod\nkubectl debug -it my-app --image=busybox --target=my-app-container\n</code></pre>"},{"location":"troubleshooting/#3-networking-debugging","title":"3. Networking Debugging","text":"<p>If Service A can't talk to Service B:</p> <ol> <li>Check DNS: <code>nslookup my-service</code> (Does it return an IP?)</li> <li>Check Service Selector: Does the Service actually point to any pods?<ul> <li><code>kubectl get endpoints my-service</code> (If this is empty, your labels are wrong).</li> </ul> </li> </ol>"},{"location":"troubleshooting/#phase-4-exit-codes-the-cheat-sheet","title":"Phase 4: Exit Codes (The Cheat Sheet)","text":"<p>Computers speak in numbers. Here is how to translate them.</p> Code Meaning Likely Cause 0 Success The process finished its job and exited. (Normal for Jobs, bad for Deployments). 1 Application Error Generic app crash. Check logs. 137 SIGKILL (OOM) Out of Memory. Increase memory <code>limits</code>. 143 SIGTERM Kubernetes asked the Pod to stop (normal during scale-down). 255 Node Error The Node itself failed (disk full, network partition)."},{"location":"troubleshooting/#summary","title":"Summary","text":"<ol> <li>Don't Panic. Read the status.</li> <li>Events First: Always run <code>kubectl describe</code> before anything else.</li> <li>Logs Second: Check <code>kubectl logs</code> (and <code>--previous</code>).</li> <li>Validate Config: Ensure Secrets/ConfigMaps exist before the Pod starts.</li> <li>Use <code>debug</code>: Learn <code>kubectl debug</code> for difficult crashes.</li> </ol> <p>Tip</p> <p>The \"Rubber Duck\" Method</p> <p>If you are stuck, read the <code>kubectl describe</code> output out loud line-by-line. You will almost always find the error staring you in the face (e.g., <code>MountVolume.SetUp failed for volume \"secret-key\": secret \"my-secret\" not found</code>).</p>"}]}